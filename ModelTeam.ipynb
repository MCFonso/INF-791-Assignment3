{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d0814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\africaans_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\afroxlmr_test_predictions.csv\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Ciluba_test_corpus (1).xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corpus_test_split.csv\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\french_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\machine-learning-on-the-multilingual-lexicon (1).ipynb\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\test_corpus_processed.csv\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\zulu_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\africaans_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\Ciluba_test_corpus (1).xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\corrected_lexicon.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\french_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\machine-learning-on-the-multilingual-lexicon (1).ipynb\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\Assignment 3 Resources\\zulu_test_corpus.xlsx\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\logs\\events.out.tfevents.1760889518.DESKTOP-KHC03S5.5468.0\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\logs\\events.out.tfevents.1760889673.DESKTOP-KHC03S5.5468.1\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\logs\\events.out.tfevents.1760891296.DESKTOP-KHC03S5.5468.2\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\logs\\events.out.tfevents.1760891658.DESKTOP-KHC03S5.5468.3\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\config.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\model.safetensors\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\optimizer.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\rng_state.pth\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\scheduler.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\trainer_state.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-105\\training_args.bin\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\config.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\model.safetensors\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\optimizer.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\rng_state.pth\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\scheduler.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\trainer_state.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-35\\training_args.bin\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\config.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\model.safetensors\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\optimizer.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\rng_state.pth\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\scheduler.pt\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\trainer_state.json\n",
      "C:\\Users\\User\\Desktop\\Assignment 3 Resources\\results_afroxlmr\\checkpoint-70\\training_args.bin\n",
      "Using file_path: C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.notebook import tqdm  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "data_dir = r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\"\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "file_path = os.path.join(data_dir, 'corrected_lexicon.xlsx')\n",
    "print('Using file_path:', file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd7af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx with shape (3234, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CILUBA</th>\n",
       "      <th>FRANCAIS</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>AFRIKAANS</th>\n",
       "      <th>ZULU</th>\n",
       "      <th>Sepedi</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>NATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>umue</td>\n",
       "      <td>un</td>\n",
       "      <td>a</td>\n",
       "      <td>N</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>nombre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biabunyi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>ga nt≈°i</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bungi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>kudu</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>adverbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dilekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>verlating</td>\n",
       "      <td>ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CILUBA  FRANCAIS      ENGLISH  AFRIKAANS       ZULU       Sepedi  SCORE  \\\n",
       "0      umue        un            a          N          I          NaN      0   \n",
       "1  Biabunyi  beaucoup        a lot       baie   okuningi      ga nt≈°i      3   \n",
       "2     bungi  beaucoup        a lot       baie   okuningi         kudu      0   \n",
       "3  dilekela   abandon  abandonment  Verlating  Ukulahlwa  hlokomologa      4   \n",
       "4  Kulekela   abandon  abandonment  verlating  ukulahlwa  hlokomologa      3   \n",
       "\n",
       "  SENTIMENT   NATURE  \n",
       "0    Neutre   nombre  \n",
       "1   Positif      mot  \n",
       "2    Neutre  adverbe  \n",
       "3   Positif      mot  \n",
       "4   Positif      mot  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local path for corrected lexicon\n",
    "file_path = r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx\"\n",
    "\n",
    "# Load the dataset (guard against missing file)\n",
    "if os.path.exists(file_path):\n",
    "    df1 = pd.read_excel(file_path)\n",
    "    print(f\"Loaded {file_path} with shape {df1.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Expected lexicon file not found at: {file_path}\")\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a131727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ciluba</th>\n",
       "      <th>french</th>\n",
       "      <th>english</th>\n",
       "      <th>afrikaans</th>\n",
       "      <th>zulu</th>\n",
       "      <th>sepedi</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>nature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biabunyi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>ga nt≈°i</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bungi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>kudu</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>adverbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dilekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>verlating</td>\n",
       "      <td>ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>Ya Bunda</td>\n",
       "      <td>construire</td>\n",
       "      <td>build</td>\n",
       "      <td>Bou</td>\n",
       "      <td>Yakha</td>\n",
       "      <td>aga</td>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>Ya ku leta</td>\n",
       "      <td>faire venir</td>\n",
       "      <td>bring</td>\n",
       "      <td>Bring</td>\n",
       "      <td>Letha</td>\n",
       "      <td>tli≈°a</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>Yamba</td>\n",
       "      <td>prendre</td>\n",
       "      <td>take</td>\n",
       "      <td>Neem</td>\n",
       "      <td>Thatha</td>\n",
       "      <td>t≈°eya</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>Yeleka</td>\n",
       "      <td>esp√©rer</td>\n",
       "      <td>hope</td>\n",
       "      <td>Hoop</td>\n",
       "      <td>Themba</td>\n",
       "      <td>tshepo</td>\n",
       "      <td>9</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>Yeye</td>\n",
       "      <td>il</td>\n",
       "      <td>he</td>\n",
       "      <td>Hy</td>\n",
       "      <td>Yena</td>\n",
       "      <td>yena</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>pronompersonnel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2957 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ciluba       french      english  afrikaans       zulu       sepedi  \\\n",
       "1       Biabunyi     beaucoup        a lot       baie   okuningi      ga nt≈°i   \n",
       "2          bungi     beaucoup        a lot       baie   okuningi         kudu   \n",
       "3       dilekela      abandon  abandonment  Verlating  Ukulahlwa  hlokomologa   \n",
       "4       Kulekela      abandon  abandonment  verlating  ukulahlwa  hlokomologa   \n",
       "5       kulekela      abandon  abandonment  Verlating  Ukulahlwa  hlokomologa   \n",
       "...          ...          ...          ...        ...        ...          ...   \n",
       "3200    Ya Bunda   construire        build        Bou      Yakha          aga   \n",
       "3201  Ya ku leta  faire venir        bring      Bring      Letha        tli≈°a   \n",
       "3202       Yamba      prendre         take       Neem     Thatha        t≈°eya   \n",
       "3205      Yeleka      esp√©rer         hope       Hoop     Themba       tshepo   \n",
       "3207        Yeye           il           he         Hy       Yena         yena   \n",
       "\n",
       "      score sentiment           nature  \n",
       "1         3   Positif              mot  \n",
       "2         0    Neutre          adverbe  \n",
       "3         4   Positif              mot  \n",
       "4         3   Positif              mot  \n",
       "5         4   Positif              mot  \n",
       "...     ...       ...              ...  \n",
       "3200      5  Positive            verbe  \n",
       "3201      4  Positive            verbe  \n",
       "3202      3  Positive            verbe  \n",
       "3205      9  Positive            verbe  \n",
       "3207      0   Neutral  pronompersonnel  \n",
       "\n",
       "[2957 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "df1.columns = ['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'score', 'sentiment', 'nature']\n",
    "\n",
    "# Drop duplicate rows based on language columns + sentiment (or all columns if you prefer)\n",
    "df = df1.drop_duplicates(subset=['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'sentiment'])\n",
    "\n",
    "# Drop rows with missing values in the language columns + sentiment\n",
    "df = df1.dropna(subset=['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'sentiment'])\n",
    "\n",
    "# Now you can proceed to vectorize, reduce and plot based on these language columns as separate features or however you want.\n",
    "# For example, you could vectorize each language column separately or combine them as needed.\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70f84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Translation dictionaries created.\n",
      "‚úÖ Sentiment scores calculated.\n",
      "Test corpus columns: ['source_language', 'target_language', 'sentence']\n",
      "Processing 500 rows (of 2999)\n",
      "‚úÖ Sentiment analysis applied to test corpus (sample).\n",
      "üìä Preview of sentiment analysis results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>total_score_avg</th>\n",
       "      <th>word_scores_avg</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>total_score_v2</th>\n",
       "      <th>word_scores_v2</th>\n",
       "      <th>sentiment_v2</th>\n",
       "      <th>vader_positive</th>\n",
       "      <th>vader_negative</th>\n",
       "      <th>vader_neutral</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>custom_sentiment_numeric</th>\n",
       "      <th>vader_sentiment_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Arrange pagne proteger Comportement Seulement</td>\n",
       "      <td>arrange loincloth protect behavior only</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>arrange:1.0; pagne:2.0; proteger:3.0; comporte...</td>\n",
       "      <td>positive</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>arrange:1; pagne:3; proteger:3; comportement:1...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Rearrange mordre purifier V√©rit√© bourse</td>\n",
       "      <td>akajilula kusuma kutokesha bulelela tshibombu</td>\n",
       "      <td>8.566667</td>\n",
       "      <td>rearrange:1.0; mordre:-2.0; purifier:3.6666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>rearrange:1; mordre:-2; purifier:3.66666666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Parle ais√© Serpent M√®re Abhorrer</td>\n",
       "      <td>praat maklik slang moeder verafsku</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>parle:2.0; ais√©:3.0; serpent:-2.25; m√®re:2.25;...</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>parle:2; ais√©:3; serpent:-4.0; m√®re:2.25; abho...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Parler √† nouveau murmurer chanter Sein castrer</td>\n",
       "      <td>khuluma futhi nyenyeza cula isibele xholosa</td>\n",
       "      <td>16.642857</td>\n",
       "      <td>parler √† nouveau:2.0; murmurer:4.0; chanter:3....</td>\n",
       "      <td>positive</td>\n",
       "      <td>16.642857</td>\n",
       "      <td>parler √† nouveau:2; murmurer:4; chanter:3.1428...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Remet D√©chirure Infid√®le √âtrangler Kubela</td>\n",
       "      <td>put back tear unfaithful strangle kubela</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>remet:3.2; d√©chirure:-4.0; infid√®le:-5.0; √©tra...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>remet:3.2; d√©chirure:-4; infid√®le:-5; √©trangle...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Dis F√©tiche finir R√™ver corps</td>\n",
       "      <td>amba manga tshinda kulota mubidimbidi</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>dis:3.0; f√©tiche:-3.0; finir:1.333333333333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9.380952</td>\n",
       "      <td>dis:3; f√©tiche:-3.0; finir:4.0; r√™ver:2.666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Superposer Preparer bruit c√©r√©moniecoutumi√®re ...</td>\n",
       "      <td>superponeer voorberei geraas gebruiklike serem...</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>superposer:2.2; preparer:3.4; bruit:2.66666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>superposer:2.2; preparer:3.4; bruit:2.66666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Ramasse parfaite voler cueillire trente-cinq</td>\n",
       "      <td>phakamisa gweda ndiza ukhethiwe trentecinq</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>ramasse:4.0; parfaite:3.0; voler:-4.2; cueilli...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>ramasse:4; parfaite:4; voler:-4.2; cueillire:1...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>D√©p√™che maladie chapeau rouler repasser</td>\n",
       "      <td>dispatch illness hat to roll go back</td>\n",
       "      <td>9.550000</td>\n",
       "      <td>d√©p√™che:4.0; maladie:-0.7; chapeau:2.75; roule...</td>\n",
       "      <td>positive</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>d√©p√™che:4; maladie:-2.5; chapeau:2.75; rouler:...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>R√©p√®te Gal√®re Expliquer trasformer voler</td>\n",
       "      <td>ambulula dikenga kuvuija kukudimuna kuiba</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>R√©pond Mouche Plaie acheter onze</td>\n",
       "      <td>antwoorde vlieg wond koop elf</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>r√©pond:5.5; mouche:3.0; plaie:3.0; acheter:2.0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>r√©pond:9; mouche:3; plaie:3; acheter:2.0; onze:0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Prend L√©ger Histoire Brouillard Exister</td>\n",
       "      <td>thatha mhlophe umlando inkungu hlala</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>prend:6.0; l√©ger:3.0; histoire:4.0; brouillard...</td>\n",
       "      <td>positive</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>prend:6.0; l√©ger:3; histoire:4; brouillard:1.6...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Reprend Pr√©dication cinquante-wight Accomplir ...</td>\n",
       "      <td>resume preaching cinquantewight accomplish put...</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>reprend:9.0; pr√©dication:2.0; cinquantewight:0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>reprend:9; pr√©dication:3; cinquantewight:0; ac...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Habits S√©rieux Lune chemise diminuer</td>\n",
       "      <td>bilamba cilongama ngondu mutelu kukeresha</td>\n",
       "      <td>11.916667</td>\n",
       "      <td>habits:5.0; s√©rieux:3.0; lune:2.25; chemise:2....</td>\n",
       "      <td>positive</td>\n",
       "      <td>12.916667</td>\n",
       "      <td>habits:8; s√©rieux:3; lune:2.25; chemise:2.6666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Blague R√©sultats(R√©compense) ouvrir Droite Adm...</td>\n",
       "      <td>grap r√©sultatsr√©compense oopmaak regs bewonder</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>blague:5.5; r√©sultatsr√©compense:0; ouvrir:2.0;...</td>\n",
       "      <td>positive</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>blague:8; r√©sultatsr√©compense:0; ouvrir:2; dro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>V√™tements Intelligence tout c√©r√©moniecoutumi√®r...</td>\n",
       "      <td>izingubo ubuhlakani konke umkhosi owesiko iphe...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>v√™tements:0.3333333333333333; intelligence:2.5...</td>\n",
       "      <td>positive</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>v√™tements:2.0; intelligence:2.5; tout:1; c√©r√©m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Larmes ventre capitaine Raison Fuir</td>\n",
       "      <td>tears belly captain reason flee</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>larmes:0.3333333333333333; ventre:3.0; capitai...</td>\n",
       "      <td>positive</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>larmes:-3.0; ventre:3.0; capitaine:2.5; raison...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Entre manger deuil babouche Eclair</td>\n",
       "      <td>buela kuja madilu mapapa mupenyimukenya</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>entre:5.0; manger:1.7777777777777777; deuil:-6...</td>\n",
       "      <td>positive</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>entre:7; manger:2.0; deuil:-6; babouche:2.6666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Lumi√®re chemise miroir ardeur cheveux</td>\n",
       "      <td>lig hemp spie√´l ywer hare</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>lumi√®re:2.5; chemise:2.6666666666666665; miroi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>lumi√®re:2.5; chemise:2.6666666666666665; miroi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Terre Couleur justification copier absence</td>\n",
       "      <td>umhlaba umbala ukuthethelela isizathu esizwaka...</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>terre:2.0; couleur:1.6666666666666667; justifi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>terre:3.0; couleur:1.6666666666666667; justifi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_language target_language  \\\n",
       "0           french         english   \n",
       "1           french          ciluba   \n",
       "2           french       afrikaans   \n",
       "3           french            zulu   \n",
       "4           french         english   \n",
       "5           french          ciluba   \n",
       "6           french       afrikaans   \n",
       "7           french            zulu   \n",
       "8           french         english   \n",
       "9           french          ciluba   \n",
       "10          french       afrikaans   \n",
       "11          french            zulu   \n",
       "12          french         english   \n",
       "13          french          ciluba   \n",
       "14          french       afrikaans   \n",
       "15          french            zulu   \n",
       "16          french         english   \n",
       "17          french          ciluba   \n",
       "18          french       afrikaans   \n",
       "19          french            zulu   \n",
       "\n",
       "                                             sentence  \\\n",
       "0       Arrange pagne proteger Comportement Seulement   \n",
       "1             Rearrange mordre purifier V√©rit√© bourse   \n",
       "2                    Parle ais√© Serpent M√®re Abhorrer   \n",
       "3      Parler √† nouveau murmurer chanter Sein castrer   \n",
       "4           Remet D√©chirure Infid√®le √âtrangler Kubela   \n",
       "5                       Dis F√©tiche finir R√™ver corps   \n",
       "6   Superposer Preparer bruit c√©r√©moniecoutumi√®re ...   \n",
       "7        Ramasse parfaite voler cueillire trente-cinq   \n",
       "8             D√©p√™che maladie chapeau rouler repasser   \n",
       "9            R√©p√®te Gal√®re Expliquer trasformer voler   \n",
       "10                   R√©pond Mouche Plaie acheter onze   \n",
       "11            Prend L√©ger Histoire Brouillard Exister   \n",
       "12  Reprend Pr√©dication cinquante-wight Accomplir ...   \n",
       "13               Habits S√©rieux Lune chemise diminuer   \n",
       "14  Blague R√©sultats(R√©compense) ouvrir Droite Adm...   \n",
       "15  V√™tements Intelligence tout c√©r√©moniecoutumi√®r...   \n",
       "16                Larmes ventre capitaine Raison Fuir   \n",
       "17                 Entre manger deuil babouche Eclair   \n",
       "18              Lumi√®re chemise miroir ardeur cheveux   \n",
       "19         Terre Couleur justification copier absence   \n",
       "\n",
       "                                      translated_text  total_score_avg  \\\n",
       "0             arrange loincloth protect behavior only        10.600000   \n",
       "1       akajilula kusuma kutokesha bulelela tshibombu         8.566667   \n",
       "2                  praat maklik slang moeder verafsku         9.000000   \n",
       "3         khuluma futhi nyenyeza cula isibele xholosa        16.642857   \n",
       "4            put back tear unfaithful strangle kubela        -1.800000   \n",
       "5               amba manga tshinda kulota mubidimbidi         6.714286   \n",
       "6   superponeer voorberei geraas gebruiklike serem...        13.266667   \n",
       "7          phakamisa gweda ndiza ukhethiwe trentecinq         3.800000   \n",
       "8                dispatch illness hat to roll go back         9.550000   \n",
       "9           ambulula dikenga kuvuija kukudimuna kuiba         0.183333   \n",
       "10                      antwoorde vlieg wond koop elf        13.500000   \n",
       "11               thatha mhlophe umlando inkungu hlala        18.666667   \n",
       "12  resume preaching cinquantewight accomplish put...        18.700000   \n",
       "13          bilamba cilongama ngondu mutelu kukeresha        11.916667   \n",
       "14     grap r√©sultatsr√©compense oopmaak regs bewonder        15.000000   \n",
       "15  izingubo ubuhlakani konke umkhosi owesiko iphe...         9.333333   \n",
       "16                    tears belly captain reason flee        10.333333   \n",
       "17            buela kuja madilu mapapa mupenyimukenya         4.444444   \n",
       "18                          lig hemp spie√´l ywer hare        14.777778   \n",
       "19  umhlaba umbala ukuthethelela isizathu esizwaka...        12.666667   \n",
       "\n",
       "                                      word_scores_avg sentiment_avg  \\\n",
       "0   arrange:1.0; pagne:2.0; proteger:3.0; comporte...      positive   \n",
       "1   rearrange:1.0; mordre:-2.0; purifier:3.6666666...      positive   \n",
       "2   parle:2.0; ais√©:3.0; serpent:-2.25; m√®re:2.25;...      positive   \n",
       "3   parler √† nouveau:2.0; murmurer:4.0; chanter:3....      positive   \n",
       "4   remet:3.2; d√©chirure:-4.0; infid√®le:-5.0; √©tra...      negative   \n",
       "5   dis:3.0; f√©tiche:-3.0; finir:1.333333333333333...      positive   \n",
       "6   superposer:2.2; preparer:3.4; bruit:2.66666666...      positive   \n",
       "7   ramasse:4.0; parfaite:3.0; voler:-4.2; cueilli...      positive   \n",
       "8   d√©p√™che:4.0; maladie:-0.7; chapeau:2.75; roule...      positive   \n",
       "9   r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...      positive   \n",
       "10  r√©pond:5.5; mouche:3.0; plaie:3.0; acheter:2.0...      positive   \n",
       "11  prend:6.0; l√©ger:3.0; histoire:4.0; brouillard...      positive   \n",
       "12  reprend:9.0; pr√©dication:2.0; cinquantewight:0...      positive   \n",
       "13  habits:5.0; s√©rieux:3.0; lune:2.25; chemise:2....      positive   \n",
       "14  blague:5.5; r√©sultatsr√©compense:0; ouvrir:2.0;...      positive   \n",
       "15  v√™tements:0.3333333333333333; intelligence:2.5...      positive   \n",
       "16  larmes:0.3333333333333333; ventre:3.0; capitai...      positive   \n",
       "17  entre:5.0; manger:1.7777777777777777; deuil:-6...      positive   \n",
       "18  lumi√®re:2.5; chemise:2.6666666666666665; miroi...      positive   \n",
       "19  terre:2.0; couleur:1.6666666666666667; justifi...      positive   \n",
       "\n",
       "    total_score_v2                                     word_scores_v2  \\\n",
       "0        11.600000  arrange:1; pagne:3; proteger:3; comportement:1...   \n",
       "1         9.066667  rearrange:1; mordre:-2; purifier:3.66666666666...   \n",
       "2         7.250000  parle:2; ais√©:3; serpent:-4.0; m√®re:2.25; abho...   \n",
       "3        16.642857  parler √† nouveau:2; murmurer:4; chanter:3.1428...   \n",
       "4        -1.800000  remet:3.2; d√©chirure:-4; infid√®le:-5; √©trangle...   \n",
       "5         9.380952  dis:3; f√©tiche:-3.0; finir:4.0; r√™ver:2.666666...   \n",
       "6        13.266667  superposer:2.2; preparer:3.4; bruit:2.66666666...   \n",
       "7         4.800000  ramasse:4; parfaite:4; voler:-4.2; cueillire:1...   \n",
       "8         8.250000  d√©p√™che:4; maladie:-2.5; chapeau:2.75; rouler:...   \n",
       "9         0.183333  r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...   \n",
       "10       17.000000   r√©pond:9; mouche:3; plaie:3; acheter:2.0; onze:0   \n",
       "11       18.666667  prend:6.0; l√©ger:3; histoire:4; brouillard:1.6...   \n",
       "12       19.700000  reprend:9; pr√©dication:3; cinquantewight:0; ac...   \n",
       "13       12.916667  habits:8; s√©rieux:3; lune:2.25; chemise:2.6666...   \n",
       "14       17.500000  blague:8; r√©sultatsr√©compense:0; ouvrir:2; dro...   \n",
       "15       11.500000  v√™tements:2.0; intelligence:2.5; tout:1; c√©r√©m...   \n",
       "16        8.500000  larmes:-3.0; ventre:3.0; capitaine:2.5; raison...   \n",
       "17        6.666667  entre:7; manger:2.0; deuil:-6; babouche:2.6666...   \n",
       "18       14.777778  lumi√®re:2.5; chemise:2.6666666666666665; miroi...   \n",
       "19       15.666667  terre:3.0; couleur:1.6666666666666667; justifi...   \n",
       "\n",
       "   sentiment_v2  vader_positive  vader_negative  vader_neutral  \\\n",
       "0      positive           0.000           0.000          1.000   \n",
       "1      positive           0.000           0.000          1.000   \n",
       "2      positive           0.000           0.000          1.000   \n",
       "3      positive           0.000           0.000          1.000   \n",
       "4      negative           0.000           0.000          1.000   \n",
       "5      positive           0.000           0.000          1.000   \n",
       "6      positive           0.000           0.000          1.000   \n",
       "7      positive           0.000           0.000          1.000   \n",
       "8      positive           0.000           0.000          1.000   \n",
       "9      positive           0.000           0.000          1.000   \n",
       "10     positive           0.000           0.000          1.000   \n",
       "11     positive           0.000           0.000          1.000   \n",
       "12     positive           0.000           0.000          1.000   \n",
       "13     positive           0.000           0.000          1.000   \n",
       "14     positive           0.412           0.000          0.588   \n",
       "15     positive           0.408           0.197          0.395   \n",
       "16     positive           0.000           0.000          1.000   \n",
       "17     positive           0.000           0.000          1.000   \n",
       "18     positive           0.000           0.000          1.000   \n",
       "19     positive           0.000           0.000          1.000   \n",
       "\n",
       "    vader_compound vader_sentiment  custom_sentiment_numeric  \\\n",
       "0           0.0000         neutral                         1   \n",
       "1           0.0000         neutral                         1   \n",
       "2           0.0000         neutral                         1   \n",
       "3           0.0000         neutral                         1   \n",
       "4           0.0000         neutral                        -1   \n",
       "5           0.0000         neutral                         1   \n",
       "6           0.0000         neutral                         1   \n",
       "7           0.0000         neutral                         1   \n",
       "8           0.0000         neutral                         1   \n",
       "9           0.0000         neutral                         1   \n",
       "10          0.0000         neutral                         1   \n",
       "11          0.0000         neutral                         1   \n",
       "12          0.0000         neutral                         1   \n",
       "13          0.0000         neutral                         1   \n",
       "14          0.4215        positive                         1   \n",
       "15          0.3818        positive                         1   \n",
       "16          0.0000         neutral                         1   \n",
       "17          0.0000         neutral                         1   \n",
       "18          0.0000         neutral                         1   \n",
       "19          0.0000         neutral                         1   \n",
       "\n",
       "    vader_sentiment_numeric  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "5                         0  \n",
       "6                         0  \n",
       "7                         0  \n",
       "8                         0  \n",
       "9                         0  \n",
       "10                        0  \n",
       "11                        0  \n",
       "12                        0  \n",
       "13                        0  \n",
       "14                        1  \n",
       "15                        1  \n",
       "16                        0  \n",
       "17                        0  \n",
       "18                        0  \n",
       "19                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# STEP 1: Load Data\n",
    "# ================================\n",
    "# Local files in data_dir set earlier in cell 1 \n",
    "lexicon_path = os.path.join(r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\", 'corrected_lexicon.xlsx')\n",
    "french_test_path = os.path.join(r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\", 'french_test_corpus.xlsx')\n",
    "\n",
    "df = pd.read_excel(lexicon_path)\n",
    "test_corpus_df = pd.read_excel(french_test_path)\n",
    "\n",
    "# ================================\n",
    "# STEP 2: Preprocess Lexicon\n",
    "# ================================\n",
    "# Rename columns\n",
    "df.columns = ['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'score', 'sentiment', 'nature']\n",
    "\n",
    "# Define languages\n",
    "supported_languages = ['french', 'afrikaans', 'zulu', 'ciluba', 'sepedi', 'english']\n",
    "\n",
    "# ================================\n",
    "# STEP 3: Translation Dictionary\n",
    "# ================================\n",
    "def clean_text_for_matching(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "    return text.lower()\n",
    "\n",
    "def create_translation_dicts(df, languages):\n",
    "    translation_dicts = {src: {tgt: {} for tgt in languages if tgt != src} for src in languages}\n",
    "    for _, row in df.iterrows():\n",
    "        for src in languages:\n",
    "            source_phrase = str(row[src]).strip().lower()\n",
    "            if pd.isna(source_phrase) or source_phrase == '':\n",
    "                continue\n",
    "            for tgt in languages:\n",
    "                if tgt == src:\n",
    "                    continue\n",
    "                target_phrase = str(row[tgt]).strip().lower()\n",
    "                if pd.isna(target_phrase) or target_phrase == '':\n",
    "                    continue\n",
    "                translation_dicts[src][tgt][source_phrase] = target_phrase\n",
    "    return translation_dicts\n",
    "\n",
    "translation_dicts = create_translation_dicts(df, supported_languages)\n",
    "print(\"‚úÖ Translation dictionaries created.\")\n",
    "\n",
    "# ================================\n",
    "# STEP 4: Sentiment Scores\n",
    "# ================================\n",
    "sentiment_averages = {}\n",
    "all_sentiments = {}\n",
    "\n",
    "for lang in supported_languages:\n",
    "    sentiment_averages[lang] = df.groupby(lang)['score'].mean().to_dict()\n",
    "    all_sentiments[lang] = df.groupby(lang)['score'].apply(list).to_dict()\n",
    "\n",
    "print(\"‚úÖ Sentiment scores calculated.\")\n",
    "\n",
    "# ================================\n",
    "# STEP 5: Custom Sentiment Function\n",
    "# ================================\n",
    "def compute_sentiment_v2(scores):\n",
    "    if len(scores) == 1:\n",
    "        return scores[0]\n",
    "    elif len(scores) == 2:\n",
    "        return max(scores, key=abs)\n",
    "    else:\n",
    "        pos = [s for s in scores if s > 0]\n",
    "        neg = [s for s in scores if s < 0]\n",
    "        if len(pos) >= len(neg):\n",
    "            return sum(pos) / len(pos) if pos else 0\n",
    "        else:\n",
    "            return sum(neg) / len(neg) if neg else 0\n",
    "\n",
    "# ================================\n",
    "# STEP 6: Translate & Analyze Function\n",
    "# ================================\n",
    "def translate_analyze_sentiments_with_vader(text, source_lang, target_lang,\n",
    "                                            translation_dicts, sentiment_averages, all_sentiments,\n",
    "                                            vader_analyzer):\n",
    "    source_lang = str(source_lang).lower()\n",
    "    target_lang = str(target_lang).lower()\n",
    "\n",
    "    if source_lang not in translation_dicts or target_lang not in translation_dicts[source_lang]:\n",
    "        return {k: '' if isinstance(v, str) else 0 for k, v in {\n",
    "            \"translated_text\": \"\",\n",
    "            \"total_score_avg\": 0,\n",
    "            \"word_scores_avg\": \"\",\n",
    "            \"sentiment_avg\": \"neutral\",\n",
    "            \"total_score_v2\": 0,\n",
    "            \"word_scores_v2\": \"\",\n",
    "            \"sentiment_v2\": \"neutral\",\n",
    "            \"vader_positive\": 0,\n",
    "            \"vader_negative\": 0,\n",
    "            \"vader_neutral\": 0,\n",
    "            \"vader_compound\": 0,\n",
    "            \"vader_sentiment\": \"neutral\"\n",
    "        }.items()}\n",
    "\n",
    "    cleaned_text = clean_text_for_matching(text)\n",
    "    words = cleaned_text.split()\n",
    "    translated_sentence = []\n",
    "    total_score_avg = 0\n",
    "    total_score_v2 = 0\n",
    "    word_scores_avg = []\n",
    "    word_scores_v2 = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        matched_phrase = None\n",
    "        translated_phrase = None\n",
    "        phrase_score_avg = 0\n",
    "        phrase_scores_v2 = 0\n",
    "        max_length = min(5, len(words) - i)\n",
    "\n",
    "        for j in range(max_length, 0, -1):\n",
    "            phrase = ' '.join(words[i:i+j])\n",
    "            if phrase in translation_dicts[source_lang][target_lang]:\n",
    "                matched_phrase = phrase\n",
    "                translated_phrase = translation_dicts[source_lang][target_lang][phrase]\n",
    "                phrase_score_avg = sentiment_averages[source_lang].get(phrase, 0)\n",
    "                phrase_scores = all_sentiments[source_lang].get(phrase, [])\n",
    "                phrase_scores_v2 = compute_sentiment_v2(phrase_scores)\n",
    "                i += j\n",
    "                break\n",
    "\n",
    "        if matched_phrase:\n",
    "            translated_sentence.append(translated_phrase)\n",
    "            total_score_avg += phrase_score_avg\n",
    "            total_score_v2 += phrase_scores_v2\n",
    "            word_scores_avg.append(f\"{matched_phrase}:{phrase_score_avg}\")\n",
    "            word_scores_v2.append(f\"{matched_phrase}:{phrase_scores_v2}\")\n",
    "        else:\n",
    "            word = words[i]\n",
    "            translated_word = translation_dicts[source_lang][target_lang].get(word, word)\n",
    "            translated_sentence.append(translated_word)\n",
    "            score_avg = sentiment_averages[source_lang].get(word, 0)\n",
    "            scores = all_sentiments[source_lang].get(word, [])\n",
    "            score_v2 = compute_sentiment_v2(scores)\n",
    "            total_score_avg += score_avg\n",
    "            total_score_v2 += score_v2\n",
    "            word_scores_avg.append(f\"{word}:{score_avg}\")\n",
    "            word_scores_v2.append(f\"{word}:{score_v2}\")\n",
    "            i += 1\n",
    "\n",
    "    translated_text = ' '.join(translated_sentence).strip()\n",
    "    sentiment_avg = \"positive\" if total_score_avg > 0.05 else \"negative\" if total_score_avg < -0.05 else \"neutral\"\n",
    "    sentiment_v2 = \"positive\" if total_score_v2 > 0.05 else \"negative\" if total_score_v2 < -0.05 else \"neutral\"\n",
    "\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    vader_sentiment = \"positive\" if vader_scores['compound'] >= 0.05 else \"negative\" if vader_scores['compound'] <= -0.05 else \"neutral\"\n",
    "\n",
    "    return {\n",
    "        \"translated_text\": translated_text,\n",
    "        \"total_score_avg\": total_score_avg,\n",
    "        \"word_scores_avg\": '; '.join(word_scores_avg),\n",
    "        \"sentiment_avg\": sentiment_avg,\n",
    "        \"total_score_v2\": total_score_v2,\n",
    "        \"word_scores_v2\": '; '.join(word_scores_v2),\n",
    "        \"sentiment_v2\": sentiment_v2,\n",
    "        \"vader_positive\": vader_scores['pos'],\n",
    "        \"vader_negative\": vader_scores['neg'],\n",
    "        \"vader_neutral\": vader_scores['neu'],\n",
    "        \"vader_compound\": vader_scores['compound'],\n",
    "        \"vader_sentiment\": vader_sentiment\n",
    "    }\n",
    "\n",
    "# ================================\n",
    "# STEP 7: Apply to Test Corpus\n",
    "# ================================\n",
    "def safe_translate_and_analyze_sentiments_with_vader(row, translation_dicts, sentiment_averages, all_sentiments, vader_analyzer):\n",
    "    try:\n",
    "        return pd.Series(translate_analyze_sentiments_with_vader(\n",
    "            row.get('sentence', ''),\n",
    "            row.get('source_language', ''),\n",
    "            row.get('target_language', ''),\n",
    "            translation_dicts,\n",
    "            sentiment_averages,\n",
    "            all_sentiments,\n",
    "            vader_analyzer\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in row {row.name}: {e}\")\n",
    "        return pd.Series({\n",
    "            \"translated_text\": \"\",\n",
    "            \"total_score_avg\": 0,\n",
    "            \"word_scores_avg\": \"\",\n",
    "            \"sentiment_avg\": \"neutral\",\n",
    "            \"total_score_v2\": 0,\n",
    "            \"word_scores_v2\": \"\",\n",
    "            \"sentiment_v2\": \"neutral\",\n",
    "            \"vader_positive\": 0,\n",
    "            \"vader_negative\": 0,\n",
    "            \"vader_neutral\": 0,\n",
    "            \"vader_compound\": 0,\n",
    "            \"vader_sentiment\": \"neutral\"\n",
    "        })\n",
    "\n",
    "# Run the sentiment analysis\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "# Ensure we have the expected columns in test_corpus_df\n",
    "print('Test corpus columns:', test_corpus_df.columns.tolist())\n",
    "if 'sentence' not in test_corpus_df.columns:\n",
    "    raise KeyError(\"Expected 'sentence' column in test corpus. Found: \" + ','.join(test_corpus_df.columns.astype(str)))\n",
    "\n",
    "# Apply translation+sentiment\n",
    "test_corpus_df = test_corpus_df.copy()\n",
    "\n",
    "# For speed in this environment, process a sample (or full if small)\n",
    "sample_size = min(len(test_corpus_df), 500)\n",
    "print(f\"Processing {sample_size} rows (of {len(test_corpus_df)})\")\n",
    "\n",
    "test_corpus_df_sample = test_corpus_df.iloc[:sample_size].copy()\n",
    "\n",
    "results = test_corpus_df_sample.apply(\n",
    "    lambda row: safe_translate_and_analyze_sentiments_with_vader(\n",
    "        row,\n",
    "        translation_dicts,\n",
    "        sentiment_averages,\n",
    "        all_sentiments,\n",
    "        vader_analyzer\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Join results back\n",
    "test_corpus_df = test_corpus_df_sample.join(results)\n",
    "\n",
    "print(\"‚úÖ Sentiment analysis applied to test corpus (sample).\")\n",
    "\n",
    "# ================================\n",
    "# STEP 8: Optional Evaluation\n",
    "# ================================\n",
    "# Map sentiment strings to numeric\n",
    "sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "if 'sentiment_v2' in test_corpus_df.columns:\n",
    "    test_corpus_df['custom_sentiment_numeric'] = test_corpus_df['sentiment_v2'].map(sentiment_mapping)\n",
    "if 'vader_sentiment' in test_corpus_df.columns:\n",
    "    test_corpus_df['vader_sentiment_numeric'] = test_corpus_df['vader_sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# ================================\n",
    "# STEP 9: Display Results\n",
    "# ================================\n",
    "print(\"üìä Preview of sentiment analysis results:\")\n",
    "display(test_corpus_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891849e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed test_corpus_df to: C:\\Users\\User\\Desktop\\Assignment 3 Resources\\test_corpus_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the processed test corpus (current `test_corpus_df` in notebook)\n",
    "import os\n",
    "out_path = os.path.join(data_dir, 'test_corpus_processed.csv')\n",
    "# Ensure DataFrame exists\n",
    "if 'test_corpus_df' in globals():\n",
    "    try:\n",
    "        test_corpus_df.to_csv(out_path, index=False, encoding='utf-8')\n",
    "        print(f\"Saved processed test_corpus_df to: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print('Error saving CSV:', e)\n",
    "else:\n",
    "    print('test_corpus_df not found in the notebook namespace.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b91701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing transformer packages...\n",
      "============================================================\n",
      "\n",
      "üì• Installing transformers...\n",
      "   ‚úÖ transformers installed\n",
      "\n",
      "üì• Installing torch...\n",
      "   ‚úÖ torch installed\n",
      "\n",
      "üì• Installing sentencepiece...\n",
      "   ‚úÖ sentencepiece installed\n",
      "\n",
      "============================================================\n",
      "‚úÖ Installation complete!\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Restart the kernel now!\n",
      "   Go to: Kernel ‚Üí Restart Kernel\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# STEP 1: Install Required Libraries (SIMPLIFIED)\n",
    "# ================================\n",
    "# Run this cell AFTER the NumPy fix cell above and kernel restart\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing transformer packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install packages one by one with proper error handling\n",
    "packages = [\n",
    "    ('transformers', 'transformers'),\n",
    "    ('torch', 'torch --index-url https://download.pytorch.org/whl/cpu'),\n",
    "    ('sentencepiece', 'sentencepiece'),\n",
    "]\n",
    "\n",
    "for name, install_cmd in packages:\n",
    "    print(f\"\\nüì• Installing {name}...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'install'] + install_cmd.split(),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ‚úÖ {name} installed\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {name} may have issues: {result.stderr[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error with {name}: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Restart the kernel now!\")\n",
    "print(\"   Go to: Kernel ‚Üí Restart Kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd65622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages...\n",
      "==================================================\n",
      "‚úÖ PyTorch (torch): Installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers (transformers): Installed\n",
      "‚úÖ SentencePiece (sentencepiece): Installed\n",
      "==================================================\n",
      "\n",
      "‚úÖ All packages are installed! You can proceed.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# STEP 1b: Verify Installation (Run this after kernel restart)\n",
    "# ================================\n",
    "# Run this cell to check if packages are properly installed\n",
    "\n",
    "import sys\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages_to_check = {\n",
    "    'torch': 'PyTorch',\n",
    "    'transformers': 'Transformers',\n",
    "    'sentencepiece': 'SentencePiece',\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages...\")\n",
    "print(\"=\" * 50)\n",
    "all_installed = True\n",
    "\n",
    "for pkg, name in packages_to_check.items():\n",
    "    if check_package(pkg):\n",
    "        print(f\"‚úÖ {name} ({pkg}): Installed\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} ({pkg}): NOT installed\")\n",
    "        all_installed = False\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_installed:\n",
    "    print(\"\\n‚úÖ All packages are installed! You can proceed.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some packages are missing. Please:\")\n",
    "    print(\"   1. Run the installation cell above\")\n",
    "    print(\"   2. Restart the kernel\")\n",
    "    print(\"   3. Run this cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb425b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# EVERYTHING BELOW IS WITH REGARDS TO MODEL TRAINING \n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 2: Load Transformer Models and Prepare Data\n",
    "# ================================\n",
    "# Run this cell AFTER restarting the kernel following package installation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model names from HuggingFace\n",
    "afroxlmr_model = \"Davlan/afro-xlmr-base\"  # AfroXLMR base model\n",
    "afriberta_model = \"castorini/afriberta_base\"  # AfriBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5590fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS BELOW IS CREATING THREE TYPES OF DATA:\n",
    "#STORING LEXICON INDIVIDUAL WORDS\n",
    "#CREATING SENTENCES FROM LESXICON WORDS\n",
    "#USING CORPUS SENTENCES BOTH TRANSLATED AND ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6082d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training data prepared:\n",
      "   Total examples: 10454\n",
      "   Label distribution:\n",
      "label\n",
      "2    8599\n",
      "0     980\n",
      "1     875\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Sample examples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>un</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beaucoup</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abhorrer</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>capacit√©</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abolir</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abolition</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abominable</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>avorter</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>absence</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text sentiment  label\n",
       "0           un   neutral      1\n",
       "1     beaucoup  positive      2\n",
       "3      abandon  positive      2\n",
       "6     abhorrer  positive      2\n",
       "7     capacit√©  positive      2\n",
       "8       abolir  positive      2\n",
       "9    abolition  positive      2\n",
       "10  abominable  positive      2\n",
       "11     avorter  negative      0\n",
       "12     absence  positive      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================\n",
    "# THIS CELL IS PURELY FOR LEXICON INDIVIDUAL WORDS - WE DONT HAVE TO INCLUDE JUST IN CASE\n",
    "# ================================\n",
    "# We'll use the lexicon data to create labeled training examples\n",
    "# The lexicon has sentiment labels we can use for fine-tuning\n",
    "\n",
    "\n",
    "# Problem: Lexicon has mixed French/English labels (\"Positif\", \"positive\", \"Negatif\", etc.)\n",
    "# Solution: Convert all variants to consistent English lowercase labels\n",
    "def normalize_sentiment(sentiment):\n",
    "    sentiment = str(sentiment).lower().strip()  # Convert to lowercase and remove spaces\n",
    "    \n",
    "    # Map all variants to standard labels\n",
    "    if sentiment in ['positif', 'positive']:\n",
    "        return 'positive'\n",
    "    elif sentiment in ['negatif', 'negative']:\n",
    "        return 'negative'\n",
    "    elif sentiment in ['neutre', 'neutral']:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return None  # Invalid labels will be removed later\n",
    "\n",
    "# Apply normalization to all sentiment labels\n",
    "df['sentiment_normalized'] = df['sentiment'].apply(normalize_sentiment)\n",
    "\n",
    "# Remove rows with unmapped/invalid sentiments\n",
    "df_clean = df[df['sentiment_normalized'].notna()].copy()\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2. EXTRACT WORDS FROM EACH LANGUAGE\n",
    "# --------------------------------------------\n",
    "# For each language column (French, English, Zulu, etc.), extract the word + sentiment\n",
    "# This creates separate training examples for each language's words\n",
    "# Example: \"beaucoup\" (French) ‚Üí positive, \"a lot\" (English) ‚Üí positive\n",
    "\n",
    "train_data = []  # Will hold data from all languages\n",
    "\n",
    "for lang in supported_languages:  # Loop through: french, afrikaans, zulu, ciluba, sepedi, english\n",
    "    # Extract the language column and sentiment\n",
    "    temp_df = df_clean[[lang, 'sentiment_normalized']].copy()\n",
    "    temp_df.columns = ['text', 'sentiment']\n",
    "    \n",
    "    # Clean up the data\n",
    "    temp_df = temp_df.dropna(subset=['text', 'sentiment'])  # Remove empty cells\n",
    "    temp_df['text'] = temp_df['text'].astype(str)           # Ensure text is string\n",
    "    temp_df = temp_df[temp_df['text'].str.strip() != '']    # Remove blank strings\n",
    "    temp_df = temp_df[temp_df['text'].str.lower() != 'nan'] # Remove \"nan\" strings\n",
    "    \n",
    "    # Add this language's data to the list\n",
    "    train_data.append(temp_df)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3. COMBINE ALL LANGUAGES\n",
    "# --------------------------------------------\n",
    "# Merge all language data into one big training set\n",
    "# Result: bungi, beaucoup, a lot, baie, okuningi, kudu all become separate training examples\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "# Remove duplicate words (if same word appears in multiple rows)\n",
    "train_df = train_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4. CONVERT SENTIMENT TO NUMBERS\n",
    "# --------------------------------------------\n",
    "# Transformers need numeric labels, not text\n",
    "# negative ‚Üí 0, neutral ‚Üí 1, positive ‚Üí 2\n",
    "sentiment_to_label = {'negative': 0, 'neutral': 1, 'positive': 2}  # Text ‚Üí Number\n",
    "label_to_sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}  # Number ‚Üí Text (for later)\n",
    "\n",
    "train_df['label'] = train_df['sentiment'].map(sentiment_to_label)\n",
    "\n",
    "# Remove any rows where mapping failed (shouldn't happen, but just in case)\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "train_df['label'] = train_df['label'].astype(int)  # Ensure label is integer\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. DISPLAY RESULTS\n",
    "# --------------------------------------------\n",
    "print(f\"\\n‚úÖ Training data prepared:\")\n",
    "print(f\"   Total examples: {len(train_df)}\")\n",
    "print(f\"   Label distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"\\n   Sample examples:\")\n",
    "display(train_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa79751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Augmenting lexicon words into sentence templates...\n",
      "‚úÖ Created 38008 augmented sentences from lexicon\n",
      "   Distribution: {'positive': 32128, 'negative': 3440, 'neutral': 2440}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CREATING SENTENCES USING LEXICON WORDS FOR MORE DATA - MAYBE LEXICON TEAM SHOULD DO THIS BUT JUST IN CASE ITS HERE\n",
    "# ================================\n",
    "# Convert isolated lexicon words into sentence contexts\n",
    "\n",
    "print(\"üîÑ Augmenting lexicon words into sentence templates...\")\n",
    "\n",
    "# Sentence templates for different languages\n",
    "templates_by_lang = {\n",
    "    'french': [\n",
    "        \"Je trouve que {} est important.\",\n",
    "        \"C'est {}.\",\n",
    "        \"Le mot {} exprime un sentiment.\",\n",
    "        \"{} dans cette phrase.\"\n",
    "    ],\n",
    "    'english': [\n",
    "        \"I think {} is important.\",\n",
    "        \"This is {}.\",\n",
    "        \"The word {} expresses a feeling.\",\n",
    "        \"{} in this sentence.\"\n",
    "    ],\n",
    "    'afrikaans': [\n",
    "        \"Dit is {}.\",\n",
    "        \"Die woord {} is belangrik.\",\n",
    "        \"{} in hierdie sin.\"\n",
    "    ],\n",
    "    'zulu': [\n",
    "        \"Lokhu {}.\",\n",
    "        \"Igama {} libalulekile.\",\n",
    "        \"{} kulesi sigaba.\"\n",
    "    ],\n",
    "    'ciluba': [\n",
    "        \"Ici {}.\",\n",
    "        \"Ijambu {} lidipingana.\",\n",
    "        \"{} mumpanzu.\"\n",
    "    ],\n",
    "    'sepedi': [\n",
    "        \"Se ke {}.\",\n",
    "        \"Lent≈°u {} le bohlokwa.\",\n",
    "        \"{} mo polelong ye.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create augmented dataset\n",
    "aug_data = []\n",
    "\n",
    "for _, row in df_clean.iterrows():\n",
    "    sentiment = row['sentiment_normalized']\n",
    "    if pd.isna(sentiment):\n",
    "        continue\n",
    "    \n",
    "    # For each language, create template-based sentences\n",
    "    for lang in supported_languages:\n",
    "        word = str(row[lang]).strip()\n",
    "        if not word or pd.isna(word) or word.lower() == 'nan':\n",
    "            continue\n",
    "        \n",
    "        # Use language-specific templates if available, else use French templates\n",
    "        templates = templates_by_lang.get(lang, templates_by_lang['french'])\n",
    "        \n",
    "        # Create 2-3 sentences per word (not all templates to avoid too much data)\n",
    "        for template in templates[:2]:\n",
    "            try:\n",
    "                sentence = template.format(word)\n",
    "                aug_data.append({\n",
    "                    'text': sentence,\n",
    "                    'sentiment': sentiment,\n",
    "                    'source': 'lexicon_augmented',\n",
    "                    'language': lang\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "aug_df = pd.DataFrame(aug_data)\n",
    "print(f\"‚úÖ Created {len(aug_df)} augmented sentences from lexicon\")\n",
    "print(f\"   Distribution: {aug_df['sentiment'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a86cbd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded corpus with 500 sentences\n",
      "‚úÖ Corpus split completed:\n",
      "   Training: 350 French sentences\n",
      "   Testing: 150 French sentences (held-out for evaluation)\n",
      "   ‚úÖ Added 350 translated sentences\n",
      "   Languages distribution:\n",
      "      - afrikaans: 96 sentences\n",
      "      - english: 87 sentences\n",
      "      - zulu: 84 sentences\n",
      "      - ciluba: 83 sentences\n",
      "\n",
      "   üìä Total training sentences: 700\n",
      "      - French (original): 350\n",
      "      - Translated: 350\n",
      "\n",
      "   Train sentiment distribution: {'positive': 664, 'negative': 34, 'neutral': 2}\n",
      "   Test sentiment distribution: {'positive': 142, 'negative': 7, 'neutral': 1}\n",
      "\n",
      "   üíæ Saved held-out test set to: corpus_test_split.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOADING CORPUS DATA AND SPLITTING TRAIN/TEST WE ALSO INCLUDE TRANSLATED SENTENCES\n",
    "# ================================\n",
    "\n",
    "# Load the processed corpus\n",
    "corpus_path = os.path.join(data_dir, 'test_corpus_processed.csv')\n",
    "if os.path.exists(corpus_path):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    corpus_df = pd.read_csv(corpus_path)\n",
    "    print(f\"‚úÖ Loaded corpus with {len(corpus_df)} sentences\")\n",
    "    \n",
    "    # Extract French sentences with sentiment_v2 labels\n",
    "    corpus_data = corpus_df[['sentence', 'sentiment_v2']].copy()\n",
    "    corpus_data.columns = ['text', 'sentiment']\n",
    "    corpus_data = corpus_data.dropna(subset=['text', 'sentiment'])\n",
    "    \n",
    "    # SPLIT: 70% train, 30% test (stratified by sentiment)\n",
    "    corpus_train, corpus_test, train_indices, test_indices = train_test_split(\n",
    "        corpus_data,\n",
    "        corpus_data.index,  # Keep track of indices for translation lookup\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=corpus_data['sentiment']\n",
    "    )\n",
    "    \n",
    "    corpus_train['source'] = 'corpus'\n",
    "    corpus_train['language'] = 'french'\n",
    "    \n",
    "    print(f\"‚úÖ Corpus split completed:\")\n",
    "    print(f\"   Training: {len(corpus_train)} French sentences\")\n",
    "    print(f\"   Testing: {len(corpus_test)} French sentences (held-out for evaluation)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # ADD TRANSLATED SENTENCES\n",
    "    # ========================================\n",
    "    \n",
    "    # Check if translated_text and target_language columns exist\n",
    "    if 'translated_text' in corpus_df.columns and 'target_language' in corpus_df.columns:\n",
    "        translated_train_data = []\n",
    "        \n",
    "        # For each training sample, get its translated version\n",
    "        for idx in train_indices:\n",
    "            row = corpus_df.loc[idx]\n",
    "            \n",
    "            # Check if translation exists and is not empty\n",
    "            translated_text = str(row.get('translated_text', '')).strip()\n",
    "            target_lang = str(row.get('target_language', '')).strip().lower()\n",
    "            sentiment = row.get('sentiment_v2', '')\n",
    "            \n",
    "            if translated_text and translated_text != 'nan' and len(translated_text) > 0:\n",
    "                # Only add if target language is different from French\n",
    "                if target_lang and target_lang != 'french':\n",
    "                    translated_train_data.append({\n",
    "                        'text': translated_text,\n",
    "                        'sentiment': sentiment,\n",
    "                        'source': 'corpus_translated',\n",
    "                        'language': target_lang\n",
    "                    })\n",
    "        \n",
    "        # Add translated sentences to training data\n",
    "        if translated_train_data:\n",
    "            corpus_train_translated = pd.DataFrame(translated_train_data)\n",
    "            corpus_train = pd.concat([corpus_train, corpus_train_translated], ignore_index=True)\n",
    "            \n",
    "            print(f\"   ‚úÖ Added {len(translated_train_data)} translated sentences\")\n",
    "            print(f\"   Languages distribution:\")\n",
    "            lang_counts = corpus_train_translated['language'].value_counts()\n",
    "            for lang, count in lang_counts.items():\n",
    "                print(f\"      - {lang}: {count} sentences\")\n",
    "            print(f\"\\n   üìä Total training sentences: {len(corpus_train)}\")\n",
    "            print(f\"      - French (original): {(corpus_train['source'] == 'corpus').sum()}\")\n",
    "            print(f\"      - Translated: {(corpus_train['source'] == 'corpus_translated').sum()}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid translations found in corpus\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è No 'translated_text' or 'target_language' columns found\")\n",
    "        print(f\"   Available columns: {corpus_df.columns.tolist()}\")\n",
    "        print(f\"   Continuing with French sentences only\")\n",
    "    \n",
    "    print(f\"\\n   Train sentiment distribution: {corpus_train['sentiment'].value_counts().to_dict()}\")\n",
    "    print(f\"   Test sentiment distribution: {corpus_test['sentiment'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Save test split for later evaluation\n",
    "    test_split_path = os.path.join(data_dir, 'corpus_test_split.csv')\n",
    "    corpus_test.to_csv(test_split_path, index=False)\n",
    "    print(f\"\\n   üíæ Saved held-out test set to: corpus_test_split.csv\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Corpus file not found at {corpus_path}\")\n",
    "    print(\"   Will use only lexicon data for training\")\n",
    "    corpus_train = pd.DataFrame(columns=['text', 'sentiment', 'source', 'language'])\n",
    "    corpus_test = pd.DataFrame(columns=['text', 'sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7883c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS ABOVE IS CREATING THREE TYPES OF DATA:\n",
    "#STORING LEXICON INDIVIDUAL WORDS\n",
    "#CREATING SENTENCES FROM LESXICON WORDS\n",
    "#USING CORPUS SENTENCES BOTH TRANSLATED AND ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a87bd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Combined training data created:\n",
      "   Total examples: 700\n",
      "   - Lexicon original: 0\n",
      "   - Lexicon augmented: 0\n",
      "   - Corpus (French): 350\n",
      "   - Corpus (Translated): 350\n",
      "\n",
      "   Language distribution:\n",
      "{'french': 350, 'afrikaans': 96, 'english': 87, 'zulu': 84, 'ciluba': 83}\n",
      "\n",
      "   Label distribution:\n",
      "label\n",
      "0     34\n",
      "1      2\n",
      "2    664\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Sample from each source:\n",
      "\n",
      "   corpus:\n",
      "      coude lit Dehors gal√®re captif... [french] ‚Üí positive\n",
      "      Courir Larme Doigt combo Poing... [french] ‚Üí positive\n",
      "\n",
      "   corpus_translated:\n",
      "      lukenyibu bulalu kuya dikenga mupika... [ciluba] ‚Üí positive\n",
      "      gijima izinyembezi umunwe isivalo inqindi... [zulu] ‚Üí positive\n",
      "\n",
      "‚úÖ Ready to train with 700 examples!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# COMBINE ALL TRAINING DATA SOURCES\n",
    "# ================================\n",
    "#TRAINING DATA - If we want to include lexicon singular words\n",
    "lexicon_original = train_df[['text', 'sentiment']].copy() # Individual words from lexicon and sentiment values: triste - negative\n",
    "lexicon_original['source'] = 'lexicon_original'  # Adds column called source to see where it came from: triste - negative - lexicon_original\n",
    "lexicon_original['language'] = 'mixed' # Add columns called language to indicate mixed languages\n",
    "\n",
    "# Takes data from sources and combines them into one big dataframe for training\n",
    "combined_train_df = pd.concat([\n",
    "    # lexicon_original, COMMENTED OUT: unless we want model to have context of specific words\n",
    "    # aug_df[['text', 'sentiment', 'source', 'language']], COMMENTED OUT: ~7,000 CREATED sentences from lexicon (uncomment to use)\n",
    "    corpus_train[['text', 'sentiment', 'source', 'language']] # Uses Corpus sentences with sentiment values\n",
    "], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "combined_train_df = combined_train_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# For each word/sentence it looks at the sentiment value and maps it to a number\n",
    "combined_train_df['label'] = combined_train_df['sentiment'].map(sentiment_to_label)\n",
    "combined_train_df = combined_train_df.dropna(subset=['label'])\n",
    "combined_train_df['label'] = combined_train_df['label'].astype(int)\n",
    "\n",
    "\n",
    "#JUST SOME LOGGING TO SEE THE DATA THE MODELS WILL USE\n",
    "print(f\"\\n‚úÖ Combined training data created:\")\n",
    "print(f\"   Total examples: {len(combined_train_df)}\")\n",
    "print(f\"   - Lexicon original: {(combined_train_df['source'] == 'lexicon_original').sum()}\")\n",
    "print(f\"   - Lexicon augmented: {(combined_train_df['source'] == 'lexicon_augmented').sum()}\")\n",
    "print(f\"   - Corpus (French): {(combined_train_df['source'] == 'corpus').sum()}\")\n",
    "print(f\"   - Corpus (Translated): {(combined_train_df['source'] == 'corpus_translated').sum()}\")\n",
    "print(f\"\\n   Language distribution:\")\n",
    "if 'language' in combined_train_df.columns:\n",
    "    print(combined_train_df['language'].value_counts().to_dict())\n",
    "print(f\"\\n   Label distribution:\")\n",
    "print(combined_train_df['label'].value_counts().sort_index())\n",
    "print(f\"\\n   Sample from each source:\")\n",
    "for src in combined_train_df['source'].unique():\n",
    "    sample = combined_train_df[combined_train_df['source'] == src].head(2)\n",
    "    print(f\"\\n   {src}:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        lang_info = f\" [{row.get('language', 'unknown')}]\" if 'language' in row else \"\"\n",
    "        print(f\"      {row['text'][:60]}...{lang_info} ‚Üí {row['sentiment']}\")\n",
    "\n",
    "# Update train_df to use combined data\n",
    "train_df = combined_train_df[['text', 'label']].copy()\n",
    "print(f\"\\n‚úÖ Ready to train with {len(train_df)} examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf92e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BELOW WE FINE TUNE & TRAIN AfroXLMR USING THE COMBINED TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e7acbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Training samples: 560\n",
      "Validation samples: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINE TUNE AfroXLMR - Getting everything ready to train AfroXLMR model\n",
    "# ================================\n",
    "\n",
    "# ==================================================================================\n",
    "#Import classes from hugging face transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "#To split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Evaluation metrix to test model performance\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "#Deep learning framework\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Create PyTorch Dataset it will accept texts (\"Je suis triste\", \"C'est beaucoup\") and labels (0, 1, 2)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Split data into train/validation\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Load AfroXLMR tokenizer and model\n",
    "afroxlmr_tokenizer = AutoTokenizer.from_pretrained(afroxlmr_model)\n",
    "afroxlmr_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    afroxlmr_model,\n",
    "    num_labels=3,  # negative, neutral, positive\n",
    "    id2label=label_to_sentiment,\n",
    "    label2id=sentiment_to_label\n",
    ")\n",
    "# ==================================================================================\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(X_train, y_train, afroxlmr_tokenizer)\n",
    "val_dataset = SentimentDataset(X_val, y_val, afroxlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2bc1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AfroXLMR training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 05:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237180</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.225907</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.235247</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AfroXLMR training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AfroXLMR Validation Results:\n",
      "   eval_loss: 0.2372\n",
      "   eval_accuracy: 0.9500\n",
      "   eval_f1: 0.9256\n",
      "   eval_precision: 0.9025\n",
      "   eval_recall: 0.9500\n",
      "   eval_runtime: 2.0722\n",
      "   eval_samples_per_second: 67.5610\n",
      "   eval_steps_per_second: 4.3430\n",
      "   epoch: 3.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Train AfroXLMR Model\n",
    "# ================================\n",
    "\n",
    "# Define metrics computation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "# Note: Using updated parameter names for newer transformers versions\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_afroxlmr',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer_afroxlmr = Trainer(\n",
    "    model=afroxlmr_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting AfroXLMR training...\")\n",
    "\n",
    "# Train the model\n",
    "trainer_afroxlmr.train()\n",
    "\n",
    "print(\"\\nAfroXLMR training completed!\")\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer_afroxlmr.evaluate()\n",
    "print(f\"\\nAfroXLMR Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a1697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BELOW I WILL VISUALISE AfroXLMR, the issue is the data is very biased mostly positive so the model will definetely reflect that:\n",
    "#Positive (label 2): 8,599 examples (83.4%)\n",
    "#Negative (label 0): 980 examples (9.5%)\n",
    "#Neutral (label 1): 875 examples (8.5%)\n",
    "\n",
    "#Maybe we use class weights, or use data augmentation to balance the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# AfroXLMR Sentiment Classifier - XAI, Ensemble, Evaluation, Data Loading\n",
    "# Adds methods required by the assignment rubric: attention extraction/visualization,\n",
    "# ensemble-ready probability/logit outputs, per-language evaluation, and lexicon/corpus loaders.\n",
    "# This cell defines the AfroXLMRSentimentClassifier class and an example usage block.\n",
    "# ================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AfroXLMRSentimentClassifier:\n",
    "    \"\"\"A wrapper around a HuggingFace AfroXLMR sequence classification model with:\n",
    "    - XAI methods (attention extraction / visualization / important token analysis)\n",
    "    - Ensemble-ready outputs (probabilities / logits)\n",
    "    - Detailed evaluation utilities (confusion matrix, per-language metrics)\n",
    "    - Static data loading helpers (lexicon & corpus)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name: str\n",
    "        HuggingFace model id to load (default: Davlan/afro-xlmr-base)\n",
    "    device: Optional[torch.device]\n",
    "        Torch device (auto-detected by default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'Davlan/afro-xlmr-base', device: Optional[torch.device] = None):\n",
    "        # Device setup\n",
    "        self.device = device if device is not None else (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Load tokenizer and model; enable attentions and hidden states for XAI\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        # Ensure the model outputs attentions & hidden states (rubric requirement)\n",
    "        self.model.config.output_attentions = True\n",
    "        self.model.config.output_hidden_states = True\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Try to get id2label mapping from model config; if missing provide default mapping\n",
    "        try:\n",
    "            self.id2label = {int(k): v for k, v in self.model.config.id2label.items()}\n",
    "        except Exception:\n",
    "            self.id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}\n",
    "\n",
    "    def _tokenize(self, texts: List[str], max_length: int = 128):\n",
    "        \"\"\"Tokenize a list of texts and move tensors to the model device.\n",
    "\n",
    "        Returns the tokenized batch (dict of tensors).\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "        return {k: v.to(self.device) for k, v in encoded.items()}\n",
    "\n",
    "    def predict(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Predict sentiment labels (readable) and confidence for a list of texts.\n",
    "\n",
    "        Returns a list of dicts: {text, sentiment, label, confidence, scores} where scores is a dict of class probabilities.\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        batch = self._tokenize(texts)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**batch)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        results = []\n",
    "        for i, text in enumerate(texts):\n",
    "            pred_label = int(np.argmax(probs[i]))\n",
    "            pred_sentiment = self.id2label.get(pred_label, str(pred_label))\n",
    "            scores = {self.id2label.get(idx, str(idx)): float(probs[i, idx]) for idx in range(probs.shape[1])}\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': pred_sentiment,\n",
    "                'label': pred_label,\n",
    "                'confidence': float(probs[i, pred_label]),\n",
    "                'scores': scores\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def predict_with_probabilities(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Return class probabilities for an input list of texts.\n",
    "\n",
    "        Returns a numpy array shape (n_texts, n_classes).\n",
    "        Useful for ensemble combination.\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        batch = self._tokenize(texts)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**batch)\n",
    "            logits = outputs.logits.cpu()\n",
    "            probs = F.softmax(logits, dim=-1).numpy()\n",
    "        return probs\n",
    "\n",
    "    def get_logits(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Return raw logits for the provided texts (ensemble use).\n",
    "\n",
    "        Returns numpy array shape (n_texts, n_classes).\n",
    "        \"\"\"\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        batch = self._tokenize(texts)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**batch)\n",
    "            logits = outputs.logits.cpu().numpy()\n",
    "        return logits\n",
    "\n",
    "    def get_attention_weights(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenize input text, run the model with attentions enabled and return token list,\n",
    "        attention tensors, prediction and sentiment string.\n",
    "\n",
    "        Returns dict: { 'tokens', 'attentions' (list of numpy arrays per layer), 'prediction', 'sentiment' }\n",
    "        Handles device transfers and single-text input.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError('text must be a single string')\n",
    "        encoded = self.tokenizer([text], return_tensors='pt', truncation=True, padding=True).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded)\n",
    "\n",
    "        # Extract attentions: tuple(num_layers) of tensors (batch, heads, seq, seq)\n",
    "        attentions = outputs.attentions\n",
    "        # Convert to cpu numpy for downstream use\n",
    "        attentions_np = [att.cpu().numpy() for att in attentions] if attentions is not None else []\n",
    "\n",
    "        # Tokens for display\n",
    "        input_ids = encoded['input_ids'].cpu().numpy()[0]\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "-1\n",
    ","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073913e",
   "metadata": {},
   "source": [
    "## 1. Import and Setup\n",
    "This section prepares the environment, loads required libraries, and instantiates the `AfroXLMRSentimentClassifier`.\n",
    "We will also ensure the VADER lexicon is available for the lexicon-based sentiment components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14df40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libs used in the new sections\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Instantiate classifier (uses the class defined earlier in the notebook)\n",
    "try:\n",
    "    clf = AfroXLMRSentimentClassifier()\n",
    "    print('AfroXLMRSentimentClassifier instantiated on device:', clf.device)\n",
    "except Exception as e:\n",
    "    print('Could not instantiate classifier (missing packages or offline). Error:', e)\n",
    "\n",
    "# Define default paths (adjust to your environment)\n",
    "LEXICON_PATH = 'expanded_lexicon.csv'\n",
    "CORPUS_PATH = 'shonasenti.csv'  # Example corpus file\n",
    "OUTPUT_DIR = './afroxlmr_trained'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24fb35a",
   "metadata": {},
   "source": [
    "## 2. Load Expanded Lexicon Data\n",
    "Load multilingual lexicon entries (zulu, xhosa, sepedi, shona, afrikaans, english).\n",
    "The `load_from_lexicon` static method will return parallel lists of texts, labels and language tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff283dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexicon using the classifier helper (falls back to sample data if file missing)\n",
    "if 'clf' in globals() and hasattr(clf, 'load_from_lexicon') and os.path.exists(LEXICON_PATH):\n",
    "    lex_texts, lex_labels, lex_langs = clf.load_from_lexicon(LEXICON_PATH, target_languages=['zulu','xhosa','sepedi','shona','afrikaans','english'])\n",
    "    print(f'Loaded lexicon samples: {len(lex_texts)}')\n",
    "else:\n",
    "    print('Lexicon file not found or classifier missing. Using demo lexicon entries.')\n",
    "    lex_texts = ['Ngiyajabula namhlanje', 'Ndiri kufara nekuti bhuku iri rakanaka', 'Ke a nyorilwe', 'Dit is baie sleg']\n",
    "    lex_labels = [2, 2, 0, 0]\n",
    "    lex_langs = ['zulu','shona','sepedi','afrikaans']\n",
    "\n",
    "# Quick sample preview\n",
    "for t, l, lg in zip(lex_texts[:5], lex_labels[:5], lex_langs[:5]):\n",
    "    print(f'[{lg}] {t} -> {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb258e3a",
   "metadata": {},
   "source": [
    "## 3. Load Corpus Data\n",
    "Load an optional corpus (e.g., ShonaSenti) via `load_from_corpus`. This is useful to provide sentence-level data for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus if available (fall back to empty lists)\n",
    "if 'clf' in globals() and hasattr(clf, 'load_from_corpus') and os.path.exists(CORPUS_PATH):\n",
    "    corp_texts, corp_labels, corp_langs = clf.load_from_corpus(CORPUS_PATH, text_column='text', label_column='sentiment', language_column='language')\n",
    "    print(f'Loaded corpus samples: {len(corp_texts)}')\n",
    "else:\n",
    "    print('Corpus file not found or classifier missing. Using empty corpus.')\n",
    "    corp_texts, corp_labels, corp_langs = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc7ac6",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Combination\n",
    "Combine lexicon and corpus data, deduplicate, and inspect distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ab516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "if 'clf' in globals() and hasattr(clf, 'combine_datasets') and len(lex_texts) > 0:\n",
    "    combo_texts, combo_labels, combo_langs = clf.combine_datasets((lex_texts, lex_labels, lex_langs), (corp_texts, corp_labels, corp_langs))\n",
    "else:\n",
    "    combo_texts, combo_labels, combo_langs = lex_texts, lex_labels, lex_langs\n",
    "\n",
    "print(f'Total combined samples: {len(combo_texts)}')\n",
    "from collections import Counter\n",
    "print('Label distribution:', Counter(combo_labels))\n",
    "print('Top languages:', Counter(combo_langs).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b9579",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "Fine-tune AfroXLMR on the combined multilingual dataset. The `train()` method in the class will handle training and logging.\n",
    "Note: training a transformer requires significant compute and time; the example below demonstrates the call and will run for a reduced number of epochs for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ef8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (demo). Increase num_epochs and batch_size for real runs.\n",
    "try:\n",
    "    if len(combo_texts) < 2:\n",
    "        raise ValueError('Not enough data to train; provide more samples or load a corpus.')\n",
    "    train_res = clf.train(combo_texts, combo_labels, num_epochs=1, batch_size=8, languages=combo_langs, output_dir=OUTPUT_DIR)\n",
    "    print('Training returned:', train_res)\n",
    "    # Save model+tokenizer for later use\n",
    "    try:\n",
    "        clf.model.save_pretrained(OUTPUT_DIR)\n",
    "        clf.tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        print('Saved model and tokenizer to', OUTPUT_DIR)\n",
    "    except Exception as e:\n",
    "        print('Could not save model locally:', e)\n",
    "except Exception as e:\n",
    "    print('Training skipped or failed (demo environment):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08eaa7",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation (overall and per-language)\n",
    "Use the classifier evaluation helpers to compute confusion matrices, classification reports, and per-language metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5187739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a test split (demo). For a real experiment, use a held-out corpus test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "if len(combo_texts) >= 4:\n",
    "    train_texts, test_texts, train_labels, test_labels, train_langs, test_langs = train_test_split(combo_texts, combo_labels, combo_langs, test_size=0.2, random_state=42, stratify=combo_labels if len(set(combo_labels))>1 else None)\n",
    "else:\n",
    "    test_texts, test_labels, test_langs = combo_texts, combo_labels, combo_langs\n",
    "\n",
    "# Detailed evaluation (confusion matrix + report)\n",
    "try:\n",
    "    eval_res = clf.evaluate_detailed(test_texts, test_labels, save_confusion_matrix=True)\n",
    "    print('Evaluation metrics:', {k: eval_res[k] for k in ['accuracy','precision','recall','f1']})\n",
    "except Exception as e:\n",
    "    print('Detailed evaluation failed:', e)\n",
    "\n",
    "# Per-language evaluation\n",
    "try:\n",
    "    per_lang = clf.evaluate_per_language(test_texts, test_labels, test_langs)\n",
    "    print('Per-language results keys:', list(per_lang.keys()))\n",
    "except Exception as e:\n",
    "    print('Per-language evaluation failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cea197",
   "metadata": {},
   "source": [
    "## 7. XAI Analysis and Visualization\n",
    "Generate attention visualizations for sample texts across different languages and extract top important words per prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a910b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample XAI demonstrations (saves images with dpi=300)\n",
    "xai_samples = [\n",
    "    ('Ngiyajabula namhlanje','zulu'),\n",
    "    ('Ndiri kufara nekuti bhuku iri rakanaka','shona'),\n",
    "    ('Ke a nyorilwe','sepedi'),\n",
    "    ('Dit is baie sleg','afrikaans')\n",
    "]\n",
    "for i, (text, lang) in enumerate(xai_samples):\n",
    "    try:\n",
    "        info = clf.analyze_important_words(text, top_k=3)\n",
    "        print(f'XAI sample {i+1} ({lang}) predicted: {info[sentiment]} - top words:', info['top_words'])\n",
    "        img_path = f'afroxlmr_attention_{lang}_{i+1}.png'\n",
    "        try:\n",
    "            clf.visualize_attention(text, layer=-1, head=0, save_path=img_path)\n",
    "            print('Saved attention visualization to', img_path)\n",
    "        except Exception as e:\n",
    "            print('Could not render attention heatmap for sample:', e)\n",
    "    except Exception as e:\n",
    "        print('XAI step failed for sample:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc455d5",
   "metadata": {},
   "source": [
    "## 8. Aspect-Based Sentiment Analysis (ABSA) Demo\n",
    "A small demonstration that extracts sentiments for specific aspects/keywords within a text using sentence splitting and focused predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_aspects(text: str, aspects: List[str]):\n",
    "    \"\"\"Return sentiment for individual aspect mentions in the text.\n",
    "\n",
    "    This simple demo looks for aspect keywords and predicts sentiment on the full text (you can refine to span-level).\n",
    "    \"\"\"\n",
    "    found = []\n",
    "    for aspect in aspects:\n",
    "        if aspect.lower() in text.lower():\n",
    "            pred = clf.predict([text])[0]\n",
    "            found.append({'aspect': aspect, 'sentiment': pred['sentiment'], 'confidence': pred['confidence']})\n",
    "    return found\n",
    "\n",
    "# Demo aspects and texts\n",
    "demo_texts = [\n",
    "    ('Ngiyathokoza ngomsebenzi wezikole, kodwa izindawo zokuhlala zimele ukuthuthukiswa', ['work','housing']),\n",
    "    ('Bhuku rakanaka asi mutengo wakaipisisa', ['book','price'])\n",
    "]\n",
    "for txt, aspects in demo_texts:\n",
    "    try:\n",
    "        res = analyze_aspects(txt, aspects)\n",
    "        print('Text:', txt)\n",
    "        print('Aspect analysis:', res)\n",
    "    except Exception as e:\n",
    "        print('ABSA demo failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6f773",
   "metadata": {},
   "source": [
    "## 9. Ensemble Output Preparation\n",
    "Extract probabilities and logits in a format ready to be combined with AfriBERTa or other model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: prepare ensemble-ready CSV for a small set of texts\n",
    "ensemble_texts = ['Ek is baie bly vandag', 'Le mosebetsi ke mobe']\n",
    "try:\n",
    "    probs = clf.predict_with_probabilities(ensemble_texts)\n",
    "    logits = clf.get_logits(ensemble_texts)\n",
    "    import numpy as np\n",
    "    df_ens = pd.DataFrame(probs, columns=[clf.id2label[i] for i in range(probs.shape[1])])\n",
    "    df_ens['text'] = ensemble_texts\n",
    "    df_ens['logit_0'] = logits[:,0]\n",
    "    df_ens['logit_1'] = logits[:,1]\n",
    "    df_ens['logit_2'] = logits[:,2]\n",
    "    ensemble_out_path = os.path.join(OUTPUT_DIR, 'afroxlmr_ensemble_ready.csv')\n",
    "    df_ens.to_csv(ensemble_out_path, index=False)\n",
    "    print('Saved ensemble-ready outputs to', ensemble_out_path)\n",
    "except Exception as e:\n",
    "    print('Ensemble preparation skipped/failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122ca9f",
   "metadata": {},
   "source": [
    "## 10. Results Summary\n",
    "Summarize training and evaluation results, and point to saved artifacts (model, confusion matrix images, attention visualizations, and ensemble CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb539bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== RESULTS SUMMARY ===')\n",
    "print('Model directory:', OUTPUT_DIR)\n",
    "print('Saved confusion matrix: afroxlmr_confusion_matrix.png (if evaluation ran)')\n",
    "print('Saved attention images: afroxlmr_attention_*.png (if XAI ran)')\n",
    "print('Saved ensemble CSV:', os.path.join(OUTPUT_DIR, 'afroxlmr_ensemble_ready.csv'))\n",
    "print('Demo complete. For production runs increase data size, epochs, and batch size; run on a GPU-enabled environment.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
