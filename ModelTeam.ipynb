{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d0814f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msentiment\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Download VADER lexicon\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.notebook import tqdm  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Download VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "data_dir = r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\"\n",
    "for dirname, _, filenames in os.walk(data_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "file_path = os.path.join(data_dir, 'corrected_lexicon.xlsx')\n",
    "print('Using file_path:', file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd7af89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx with shape (3234, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CILUBA</th>\n",
       "      <th>FRANCAIS</th>\n",
       "      <th>ENGLISH</th>\n",
       "      <th>AFRIKAANS</th>\n",
       "      <th>ZULU</th>\n",
       "      <th>Sepedi</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>NATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>umue</td>\n",
       "      <td>un</td>\n",
       "      <td>a</td>\n",
       "      <td>N</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>nombre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biabunyi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>ga nt≈°i</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bungi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>kudu</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>adverbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dilekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>verlating</td>\n",
       "      <td>ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     CILUBA  FRANCAIS      ENGLISH  AFRIKAANS       ZULU       Sepedi  SCORE  \\\n",
       "0      umue        un            a          N          I          NaN      0   \n",
       "1  Biabunyi  beaucoup        a lot       baie   okuningi      ga nt≈°i      3   \n",
       "2     bungi  beaucoup        a lot       baie   okuningi         kudu      0   \n",
       "3  dilekela   abandon  abandonment  Verlating  Ukulahlwa  hlokomologa      4   \n",
       "4  Kulekela   abandon  abandonment  verlating  ukulahlwa  hlokomologa      3   \n",
       "\n",
       "  SENTIMENT   NATURE  \n",
       "0    Neutre   nombre  \n",
       "1   Positif      mot  \n",
       "2    Neutre  adverbe  \n",
       "3   Positif      mot  \n",
       "4   Positif      mot  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local path for corrected lexicon\n",
    "file_path = r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\\corrected_lexicon.xlsx\"\n",
    "\n",
    "# Load the dataset (guard against missing file)\n",
    "if os.path.exists(file_path):\n",
    "    df1 = pd.read_excel(file_path)\n",
    "    print(f\"Loaded {file_path} with shape {df1.shape}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Expected lexicon file not found at: {file_path}\")\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a131727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ciluba</th>\n",
       "      <th>french</th>\n",
       "      <th>english</th>\n",
       "      <th>afrikaans</th>\n",
       "      <th>zulu</th>\n",
       "      <th>sepedi</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>nature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biabunyi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>ga nt≈°i</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bungi</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>a lot</td>\n",
       "      <td>baie</td>\n",
       "      <td>okuningi</td>\n",
       "      <td>kudu</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutre</td>\n",
       "      <td>adverbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dilekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>verlating</td>\n",
       "      <td>ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>3</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>Verlating</td>\n",
       "      <td>Ukulahlwa</td>\n",
       "      <td>hlokomologa</td>\n",
       "      <td>4</td>\n",
       "      <td>Positif</td>\n",
       "      <td>mot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>Ya Bunda</td>\n",
       "      <td>construire</td>\n",
       "      <td>build</td>\n",
       "      <td>Bou</td>\n",
       "      <td>Yakha</td>\n",
       "      <td>aga</td>\n",
       "      <td>5</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>Ya ku leta</td>\n",
       "      <td>faire venir</td>\n",
       "      <td>bring</td>\n",
       "      <td>Bring</td>\n",
       "      <td>Letha</td>\n",
       "      <td>tli≈°a</td>\n",
       "      <td>4</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>Yamba</td>\n",
       "      <td>prendre</td>\n",
       "      <td>take</td>\n",
       "      <td>Neem</td>\n",
       "      <td>Thatha</td>\n",
       "      <td>t≈°eya</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>Yeleka</td>\n",
       "      <td>esp√©rer</td>\n",
       "      <td>hope</td>\n",
       "      <td>Hoop</td>\n",
       "      <td>Themba</td>\n",
       "      <td>tshepo</td>\n",
       "      <td>9</td>\n",
       "      <td>Positive</td>\n",
       "      <td>verbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>Yeye</td>\n",
       "      <td>il</td>\n",
       "      <td>he</td>\n",
       "      <td>Hy</td>\n",
       "      <td>Yena</td>\n",
       "      <td>yena</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>pronompersonnel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2957 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ciluba       french      english  afrikaans       zulu       sepedi  \\\n",
       "1       Biabunyi     beaucoup        a lot       baie   okuningi      ga nt≈°i   \n",
       "2          bungi     beaucoup        a lot       baie   okuningi         kudu   \n",
       "3       dilekela      abandon  abandonment  Verlating  Ukulahlwa  hlokomologa   \n",
       "4       Kulekela      abandon  abandonment  verlating  ukulahlwa  hlokomologa   \n",
       "5       kulekela      abandon  abandonment  Verlating  Ukulahlwa  hlokomologa   \n",
       "...          ...          ...          ...        ...        ...          ...   \n",
       "3200    Ya Bunda   construire        build        Bou      Yakha          aga   \n",
       "3201  Ya ku leta  faire venir        bring      Bring      Letha        tli≈°a   \n",
       "3202       Yamba      prendre         take       Neem     Thatha        t≈°eya   \n",
       "3205      Yeleka      esp√©rer         hope       Hoop     Themba       tshepo   \n",
       "3207        Yeye           il           he         Hy       Yena         yena   \n",
       "\n",
       "      score sentiment           nature  \n",
       "1         3   Positif              mot  \n",
       "2         0    Neutre          adverbe  \n",
       "3         4   Positif              mot  \n",
       "4         3   Positif              mot  \n",
       "5         4   Positif              mot  \n",
       "...     ...       ...              ...  \n",
       "3200      5  Positive            verbe  \n",
       "3201      4  Positive            verbe  \n",
       "3202      3  Positive            verbe  \n",
       "3205      9  Positive            verbe  \n",
       "3207      0   Neutral  pronompersonnel  \n",
       "\n",
       "[2957 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns\n",
    "df1.columns = ['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'score', 'sentiment', 'nature']\n",
    "\n",
    "# Drop duplicate rows based on language columns + sentiment (or all columns if you prefer)\n",
    "df = df1.drop_duplicates(subset=['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'sentiment'])\n",
    "\n",
    "# Drop rows with missing values in the language columns + sentiment\n",
    "df = df1.dropna(subset=['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'sentiment'])\n",
    "\n",
    "# Now you can proceed to vectorize, reduce and plot based on these language columns as separate features or however you want.\n",
    "# For example, you could vectorize each language column separately or combine them as needed.\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Translation dictionaries created.\n",
      "‚úÖ Sentiment scores calculated.\n",
      "Test corpus columns: ['source_language', 'target_language', 'sentence']\n",
      "Processing 500 rows (of 2999)\n",
      "‚úÖ Sentiment analysis applied to test corpus (sample).\n",
      "üìä Preview of sentiment analysis results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>total_score_avg</th>\n",
       "      <th>word_scores_avg</th>\n",
       "      <th>sentiment_avg</th>\n",
       "      <th>total_score_v2</th>\n",
       "      <th>word_scores_v2</th>\n",
       "      <th>sentiment_v2</th>\n",
       "      <th>vader_positive</th>\n",
       "      <th>vader_negative</th>\n",
       "      <th>vader_neutral</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>custom_sentiment_numeric</th>\n",
       "      <th>vader_sentiment_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Arrange pagne proteger Comportement Seulement</td>\n",
       "      <td>arrange loincloth protect behavior only</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>arrange:1.0; pagne:2.0; proteger:3.0; comporte...</td>\n",
       "      <td>positive</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>arrange:1; pagne:3; proteger:3; comportement:1...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Rearrange mordre purifier V√©rit√© bourse</td>\n",
       "      <td>akajilula kusuma kutokesha bulelela tshibombu</td>\n",
       "      <td>8.566667</td>\n",
       "      <td>rearrange:1.0; mordre:-2.0; purifier:3.6666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9.066667</td>\n",
       "      <td>rearrange:1; mordre:-2; purifier:3.66666666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Parle ais√© Serpent M√®re Abhorrer</td>\n",
       "      <td>praat maklik slang moeder verafsku</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>parle:2.0; ais√©:3.0; serpent:-2.25; m√®re:2.25;...</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>parle:2; ais√©:3; serpent:-4.0; m√®re:2.25; abho...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Parler √† nouveau murmurer chanter Sein castrer</td>\n",
       "      <td>khuluma futhi nyenyeza cula isibele xholosa</td>\n",
       "      <td>16.642857</td>\n",
       "      <td>parler √† nouveau:2.0; murmurer:4.0; chanter:3....</td>\n",
       "      <td>positive</td>\n",
       "      <td>16.642857</td>\n",
       "      <td>parler √† nouveau:2; murmurer:4; chanter:3.1428...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Remet D√©chirure Infid√®le √âtrangler Kubela</td>\n",
       "      <td>put back tear unfaithful strangle kubela</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>remet:3.2; d√©chirure:-4.0; infid√®le:-5.0; √©tra...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1.800000</td>\n",
       "      <td>remet:3.2; d√©chirure:-4; infid√®le:-5; √©trangle...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Dis F√©tiche finir R√™ver corps</td>\n",
       "      <td>amba manga tshinda kulota mubidimbidi</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>dis:3.0; f√©tiche:-3.0; finir:1.333333333333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9.380952</td>\n",
       "      <td>dis:3; f√©tiche:-3.0; finir:4.0; r√™ver:2.666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Superposer Preparer bruit c√©r√©moniecoutumi√®re ...</td>\n",
       "      <td>superponeer voorberei geraas gebruiklike serem...</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>superposer:2.2; preparer:3.4; bruit:2.66666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>13.266667</td>\n",
       "      <td>superposer:2.2; preparer:3.4; bruit:2.66666666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Ramasse parfaite voler cueillire trente-cinq</td>\n",
       "      <td>phakamisa gweda ndiza ukhethiwe trentecinq</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>ramasse:4.0; parfaite:3.0; voler:-4.2; cueilli...</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>ramasse:4; parfaite:4; voler:-4.2; cueillire:1...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>D√©p√™che maladie chapeau rouler repasser</td>\n",
       "      <td>dispatch illness hat to roll go back</td>\n",
       "      <td>9.550000</td>\n",
       "      <td>d√©p√™che:4.0; maladie:-0.7; chapeau:2.75; roule...</td>\n",
       "      <td>positive</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>d√©p√™che:4; maladie:-2.5; chapeau:2.75; rouler:...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>R√©p√®te Gal√®re Expliquer trasformer voler</td>\n",
       "      <td>ambulula dikenga kuvuija kukudimuna kuiba</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>R√©pond Mouche Plaie acheter onze</td>\n",
       "      <td>antwoorde vlieg wond koop elf</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>r√©pond:5.5; mouche:3.0; plaie:3.0; acheter:2.0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>r√©pond:9; mouche:3; plaie:3; acheter:2.0; onze:0</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Prend L√©ger Histoire Brouillard Exister</td>\n",
       "      <td>thatha mhlophe umlando inkungu hlala</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>prend:6.0; l√©ger:3.0; histoire:4.0; brouillard...</td>\n",
       "      <td>positive</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>prend:6.0; l√©ger:3; histoire:4; brouillard:1.6...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Reprend Pr√©dication cinquante-wight Accomplir ...</td>\n",
       "      <td>resume preaching cinquantewight accomplish put...</td>\n",
       "      <td>18.700000</td>\n",
       "      <td>reprend:9.0; pr√©dication:2.0; cinquantewight:0...</td>\n",
       "      <td>positive</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>reprend:9; pr√©dication:3; cinquantewight:0; ac...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Habits S√©rieux Lune chemise diminuer</td>\n",
       "      <td>bilamba cilongama ngondu mutelu kukeresha</td>\n",
       "      <td>11.916667</td>\n",
       "      <td>habits:5.0; s√©rieux:3.0; lune:2.25; chemise:2....</td>\n",
       "      <td>positive</td>\n",
       "      <td>12.916667</td>\n",
       "      <td>habits:8; s√©rieux:3; lune:2.25; chemise:2.6666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Blague R√©sultats(R√©compense) ouvrir Droite Adm...</td>\n",
       "      <td>grap r√©sultatsr√©compense oopmaak regs bewonder</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>blague:5.5; r√©sultatsr√©compense:0; ouvrir:2.0;...</td>\n",
       "      <td>positive</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>blague:8; r√©sultatsr√©compense:0; ouvrir:2; dro...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>V√™tements Intelligence tout c√©r√©moniecoutumi√®r...</td>\n",
       "      <td>izingubo ubuhlakani konke umkhosi owesiko iphe...</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>v√™tements:0.3333333333333333; intelligence:2.5...</td>\n",
       "      <td>positive</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>v√™tements:2.0; intelligence:2.5; tout:1; c√©r√©m...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>french</td>\n",
       "      <td>english</td>\n",
       "      <td>Larmes ventre capitaine Raison Fuir</td>\n",
       "      <td>tears belly captain reason flee</td>\n",
       "      <td>10.333333</td>\n",
       "      <td>larmes:0.3333333333333333; ventre:3.0; capitai...</td>\n",
       "      <td>positive</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>larmes:-3.0; ventre:3.0; capitaine:2.5; raison...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>french</td>\n",
       "      <td>ciluba</td>\n",
       "      <td>Entre manger deuil babouche Eclair</td>\n",
       "      <td>buela kuja madilu mapapa mupenyimukenya</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>entre:5.0; manger:1.7777777777777777; deuil:-6...</td>\n",
       "      <td>positive</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>entre:7; manger:2.0; deuil:-6; babouche:2.6666...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>french</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>Lumi√®re chemise miroir ardeur cheveux</td>\n",
       "      <td>lig hemp spie√´l ywer hare</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>lumi√®re:2.5; chemise:2.6666666666666665; miroi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>lumi√®re:2.5; chemise:2.6666666666666665; miroi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>french</td>\n",
       "      <td>zulu</td>\n",
       "      <td>Terre Couleur justification copier absence</td>\n",
       "      <td>umhlaba umbala ukuthethelela isizathu esizwaka...</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>terre:2.0; couleur:1.6666666666666667; justifi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>terre:3.0; couleur:1.6666666666666667; justifi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_language target_language  \\\n",
       "0           french         english   \n",
       "1           french          ciluba   \n",
       "2           french       afrikaans   \n",
       "3           french            zulu   \n",
       "4           french         english   \n",
       "5           french          ciluba   \n",
       "6           french       afrikaans   \n",
       "7           french            zulu   \n",
       "8           french         english   \n",
       "9           french          ciluba   \n",
       "10          french       afrikaans   \n",
       "11          french            zulu   \n",
       "12          french         english   \n",
       "13          french          ciluba   \n",
       "14          french       afrikaans   \n",
       "15          french            zulu   \n",
       "16          french         english   \n",
       "17          french          ciluba   \n",
       "18          french       afrikaans   \n",
       "19          french            zulu   \n",
       "\n",
       "                                             sentence  \\\n",
       "0       Arrange pagne proteger Comportement Seulement   \n",
       "1             Rearrange mordre purifier V√©rit√© bourse   \n",
       "2                    Parle ais√© Serpent M√®re Abhorrer   \n",
       "3      Parler √† nouveau murmurer chanter Sein castrer   \n",
       "4           Remet D√©chirure Infid√®le √âtrangler Kubela   \n",
       "5                       Dis F√©tiche finir R√™ver corps   \n",
       "6   Superposer Preparer bruit c√©r√©moniecoutumi√®re ...   \n",
       "7        Ramasse parfaite voler cueillire trente-cinq   \n",
       "8             D√©p√™che maladie chapeau rouler repasser   \n",
       "9            R√©p√®te Gal√®re Expliquer trasformer voler   \n",
       "10                   R√©pond Mouche Plaie acheter onze   \n",
       "11            Prend L√©ger Histoire Brouillard Exister   \n",
       "12  Reprend Pr√©dication cinquante-wight Accomplir ...   \n",
       "13               Habits S√©rieux Lune chemise diminuer   \n",
       "14  Blague R√©sultats(R√©compense) ouvrir Droite Adm...   \n",
       "15  V√™tements Intelligence tout c√©r√©moniecoutumi√®r...   \n",
       "16                Larmes ventre capitaine Raison Fuir   \n",
       "17                 Entre manger deuil babouche Eclair   \n",
       "18              Lumi√®re chemise miroir ardeur cheveux   \n",
       "19         Terre Couleur justification copier absence   \n",
       "\n",
       "                                      translated_text  total_score_avg  \\\n",
       "0             arrange loincloth protect behavior only        10.600000   \n",
       "1       akajilula kusuma kutokesha bulelela tshibombu         8.566667   \n",
       "2                  praat maklik slang moeder verafsku         9.000000   \n",
       "3         khuluma futhi nyenyeza cula isibele xholosa        16.642857   \n",
       "4            put back tear unfaithful strangle kubela        -1.800000   \n",
       "5               amba manga tshinda kulota mubidimbidi         6.714286   \n",
       "6   superponeer voorberei geraas gebruiklike serem...        13.266667   \n",
       "7          phakamisa gweda ndiza ukhethiwe trentecinq         3.800000   \n",
       "8                dispatch illness hat to roll go back         9.550000   \n",
       "9           ambulula dikenga kuvuija kukudimuna kuiba         0.183333   \n",
       "10                      antwoorde vlieg wond koop elf        13.500000   \n",
       "11               thatha mhlophe umlando inkungu hlala        18.666667   \n",
       "12  resume preaching cinquantewight accomplish put...        18.700000   \n",
       "13          bilamba cilongama ngondu mutelu kukeresha        11.916667   \n",
       "14     grap r√©sultatsr√©compense oopmaak regs bewonder        15.000000   \n",
       "15  izingubo ubuhlakani konke umkhosi owesiko iphe...         9.333333   \n",
       "16                    tears belly captain reason flee        10.333333   \n",
       "17            buela kuja madilu mapapa mupenyimukenya         4.444444   \n",
       "18                          lig hemp spie√´l ywer hare        14.777778   \n",
       "19  umhlaba umbala ukuthethelela isizathu esizwaka...        12.666667   \n",
       "\n",
       "                                      word_scores_avg sentiment_avg  \\\n",
       "0   arrange:1.0; pagne:2.0; proteger:3.0; comporte...      positive   \n",
       "1   rearrange:1.0; mordre:-2.0; purifier:3.6666666...      positive   \n",
       "2   parle:2.0; ais√©:3.0; serpent:-2.25; m√®re:2.25;...      positive   \n",
       "3   parler √† nouveau:2.0; murmurer:4.0; chanter:3....      positive   \n",
       "4   remet:3.2; d√©chirure:-4.0; infid√®le:-5.0; √©tra...      negative   \n",
       "5   dis:3.0; f√©tiche:-3.0; finir:1.333333333333333...      positive   \n",
       "6   superposer:2.2; preparer:3.4; bruit:2.66666666...      positive   \n",
       "7   ramasse:4.0; parfaite:3.0; voler:-4.2; cueilli...      positive   \n",
       "8   d√©p√™che:4.0; maladie:-0.7; chapeau:2.75; roule...      positive   \n",
       "9   r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...      positive   \n",
       "10  r√©pond:5.5; mouche:3.0; plaie:3.0; acheter:2.0...      positive   \n",
       "11  prend:6.0; l√©ger:3.0; histoire:4.0; brouillard...      positive   \n",
       "12  reprend:9.0; pr√©dication:2.0; cinquantewight:0...      positive   \n",
       "13  habits:5.0; s√©rieux:3.0; lune:2.25; chemise:2....      positive   \n",
       "14  blague:5.5; r√©sultatsr√©compense:0; ouvrir:2.0;...      positive   \n",
       "15  v√™tements:0.3333333333333333; intelligence:2.5...      positive   \n",
       "16  larmes:0.3333333333333333; ventre:3.0; capitai...      positive   \n",
       "17  entre:5.0; manger:1.7777777777777777; deuil:-6...      positive   \n",
       "18  lumi√®re:2.5; chemise:2.6666666666666665; miroi...      positive   \n",
       "19  terre:2.0; couleur:1.6666666666666667; justifi...      positive   \n",
       "\n",
       "    total_score_v2                                     word_scores_v2  \\\n",
       "0        11.600000  arrange:1; pagne:3; proteger:3; comportement:1...   \n",
       "1         9.066667  rearrange:1; mordre:-2; purifier:3.66666666666...   \n",
       "2         7.250000  parle:2; ais√©:3; serpent:-4.0; m√®re:2.25; abho...   \n",
       "3        16.642857  parler √† nouveau:2; murmurer:4; chanter:3.1428...   \n",
       "4        -1.800000  remet:3.2; d√©chirure:-4; infid√®le:-5; √©trangle...   \n",
       "5         9.380952  dis:3; f√©tiche:-3.0; finir:4.0; r√™ver:2.666666...   \n",
       "6        13.266667  superposer:2.2; preparer:3.4; bruit:2.66666666...   \n",
       "7         4.800000  ramasse:4; parfaite:4; voler:-4.2; cueillire:1...   \n",
       "8         8.250000  d√©p√™che:4; maladie:-2.5; chapeau:2.75; rouler:...   \n",
       "9         0.183333  r√©p√®te:2.8; gal√®re:-3.75; expliquer:4.33333333...   \n",
       "10       17.000000   r√©pond:9; mouche:3; plaie:3; acheter:2.0; onze:0   \n",
       "11       18.666667  prend:6.0; l√©ger:3; histoire:4; brouillard:1.6...   \n",
       "12       19.700000  reprend:9; pr√©dication:3; cinquantewight:0; ac...   \n",
       "13       12.916667  habits:8; s√©rieux:3; lune:2.25; chemise:2.6666...   \n",
       "14       17.500000  blague:8; r√©sultatsr√©compense:0; ouvrir:2; dro...   \n",
       "15       11.500000  v√™tements:2.0; intelligence:2.5; tout:1; c√©r√©m...   \n",
       "16        8.500000  larmes:-3.0; ventre:3.0; capitaine:2.5; raison...   \n",
       "17        6.666667  entre:7; manger:2.0; deuil:-6; babouche:2.6666...   \n",
       "18       14.777778  lumi√®re:2.5; chemise:2.6666666666666665; miroi...   \n",
       "19       15.666667  terre:3.0; couleur:1.6666666666666667; justifi...   \n",
       "\n",
       "   sentiment_v2  vader_positive  vader_negative  vader_neutral  \\\n",
       "0      positive           0.000           0.000          1.000   \n",
       "1      positive           0.000           0.000          1.000   \n",
       "2      positive           0.000           0.000          1.000   \n",
       "3      positive           0.000           0.000          1.000   \n",
       "4      negative           0.000           0.000          1.000   \n",
       "5      positive           0.000           0.000          1.000   \n",
       "6      positive           0.000           0.000          1.000   \n",
       "7      positive           0.000           0.000          1.000   \n",
       "8      positive           0.000           0.000          1.000   \n",
       "9      positive           0.000           0.000          1.000   \n",
       "10     positive           0.000           0.000          1.000   \n",
       "11     positive           0.000           0.000          1.000   \n",
       "12     positive           0.000           0.000          1.000   \n",
       "13     positive           0.000           0.000          1.000   \n",
       "14     positive           0.412           0.000          0.588   \n",
       "15     positive           0.408           0.197          0.395   \n",
       "16     positive           0.000           0.000          1.000   \n",
       "17     positive           0.000           0.000          1.000   \n",
       "18     positive           0.000           0.000          1.000   \n",
       "19     positive           0.000           0.000          1.000   \n",
       "\n",
       "    vader_compound vader_sentiment  custom_sentiment_numeric  \\\n",
       "0           0.0000         neutral                         1   \n",
       "1           0.0000         neutral                         1   \n",
       "2           0.0000         neutral                         1   \n",
       "3           0.0000         neutral                         1   \n",
       "4           0.0000         neutral                        -1   \n",
       "5           0.0000         neutral                         1   \n",
       "6           0.0000         neutral                         1   \n",
       "7           0.0000         neutral                         1   \n",
       "8           0.0000         neutral                         1   \n",
       "9           0.0000         neutral                         1   \n",
       "10          0.0000         neutral                         1   \n",
       "11          0.0000         neutral                         1   \n",
       "12          0.0000         neutral                         1   \n",
       "13          0.0000         neutral                         1   \n",
       "14          0.4215        positive                         1   \n",
       "15          0.3818        positive                         1   \n",
       "16          0.0000         neutral                         1   \n",
       "17          0.0000         neutral                         1   \n",
       "18          0.0000         neutral                         1   \n",
       "19          0.0000         neutral                         1   \n",
       "\n",
       "    vader_sentiment_numeric  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "5                         0  \n",
       "6                         0  \n",
       "7                         0  \n",
       "8                         0  \n",
       "9                         0  \n",
       "10                        0  \n",
       "11                        0  \n",
       "12                        0  \n",
       "13                        0  \n",
       "14                        1  \n",
       "15                        1  \n",
       "16                        0  \n",
       "17                        0  \n",
       "18                        0  \n",
       "19                        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# STEP 1: Load Data\n",
    "# ================================\n",
    "# Local files in data_dir set earlier in cell 1 \n",
    "lexicon_path = os.path.join(r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\", 'corrected_lexicon.xlsx')\n",
    "french_test_path = os.path.join(r\"C:\\Users\\User\\Desktop\\Assignment 3 Resources\", 'french_test_corpus.xlsx')\n",
    "\n",
    "df = pd.read_excel(lexicon_path)\n",
    "test_corpus_df = pd.read_excel(french_test_path)\n",
    "\n",
    "# ================================\n",
    "# STEP 2: Preprocess Lexicon\n",
    "# ================================\n",
    "# Rename columns\n",
    "df.columns = ['ciluba', 'french', 'english', 'afrikaans', 'zulu', 'sepedi', 'score', 'sentiment', 'nature']\n",
    "\n",
    "# Define languages\n",
    "supported_languages = ['french', 'afrikaans', 'zulu', 'ciluba', 'sepedi', 'english']\n",
    "\n",
    "# ================================\n",
    "# STEP 3: Translation Dictionary\n",
    "# ================================\n",
    "def clean_text_for_matching(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))\n",
    "    return text.lower()\n",
    "\n",
    "def create_translation_dicts(df, languages):\n",
    "    translation_dicts = {src: {tgt: {} for tgt in languages if tgt != src} for src in languages}\n",
    "    for _, row in df.iterrows():\n",
    "        for src in languages:\n",
    "            source_phrase = str(row[src]).strip().lower()\n",
    "            if pd.isna(source_phrase) or source_phrase == '':\n",
    "                continue\n",
    "            for tgt in languages:\n",
    "                if tgt == src:\n",
    "                    continue\n",
    "                target_phrase = str(row[tgt]).strip().lower()\n",
    "                if pd.isna(target_phrase) or target_phrase == '':\n",
    "                    continue\n",
    "                translation_dicts[src][tgt][source_phrase] = target_phrase\n",
    "    return translation_dicts\n",
    "\n",
    "translation_dicts = create_translation_dicts(df, supported_languages)\n",
    "print(\"‚úÖ Translation dictionaries created.\")\n",
    "\n",
    "# ================================\n",
    "# STEP 4: Sentiment Scores\n",
    "# ================================\n",
    "sentiment_averages = {}\n",
    "all_sentiments = {}\n",
    "\n",
    "for lang in supported_languages:\n",
    "    sentiment_averages[lang] = df.groupby(lang)['score'].mean().to_dict()\n",
    "    all_sentiments[lang] = df.groupby(lang)['score'].apply(list).to_dict()\n",
    "\n",
    "print(\"‚úÖ Sentiment scores calculated.\")\n",
    "\n",
    "# ================================\n",
    "# STEP 5: Custom Sentiment Function\n",
    "# ================================\n",
    "def compute_sentiment_v2(scores):\n",
    "    if len(scores) == 1:\n",
    "        return scores[0]\n",
    "    elif len(scores) == 2:\n",
    "        return max(scores, key=abs)\n",
    "    else:\n",
    "        pos = [s for s in scores if s > 0]\n",
    "        neg = [s for s in scores if s < 0]\n",
    "        if len(pos) >= len(neg):\n",
    "            return sum(pos) / len(pos) if pos else 0\n",
    "        else:\n",
    "            return sum(neg) / len(neg) if neg else 0\n",
    "\n",
    "# ================================\n",
    "# STEP 6: Translate & Analyze Function\n",
    "# ================================\n",
    "def translate_analyze_sentiments_with_vader(text, source_lang, target_lang,\n",
    "                                            translation_dicts, sentiment_averages, all_sentiments,\n",
    "                                            vader_analyzer):\n",
    "    source_lang = str(source_lang).lower()\n",
    "    target_lang = str(target_lang).lower()\n",
    "\n",
    "    if source_lang not in translation_dicts or target_lang not in translation_dicts[source_lang]:\n",
    "        return {k: '' if isinstance(v, str) else 0 for k, v in {\n",
    "            \"translated_text\": \"\",\n",
    "            \"total_score_avg\": 0,\n",
    "            \"word_scores_avg\": \"\",\n",
    "            \"sentiment_avg\": \"neutral\",\n",
    "            \"total_score_v2\": 0,\n",
    "            \"word_scores_v2\": \"\",\n",
    "            \"sentiment_v2\": \"neutral\",\n",
    "            \"vader_positive\": 0,\n",
    "            \"vader_negative\": 0,\n",
    "            \"vader_neutral\": 0,\n",
    "            \"vader_compound\": 0,\n",
    "            \"vader_sentiment\": \"neutral\"\n",
    "        }.items()}\n",
    "\n",
    "    cleaned_text = clean_text_for_matching(text)\n",
    "    words = cleaned_text.split()\n",
    "    translated_sentence = []\n",
    "    total_score_avg = 0\n",
    "    total_score_v2 = 0\n",
    "    word_scores_avg = []\n",
    "    word_scores_v2 = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        matched_phrase = None\n",
    "        translated_phrase = None\n",
    "        phrase_score_avg = 0\n",
    "        phrase_scores_v2 = 0\n",
    "        max_length = min(5, len(words) - i)\n",
    "\n",
    "        for j in range(max_length, 0, -1):\n",
    "            phrase = ' '.join(words[i:i+j])\n",
    "            if phrase in translation_dicts[source_lang][target_lang]:\n",
    "                matched_phrase = phrase\n",
    "                translated_phrase = translation_dicts[source_lang][target_lang][phrase]\n",
    "                phrase_score_avg = sentiment_averages[source_lang].get(phrase, 0)\n",
    "                phrase_scores = all_sentiments[source_lang].get(phrase, [])\n",
    "                phrase_scores_v2 = compute_sentiment_v2(phrase_scores)\n",
    "                i += j\n",
    "                break\n",
    "\n",
    "        if matched_phrase:\n",
    "            translated_sentence.append(translated_phrase)\n",
    "            total_score_avg += phrase_score_avg\n",
    "            total_score_v2 += phrase_scores_v2\n",
    "            word_scores_avg.append(f\"{matched_phrase}:{phrase_score_avg}\")\n",
    "            word_scores_v2.append(f\"{matched_phrase}:{phrase_scores_v2}\")\n",
    "        else:\n",
    "            word = words[i]\n",
    "            translated_word = translation_dicts[source_lang][target_lang].get(word, word)\n",
    "            translated_sentence.append(translated_word)\n",
    "            score_avg = sentiment_averages[source_lang].get(word, 0)\n",
    "            scores = all_sentiments[source_lang].get(word, [])\n",
    "            score_v2 = compute_sentiment_v2(scores)\n",
    "            total_score_avg += score_avg\n",
    "            total_score_v2 += score_v2\n",
    "            word_scores_avg.append(f\"{word}:{score_avg}\")\n",
    "            word_scores_v2.append(f\"{word}:{score_v2}\")\n",
    "            i += 1\n",
    "\n",
    "    translated_text = ' '.join(translated_sentence).strip()\n",
    "    sentiment_avg = \"positive\" if total_score_avg > 0.05 else \"negative\" if total_score_avg < -0.05 else \"neutral\"\n",
    "    sentiment_v2 = \"positive\" if total_score_v2 > 0.05 else \"negative\" if total_score_v2 < -0.05 else \"neutral\"\n",
    "\n",
    "    vader_scores = vader_analyzer.polarity_scores(text)\n",
    "    vader_sentiment = \"positive\" if vader_scores['compound'] >= 0.05 else \"negative\" if vader_scores['compound'] <= -0.05 else \"neutral\"\n",
    "\n",
    "    return {\n",
    "        \"translated_text\": translated_text,\n",
    "        \"total_score_avg\": total_score_avg,\n",
    "        \"word_scores_avg\": '; '.join(word_scores_avg),\n",
    "        \"sentiment_avg\": sentiment_avg,\n",
    "        \"total_score_v2\": total_score_v2,\n",
    "        \"word_scores_v2\": '; '.join(word_scores_v2),\n",
    "        \"sentiment_v2\": sentiment_v2,\n",
    "        \"vader_positive\": vader_scores['pos'],\n",
    "        \"vader_negative\": vader_scores['neg'],\n",
    "        \"vader_neutral\": vader_scores['neu'],\n",
    "        \"vader_compound\": vader_scores['compound'],\n",
    "        \"vader_sentiment\": vader_sentiment\n",
    "    }\n",
    "\n",
    "# ================================\n",
    "# STEP 7: Apply to Test Corpus\n",
    "# ================================\n",
    "def safe_translate_and_analyze_sentiments_with_vader(row, translation_dicts, sentiment_averages, all_sentiments, vader_analyzer):\n",
    "    try:\n",
    "        return pd.Series(translate_analyze_sentiments_with_vader(\n",
    "            row.get('sentence', ''),\n",
    "            row.get('source_language', ''),\n",
    "            row.get('target_language', ''),\n",
    "            translation_dicts,\n",
    "            sentiment_averages,\n",
    "            all_sentiments,\n",
    "            vader_analyzer\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in row {row.name}: {e}\")\n",
    "        return pd.Series({\n",
    "            \"translated_text\": \"\",\n",
    "            \"total_score_avg\": 0,\n",
    "            \"word_scores_avg\": \"\",\n",
    "            \"sentiment_avg\": \"neutral\",\n",
    "            \"total_score_v2\": 0,\n",
    "            \"word_scores_v2\": \"\",\n",
    "            \"sentiment_v2\": \"neutral\",\n",
    "            \"vader_positive\": 0,\n",
    "            \"vader_negative\": 0,\n",
    "            \"vader_neutral\": 0,\n",
    "            \"vader_compound\": 0,\n",
    "            \"vader_sentiment\": \"neutral\"\n",
    "        })\n",
    "\n",
    "# Run the sentiment analysis\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "# Ensure we have the expected columns in test_corpus_df\n",
    "print('Test corpus columns:', test_corpus_df.columns.tolist())\n",
    "if 'sentence' not in test_corpus_df.columns:\n",
    "    raise KeyError(\"Expected 'sentence' column in test corpus. Found: \" + ','.join(test_corpus_df.columns.astype(str)))\n",
    "\n",
    "# Apply translation+sentiment\n",
    "test_corpus_df = test_corpus_df.copy()\n",
    "\n",
    "# For speed in this environment, process a sample (or full if small)\n",
    "sample_size = min(len(test_corpus_df), 500)\n",
    "print(f\"Processing {sample_size} rows (of {len(test_corpus_df)})\")\n",
    "\n",
    "test_corpus_df_sample = test_corpus_df.iloc[:sample_size].copy()\n",
    "\n",
    "results = test_corpus_df_sample.apply(\n",
    "    lambda row: safe_translate_and_analyze_sentiments_with_vader(\n",
    "        row,\n",
    "        translation_dicts,\n",
    "        sentiment_averages,\n",
    "        all_sentiments,\n",
    "        vader_analyzer\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Join results back\n",
    "test_corpus_df = test_corpus_df_sample.join(results)\n",
    "\n",
    "print(\"‚úÖ Sentiment analysis applied to test corpus (sample).\")\n",
    "\n",
    "# ================================\n",
    "# STEP 8: Optional Evaluation\n",
    "# ================================\n",
    "# Map sentiment strings to numeric\n",
    "sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "if 'sentiment_v2' in test_corpus_df.columns:\n",
    "    test_corpus_df['custom_sentiment_numeric'] = test_corpus_df['sentiment_v2'].map(sentiment_mapping)\n",
    "if 'vader_sentiment' in test_corpus_df.columns:\n",
    "    test_corpus_df['vader_sentiment_numeric'] = test_corpus_df['vader_sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# ================================\n",
    "# STEP 9: Display Results\n",
    "# ================================\n",
    "print(\"üìä Preview of sentiment analysis results:\")\n",
    "display(test_corpus_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891849e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed test_corpus_df to: C:\\Users\\User\\Desktop\\Assignment 3 Resources\\test_corpus_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the processed test corpus (current `test_corpus_df` in notebook)\n",
    "import os\n",
    "out_path = os.path.join(data_dir, 'test_corpus_processed.csv')\n",
    "# Ensure DataFrame exists\n",
    "if 'test_corpus_df' in globals():\n",
    "    try:\n",
    "        test_corpus_df.to_csv(out_path, index=False, encoding='utf-8')\n",
    "        print(f\"Saved processed test_corpus_df to: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print('Error saving CSV:', e)\n",
    "else:\n",
    "    print('test_corpus_df not found in the notebook namespace.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b91701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing transformer packages...\n",
      "============================================================\n",
      "\n",
      "üì• Installing transformers...\n",
      "   ‚úÖ transformers installed\n",
      "\n",
      "üì• Installing torch...\n",
      "   ‚úÖ torch installed\n",
      "\n",
      "üì• Installing sentencepiece...\n",
      "   ‚úÖ sentencepiece installed\n",
      "\n",
      "============================================================\n",
      "‚úÖ Installation complete!\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: Restart the kernel now!\n",
      "   Go to: Kernel ‚Üí Restart Kernel\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# STEP 1: Install Required Libraries (SIMPLIFIED)\n",
    "# ================================\n",
    "# Run this cell AFTER the NumPy fix cell above and kernel restart\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üì¶ Installing transformer packages...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install packages one by one with proper error handling\n",
    "packages = [\n",
    "    ('transformers', 'transformers'),\n",
    "    ('torch', 'torch --index-url https://download.pytorch.org/whl/cpu'),\n",
    "    ('sentencepiece', 'sentencepiece'),\n",
    "]\n",
    "\n",
    "for name, install_cmd in packages:\n",
    "    print(f\"\\nüì• Installing {name}...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, '-m', 'pip', 'install'] + install_cmd.split(),\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ‚úÖ {name} installed\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {name} may have issues: {result.stderr[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error with {name}: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Restart the kernel now!\")\n",
    "print(\"   Go to: Kernel ‚Üí Restart Kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd65622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages...\n",
      "==================================================\n",
      "‚úÖ PyTorch (torch): Installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers (transformers): Installed\n",
      "‚úÖ SentencePiece (sentencepiece): Installed\n",
      "==================================================\n",
      "\n",
      "‚úÖ All packages are installed! You can proceed.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# STEP 1b: Verify Installation (Run this after kernel restart)\n",
    "# ================================\n",
    "# Run this cell to check if packages are properly installed\n",
    "\n",
    "import sys\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages_to_check = {\n",
    "    'torch': 'PyTorch',\n",
    "    'transformers': 'Transformers',\n",
    "    'sentencepiece': 'SentencePiece',\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages...\")\n",
    "print(\"=\" * 50)\n",
    "all_installed = True\n",
    "\n",
    "for pkg, name in packages_to_check.items():\n",
    "    if check_package(pkg):\n",
    "        print(f\"‚úÖ {name} ({pkg}): Installed\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} ({pkg}): NOT installed\")\n",
    "        all_installed = False\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_installed:\n",
    "    print(\"\\n‚úÖ All packages are installed! You can proceed.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some packages are missing. Please:\")\n",
    "    print(\"   1. Run the installation cell above\")\n",
    "    print(\"   2. Restart the kernel\")\n",
    "    print(\"   3. Run this cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb425b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# EVERYTHING BELOW IS WITH REGARDS TO MODEL TRAINING \n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 2: Load Transformer Models and Prepare Data\n",
    "# ================================\n",
    "# Run this cell AFTER restarting the kernel following package installation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model names from HuggingFace\n",
    "afroxlmr_model = \"Davlan/afro-xlmr-base\"  # AfroXLMR base model\n",
    "afriberta_model = \"castorini/afriberta_base\"  # AfriBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS BELOW IS CREATING THREE TYPES OF DATA:\n",
    "#STORING LEXICON INDIVIDUAL WORDS\n",
    "#CREATING SENTENCES FROM LESXICON WORDS\n",
    "#USING CORPUS SENTENCES BOTH TRANSLATED AND ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6082d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training data prepared:\n",
      "   Total examples: 10454\n",
      "   Label distribution:\n",
      "label\n",
      "2    8599\n",
      "0     980\n",
      "1     875\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Sample examples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>un</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beaucoup</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abhorrer</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>capacit√©</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abolir</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abolition</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abominable</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>avorter</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>absence</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text sentiment  label\n",
       "0           un   neutral      1\n",
       "1     beaucoup  positive      2\n",
       "3      abandon  positive      2\n",
       "6     abhorrer  positive      2\n",
       "7     capacit√©  positive      2\n",
       "8       abolir  positive      2\n",
       "9    abolition  positive      2\n",
       "10  abominable  positive      2\n",
       "11     avorter  negative      0\n",
       "12     absence  positive      2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================\n",
    "# THIS CELL IS PURELY FOR LEXICON INDIVIDUAL WORDS - WE DONT HAVE TO INCLUDE JUST IN CASE\n",
    "# ================================\n",
    "# We'll use the lexicon data to create labeled training examples\n",
    "# The lexicon has sentiment labels we can use for fine-tuning\n",
    "\n",
    "\n",
    "# Problem: Lexicon has mixed French/English labels (\"Positif\", \"positive\", \"Negatif\", etc.)\n",
    "# Solution: Convert all variants to consistent English lowercase labels\n",
    "def normalize_sentiment(sentiment):\n",
    "    sentiment = str(sentiment).lower().strip()  # Convert to lowercase and remove spaces\n",
    "    \n",
    "    # Map all variants to standard labels\n",
    "    if sentiment in ['positif', 'positive']:\n",
    "        return 'positive'\n",
    "    elif sentiment in ['negatif', 'negative']:\n",
    "        return 'negative'\n",
    "    elif sentiment in ['neutre', 'neutral']:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return None  # Invalid labels will be removed later\n",
    "\n",
    "# Apply normalization to all sentiment labels\n",
    "df['sentiment_normalized'] = df['sentiment'].apply(normalize_sentiment)\n",
    "\n",
    "# Remove rows with unmapped/invalid sentiments\n",
    "df_clean = df[df['sentiment_normalized'].notna()].copy()\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 2. EXTRACT WORDS FROM EACH LANGUAGE\n",
    "# --------------------------------------------\n",
    "# For each language column (French, English, Zulu, etc.), extract the word + sentiment\n",
    "# This creates separate training examples for each language's words\n",
    "# Example: \"beaucoup\" (French) ‚Üí positive, \"a lot\" (English) ‚Üí positive\n",
    "\n",
    "train_data = []  # Will hold data from all languages\n",
    "\n",
    "for lang in supported_languages:  # Loop through: french, afrikaans, zulu, ciluba, sepedi, english\n",
    "    # Extract the language column and sentiment\n",
    "    temp_df = df_clean[[lang, 'sentiment_normalized']].copy()\n",
    "    temp_df.columns = ['text', 'sentiment']\n",
    "    \n",
    "    # Clean up the data\n",
    "    temp_df = temp_df.dropna(subset=['text', 'sentiment'])  # Remove empty cells\n",
    "    temp_df['text'] = temp_df['text'].astype(str)           # Ensure text is string\n",
    "    temp_df = temp_df[temp_df['text'].str.strip() != '']    # Remove blank strings\n",
    "    temp_df = temp_df[temp_df['text'].str.lower() != 'nan'] # Remove \"nan\" strings\n",
    "    \n",
    "    # Add this language's data to the list\n",
    "    train_data.append(temp_df)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3. COMBINE ALL LANGUAGES\n",
    "# --------------------------------------------\n",
    "# Merge all language data into one big training set\n",
    "# Result: bungi, beaucoup, a lot, baie, okuningi, kudu all become separate training examples\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "# Remove duplicate words (if same word appears in multiple rows)\n",
    "train_df = train_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# --------------------------------------------\n",
    "# 4. CONVERT SENTIMENT TO NUMBERS\n",
    "# --------------------------------------------\n",
    "# Transformers need numeric labels, not text\n",
    "# negative ‚Üí 0, neutral ‚Üí 1, positive ‚Üí 2\n",
    "sentiment_to_label = {'negative': 0, 'neutral': 1, 'positive': 2}  # Text ‚Üí Number\n",
    "label_to_sentiment = {0: 'negative', 1: 'neutral', 2: 'positive'}  # Number ‚Üí Text (for later)\n",
    "\n",
    "train_df['label'] = train_df['sentiment'].map(sentiment_to_label)\n",
    "\n",
    "# Remove any rows where mapping failed (shouldn't happen, but just in case)\n",
    "train_df = train_df.dropna(subset=['label'])\n",
    "train_df['label'] = train_df['label'].astype(int)  # Ensure label is integer\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. DISPLAY RESULTS\n",
    "# --------------------------------------------\n",
    "print(f\"\\n‚úÖ Training data prepared:\")\n",
    "print(f\"   Total examples: {len(train_df)}\")\n",
    "print(f\"   Label distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"\\n   Sample examples:\")\n",
    "display(train_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa79751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Augmenting lexicon words into sentence templates...\n",
      "‚úÖ Created 38008 augmented sentences from lexicon\n",
      "   Distribution: {'positive': 32128, 'negative': 3440, 'neutral': 2440}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CREATING SENTENCES USING LEXICON WORDS FOR MORE DATA - MAYBE LEXICON TEAM SHOULD DO THIS BUT JUST IN CASE ITS HERE\n",
    "# ================================\n",
    "# Convert isolated lexicon words into sentence contexts\n",
    "\n",
    "print(\"üîÑ Augmenting lexicon words into sentence templates...\")\n",
    "\n",
    "# Sentence templates for different languages\n",
    "templates_by_lang = {\n",
    "    'french': [\n",
    "        \"Je trouve que {} est important.\",\n",
    "        \"C'est {}.\",\n",
    "        \"Le mot {} exprime un sentiment.\",\n",
    "        \"{} dans cette phrase.\"\n",
    "    ],\n",
    "    'english': [\n",
    "        \"I think {} is important.\",\n",
    "        \"This is {}.\",\n",
    "        \"The word {} expresses a feeling.\",\n",
    "        \"{} in this sentence.\"\n",
    "    ],\n",
    "    'afrikaans': [\n",
    "        \"Dit is {}.\",\n",
    "        \"Die woord {} is belangrik.\",\n",
    "        \"{} in hierdie sin.\"\n",
    "    ],\n",
    "    'zulu': [\n",
    "        \"Lokhu {}.\",\n",
    "        \"Igama {} libalulekile.\",\n",
    "        \"{} kulesi sigaba.\"\n",
    "    ],\n",
    "    'ciluba': [\n",
    "        \"Ici {}.\",\n",
    "        \"Ijambu {} lidipingana.\",\n",
    "        \"{} mumpanzu.\"\n",
    "    ],\n",
    "    'sepedi': [\n",
    "        \"Se ke {}.\",\n",
    "        \"Lent≈°u {} le bohlokwa.\",\n",
    "        \"{} mo polelong ye.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create augmented dataset\n",
    "aug_data = []\n",
    "\n",
    "for _, row in df_clean.iterrows():\n",
    "    sentiment = row['sentiment_normalized']\n",
    "    if pd.isna(sentiment):\n",
    "        continue\n",
    "    \n",
    "    # For each language, create template-based sentences\n",
    "    for lang in supported_languages:\n",
    "        word = str(row[lang]).strip()\n",
    "        if not word or pd.isna(word) or word.lower() == 'nan':\n",
    "            continue\n",
    "        \n",
    "        # Use language-specific templates if available, else use French templates\n",
    "        templates = templates_by_lang.get(lang, templates_by_lang['french'])\n",
    "        \n",
    "        # Create 2-3 sentences per word (not all templates to avoid too much data)\n",
    "        for template in templates[:2]:\n",
    "            try:\n",
    "                sentence = template.format(word)\n",
    "                aug_data.append({\n",
    "                    'text': sentence,\n",
    "                    'sentiment': sentiment,\n",
    "                    'source': 'lexicon_augmented',\n",
    "                    'language': lang\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "aug_df = pd.DataFrame(aug_data)\n",
    "print(f\"‚úÖ Created {len(aug_df)} augmented sentences from lexicon\")\n",
    "print(f\"   Distribution: {aug_df['sentiment'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cbd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded corpus with 500 sentences\n",
      "‚úÖ Corpus split completed:\n",
      "   Training: 350 French sentences\n",
      "   Testing: 150 French sentences (held-out for evaluation)\n",
      "   ‚úÖ Added 350 translated sentences\n",
      "   Languages distribution:\n",
      "      - afrikaans: 96 sentences\n",
      "      - english: 87 sentences\n",
      "      - zulu: 84 sentences\n",
      "      - ciluba: 83 sentences\n",
      "\n",
      "   üìä Total training sentences: 700\n",
      "      - French (original): 350\n",
      "      - Translated: 350\n",
      "\n",
      "   Train sentiment distribution: {'positive': 664, 'negative': 34, 'neutral': 2}\n",
      "   Test sentiment distribution: {'positive': 142, 'negative': 7, 'neutral': 1}\n",
      "\n",
      "   üíæ Saved held-out test set to: corpus_test_split.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOADING CORPUS DATA AND SPLITTING TRAIN/TEST WE ALSO INCLUDE TRANSLATED SENTENCES\n",
    "# ================================\n",
    "\n",
    "# Load the processed corpus\n",
    "corpus_path = os.path.join(data_dir, 'test_corpus_processed.csv')\n",
    "if os.path.exists(corpus_path):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    corpus_df = pd.read_csv(corpus_path)\n",
    "    print(f\"‚úÖ Loaded corpus with {len(corpus_df)} sentences\")\n",
    "    \n",
    "    # Extract French sentences with sentiment_v2 labels\n",
    "    corpus_data = corpus_df[['sentence', 'sentiment_v2']].copy()\n",
    "    corpus_data.columns = ['text', 'sentiment']\n",
    "    corpus_data = corpus_data.dropna(subset=['text', 'sentiment'])\n",
    "    \n",
    "    # SPLIT: 70% train, 30% test (stratified by sentiment)\n",
    "    corpus_train, corpus_test, train_indices, test_indices = train_test_split(\n",
    "        corpus_data,\n",
    "        corpus_data.index,  # Keep track of indices for translation lookup\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=corpus_data['sentiment']\n",
    "    )\n",
    "    \n",
    "    corpus_train['source'] = 'corpus'\n",
    "    corpus_train['language'] = 'french'\n",
    "    \n",
    "    print(f\"‚úÖ Corpus split completed:\")\n",
    "    print(f\"   Training: {len(corpus_train)} French sentences\")\n",
    "    print(f\"   Testing: {len(corpus_test)} French sentences (held-out for evaluation)\")\n",
    "    \n",
    "    # ========================================\n",
    "    # ADD TRANSLATED SENTENCES\n",
    "    # ========================================\n",
    "    \n",
    "    # Check if translated_text and target_language columns exist\n",
    "    if 'translated_text' in corpus_df.columns and 'target_language' in corpus_df.columns:\n",
    "        translated_train_data = []\n",
    "        \n",
    "        # For each training sample, get its translated version\n",
    "        for idx in train_indices:\n",
    "            row = corpus_df.loc[idx]\n",
    "            \n",
    "            # Check if translation exists and is not empty\n",
    "            translated_text = str(row.get('translated_text', '')).strip()\n",
    "            target_lang = str(row.get('target_language', '')).strip().lower()\n",
    "            sentiment = row.get('sentiment_v2', '')\n",
    "            \n",
    "            if translated_text and translated_text != 'nan' and len(translated_text) > 0:\n",
    "                # Only add if target language is different from French\n",
    "                if target_lang and target_lang != 'french':\n",
    "                    translated_train_data.append({\n",
    "                        'text': translated_text,\n",
    "                        'sentiment': sentiment,\n",
    "                        'source': 'corpus_translated',\n",
    "                        'language': target_lang\n",
    "                    })\n",
    "        \n",
    "        # Add translated sentences to training data\n",
    "        if translated_train_data:\n",
    "            corpus_train_translated = pd.DataFrame(translated_train_data)\n",
    "            corpus_train = pd.concat([corpus_train, corpus_train_translated], ignore_index=True)\n",
    "            \n",
    "            print(f\"   ‚úÖ Added {len(translated_train_data)} translated sentences\")\n",
    "            print(f\"   Languages distribution:\")\n",
    "            lang_counts = corpus_train_translated['language'].value_counts()\n",
    "            for lang, count in lang_counts.items():\n",
    "                print(f\"      - {lang}: {count} sentences\")\n",
    "            print(f\"\\n   üìä Total training sentences: {len(corpus_train)}\")\n",
    "            print(f\"      - French (original): {(corpus_train['source'] == 'corpus').sum()}\")\n",
    "            print(f\"      - Translated: {(corpus_train['source'] == 'corpus_translated').sum()}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No valid translations found in corpus\")\n",
    "    else:\n",
    "        print(f\"   ‚ÑπÔ∏è No 'translated_text' or 'target_language' columns found\")\n",
    "        print(f\"   Available columns: {corpus_df.columns.tolist()}\")\n",
    "        print(f\"   Continuing with French sentences only\")\n",
    "    \n",
    "    print(f\"\\n   Train sentiment distribution: {corpus_train['sentiment'].value_counts().to_dict()}\")\n",
    "    print(f\"   Test sentiment distribution: {corpus_test['sentiment'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Save test split for later evaluation\n",
    "    test_split_path = os.path.join(data_dir, 'corpus_test_split.csv')\n",
    "    corpus_test.to_csv(test_split_path, index=False)\n",
    "    print(f\"\\n   üíæ Saved held-out test set to: corpus_test_split.csv\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Corpus file not found at {corpus_path}\")\n",
    "    print(\"   Will use only lexicon data for training\")\n",
    "    corpus_train = pd.DataFrame(columns=['text', 'sentiment', 'source', 'language'])\n",
    "    corpus_test = pd.DataFrame(columns=['text', 'sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS ABOVE IS CREATING THREE TYPES OF DATA:\n",
    "#STORING LEXICON INDIVIDUAL WORDS\n",
    "#CREATING SENTENCES FROM LESXICON WORDS\n",
    "#USING CORPUS SENTENCES BOTH TRANSLATED AND ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Combined training data created:\n",
      "   Total examples: 700\n",
      "   - Lexicon original: 0\n",
      "   - Lexicon augmented: 0\n",
      "   - Corpus (French): 350\n",
      "   - Corpus (Translated): 350\n",
      "\n",
      "   Language distribution:\n",
      "{'french': 350, 'afrikaans': 96, 'english': 87, 'zulu': 84, 'ciluba': 83}\n",
      "\n",
      "   Label distribution:\n",
      "label\n",
      "0     34\n",
      "1      2\n",
      "2    664\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Sample from each source:\n",
      "\n",
      "   corpus:\n",
      "      coude lit Dehors gal√®re captif... [french] ‚Üí positive\n",
      "      Courir Larme Doigt combo Poing... [french] ‚Üí positive\n",
      "\n",
      "   corpus_translated:\n",
      "      lukenyibu bulalu kuya dikenga mupika... [ciluba] ‚Üí positive\n",
      "      gijima izinyembezi umunwe isivalo inqindi... [zulu] ‚Üí positive\n",
      "\n",
      "‚úÖ Ready to train with 700 examples!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# COMBINE ALL TRAINING DATA SOURCES\n",
    "# ================================\n",
    "#TRAINING DATA - If we want to include lexicon singular words\n",
    "lexicon_original = train_df[['text', 'sentiment']].copy() # Individual words from lexicon and sentiment values: triste - negative\n",
    "lexicon_original['source'] = 'lexicon_original'  # Adds column called source to see where it came from: triste - negative - lexicon_original\n",
    "lexicon_original['language'] = 'mixed' # Add columns called language to indicate mixed languages\n",
    "\n",
    "# Takes data from sources and combines them into one big dataframe for training\n",
    "combined_train_df = pd.concat([\n",
    "    # lexicon_original, COMMENTED OUT: unless we want model to have context of specific words\n",
    "    # aug_df[['text', 'sentiment', 'source', 'language']], COMMENTED OUT: ~7,000 CREATED sentences from lexicon (uncomment to use)\n",
    "    corpus_train[['text', 'sentiment', 'source', 'language']] # Uses Corpus sentences with sentiment values\n",
    "], ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "combined_train_df = combined_train_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# For each word/sentence it looks at the sentiment value and maps it to a number\n",
    "combined_train_df['label'] = combined_train_df['sentiment'].map(sentiment_to_label)\n",
    "combined_train_df = combined_train_df.dropna(subset=['label'])\n",
    "combined_train_df['label'] = combined_train_df['label'].astype(int)\n",
    "\n",
    "\n",
    "#JUST SOME LOGGING TO SEE THE DATA THE MODELS WILL USE\n",
    "print(f\"\\n‚úÖ Combined training data created:\")\n",
    "print(f\"   Total examples: {len(combined_train_df)}\")\n",
    "print(f\"   - Lexicon original: {(combined_train_df['source'] == 'lexicon_original').sum()}\")\n",
    "print(f\"   - Lexicon augmented: {(combined_train_df['source'] == 'lexicon_augmented').sum()}\")\n",
    "print(f\"   - Corpus (French): {(combined_train_df['source'] == 'corpus').sum()}\")\n",
    "print(f\"   - Corpus (Translated): {(combined_train_df['source'] == 'corpus_translated').sum()}\")\n",
    "print(f\"\\n   Language distribution:\")\n",
    "if 'language' in combined_train_df.columns:\n",
    "    print(combined_train_df['language'].value_counts().to_dict())\n",
    "print(f\"\\n   Label distribution:\")\n",
    "print(combined_train_df['label'].value_counts().sort_index())\n",
    "print(f\"\\n   Sample from each source:\")\n",
    "for src in combined_train_df['source'].unique():\n",
    "    sample = combined_train_df[combined_train_df['source'] == src].head(2)\n",
    "    print(f\"\\n   {src}:\")\n",
    "    for _, row in sample.iterrows():\n",
    "        lang_info = f\" [{row.get('language', 'unknown')}]\" if 'language' in row else \"\"\n",
    "        print(f\"      {row['text'][:60]}...{lang_info} ‚Üí {row['sentiment']}\")\n",
    "\n",
    "# Update train_df to use combined data\n",
    "train_df = combined_train_df[['text', 'label']].copy()\n",
    "print(f\"\\n‚úÖ Ready to train with {len(train_df)} examples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BELOW WE FINE TUNE & TRAIN AfroXLMR USING THE COMBINED TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7acbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Training samples: 560\n",
      "Validation samples: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINE TUNE AfroXLMR - Getting everything ready to train AfroXLMR model\n",
    "# ================================\n",
    "\n",
    "# ==================================================================================\n",
    "#Import classes from hugging face transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "#To split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Evaluation metrix to test model performance\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "#Deep learning framework\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Create PyTorch Dataset it will accept texts (\"Je suis triste\", \"C'est beaucoup\") and labels (0, 1, 2)\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Split data into train/validation\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "# ==================================================================================\n",
    "\n",
    "# ==================================================================================\n",
    "# Load AfroXLMR tokenizer and model\n",
    "afroxlmr_tokenizer = AutoTokenizer.from_pretrained(afroxlmr_model)\n",
    "afroxlmr_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
    "    afroxlmr_model,\n",
    "    num_labels=3,  # negative, neutral, positive\n",
    "    id2label=label_to_sentiment,\n",
    "    label2id=sentiment_to_label\n",
    ")\n",
    "# ==================================================================================\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(X_train, y_train, afroxlmr_tokenizer)\n",
    "val_dataset = SentimentDataset(X_val, y_val, afroxlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AfroXLMR training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 05:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.237180</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.225907</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.235247</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925641</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AfroXLMR training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AfroXLMR Validation Results:\n",
      "   eval_loss: 0.2372\n",
      "   eval_accuracy: 0.9500\n",
      "   eval_f1: 0.9256\n",
      "   eval_precision: 0.9025\n",
      "   eval_recall: 0.9500\n",
      "   eval_runtime: 2.0722\n",
      "   eval_samples_per_second: 67.5610\n",
      "   eval_steps_per_second: 4.3430\n",
      "   epoch: 3.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Train AfroXLMR Model\n",
    "# ================================\n",
    "\n",
    "# Define metrics computation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "# Note: Using updated parameter names for newer transformers versions\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_afroxlmr',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer_afroxlmr = Trainer(\n",
    "    model=afroxlmr_classifier,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting AfroXLMR training...\")\n",
    "\n",
    "# Train the model\n",
    "trainer_afroxlmr.train()\n",
    "\n",
    "print(\"\\nAfroXLMR training completed!\")\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer_afroxlmr.evaluate()\n",
    "print(f\"\\nAfroXLMR Validation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234171ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Enhanced AfroXLMR Sentiment Classifier Implementation\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "class AfroXLMRSentimentClassifier:\n",
    "    def __init__(self, model_name=\"Davlan/afro-xlmr-base\", num_labels=3, device=None):\n",
    "        \"\"\"\n",
    "        Initialize AfroXLMR sentiment classifier with XAI support.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): HuggingFace model name/path\n",
    "            num_labels (int): Number of sentiment classes\n",
    "            device (str): 'cuda' or 'cpu', will auto-detect if None\n",
    "        \"\"\"\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize with attention output enabled for XAI\n",
    "        config = AutoConfig.from_pretrained(model_name, \n",
    "                                          num_labels=num_labels,\n",
    "                                          output_attentions=True,\n",
    "                                          output_hidden_states=True)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "        self.label2id = {v: k for k, v in self.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_attention_weights(self, text):\n",
    "        \"\"\"\n",
    "        Get attention weights for a given text input.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text for sentiment analysis\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains tokens, attention weights, prediction and sentiment\n",
    "        \"\"\"\n",
    "        # Tokenize and prepare input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs with attention\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_attentions=True)\n",
    "        \n",
    "        # Get prediction\n",
    "        logits = outputs.logits\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "        sentiment = self.id2label[pred]\n",
    "        \n",
    "        # Convert attention to CPU numpy\n",
    "        attention = [layer.cpu().numpy() for layer in outputs.attentions]\n",
    "        \n",
    "        # Get tokens from input IDs\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'attention': attention,\n",
    "            'prediction': pred,\n",
    "            'sentiment': sentiment\n",
    "        }\n",
    "    \n",
    "    def visualize_attention(self, text, layer=-1, head=0, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize attention weights for a given text input.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text for sentiment analysis\n",
    "            layer (int): Transformer layer to visualize (-1 for last layer)\n",
    "            head (int): Attention head to visualize\n",
    "            save_path (str): Optional path to save the visualization\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Attention weights matrix\n",
    "        \"\"\"\n",
    "        # Get attention weights\n",
    "        attention_data = self.get_attention_weights(text)\n",
    "        tokens = attention_data['tokens']\n",
    "        \n",
    "        # Get attention matrix for specified layer and head\n",
    "        attention_matrix = attention_data['attention'][layer][0, head]\n",
    "        \n",
    "        # Create figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(attention_matrix, \n",
    "                   xticklabels=tokens,\n",
    "                   yticklabels=tokens,\n",
    "                   cmap='YlOrRd',\n",
    "                   cbar_kws={'label': 'Attention Weight'})\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        # Add title with prediction\n",
    "        plt.title(f'Attention Weights (Layer {layer+1}, Head {head+1})\\n'\n",
    "                 f'Predicted Sentiment: {attention_data[\"sentiment\"]}')\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Visualization saved to: {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "        return attention_matrix\n",
    "    \n",
    "    def analyze_important_words(self, text, top_k=5):\n",
    "        \"\"\"\n",
    "        Analyze and return the most important words based on attention weights.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text for sentiment analysis\n",
    "            top_k (int): Number of top words to return\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains input text, sentiment, and top important words\n",
    "        \"\"\"\n",
    "        # Get attention weights\n",
    "        attention_data = self.get_attention_weights(text)\n",
    "        tokens = attention_data['tokens']\n",
    "        \n",
    "        # Get attention from last layer\n",
    "        last_layer_attention = attention_data['attention'][-1][0]  # shape: (num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Average across all heads\n",
    "        mean_attention = last_layer_attention.mean(axis=0)  # shape: (seq_len, seq_len)\n",
    "        \n",
    "        # Get CLS token attention (first token's attention to all other tokens)\n",
    "        token_importance = mean_attention[0]  # shape: (seq_len,)\n",
    "        \n",
    "        # Create token-importance pairs\n",
    "        token_scores = []\n",
    "        for idx, (token, score) in enumerate(zip(tokens, token_importance)):\n",
    "            # Skip special tokens\n",
    "            if token in ['<s>', '</s>', '<pad>', '<unk>']:\n",
    "                continue\n",
    "            token_scores.append({\n",
    "                'token': token,\n",
    "                'importance': float(score),\n",
    "                'position': idx\n",
    "            })\n",
    "        \n",
    "        # Sort by importance score\n",
    "        token_scores.sort(key=lambda x: x['importance'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'sentiment': attention_data['sentiment'],\n",
    "            'top_words': token_scores[:top_k]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa82a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict_with_probabilities(self, texts):\n",
    "        \"\"\"\n",
    "        Get probability distributions for predictions to use in ensemble methods.\n",
    "        \n",
    "        Args:\n",
    "            texts (str or list): Input text(s) for sentiment analysis\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Probability distributions, shape (num_texts, num_classes)\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        return probs.cpu().numpy()\n",
    "    \n",
    "    def get_logits(self, texts):\n",
    "        \"\"\"\n",
    "        Get raw logits for ensemble methods that combine pre-softmax values.\n",
    "        \n",
    "        Args:\n",
    "            texts (str or list): Input text(s) for sentiment analysis\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Raw logits, shape (num_texts, num_classes)\n",
    "        \"\"\"\n",
    "        # Handle single text input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        # Convert to numpy array\n",
    "        return outputs.logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def evaluate_detailed(self, test_texts, test_labels, save_confusion_matrix=True):\n",
    "        \"\"\"\n",
    "        Perform detailed evaluation including confusion matrix and classification report.\n",
    "        \n",
    "        Args:\n",
    "            test_texts (list): List of texts to evaluate\n",
    "            test_labels (list): True labels (can be strings or integers)\n",
    "            save_confusion_matrix (bool): Whether to save confusion matrix plot\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing evaluation metrics and predictions\n",
    "        \"\"\"\n",
    "        # Convert string labels to integers if needed\n",
    "        if isinstance(test_labels[0], str):\n",
    "            test_labels = [self.label2id[label] for label in test_labels]\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = self.predict_with_probabilities(test_texts)\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "        report = classification_report(test_labels, predictions, \n",
    "                                    target_names=list(self.id2label.values()),\n",
    "                                    output_dict=True)\n",
    "        \n",
    "        # Create confusion matrix visualization\n",
    "        if save_confusion_matrix:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                       xticklabels=list(self.id2label.values()),\n",
    "                       yticklabels=list(self.id2label.values()))\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('afroxlmr_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(\"Confusion matrix saved as 'afroxlmr_confusion_matrix.png'\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(test_labels, predictions, \n",
    "                                 target_names=list(self.id2label.values())))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': report['accuracy'],\n",
    "            'precision': report['weighted avg']['precision'],\n",
    "            'recall': report['weighted avg']['recall'],\n",
    "            'f1': report['weighted avg']['f1-score'],\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': predictions,\n",
    "            'detailed_report': report\n",
    "        }\n",
    "    \n",
    "    def evaluate_per_language(self, test_texts, test_labels, languages):\n",
    "        \"\"\"\n",
    "        Evaluate model performance separately for each language.\n",
    "        \n",
    "        Args:\n",
    "            test_texts (list): List of texts to evaluate\n",
    "            test_labels (list): True labels\n",
    "            languages (list): List of language identifiers for each text\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with metrics for each language\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        unique_languages = set(languages)\n",
    "        \n",
    "        print(\"\\nPer-Language Evaluation:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{'Language':<15} {'Samples':<8} {'Accuracy':<10} {'F1':<10}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for lang in unique_languages:\n",
    "            # Get indices for this language\n",
    "            lang_indices = [i for i, l in enumerate(languages) if l == lang]\n",
    "            if not lang_indices:\n",
    "                continue\n",
    "                \n",
    "            # Filter data for this language\n",
    "            lang_texts = [test_texts[i] for i in lang_indices]\n",
    "            lang_labels = [test_labels[i] for i in lang_indices]\n",
    "            \n",
    "            # Get predictions\n",
    "            probs = self.predict_with_probabilities(lang_texts)\n",
    "            predictions = np.argmax(probs, axis=1)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            report = classification_report(lang_labels, predictions, \n",
    "                                        target_names=list(self.id2label.values()),\n",
    "                                        output_dict=True)\n",
    "            \n",
    "            # Store results\n",
    "            results[lang] = {\n",
    "                'num_samples': len(lang_texts),\n",
    "                'accuracy': report['accuracy'],\n",
    "                'precision': report['weighted avg']['precision'],\n",
    "                'recall': report['weighted avg']['recall'],\n",
    "                'f1': report['weighted avg']['f1-score']\n",
    "            }\n",
    "            \n",
    "            # Print results row\n",
    "            print(f\"{lang:<15} {len(lang_texts):<8} \"\n",
    "                  f\"{results[lang]['accuracy']:.3f}  \"\n",
    "                  f\"{results[lang]['f1']:.3f}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def load_from_lexicon(lexicon_path, target_languages=None):\n",
    "        \"\"\"\n",
    "        Load data from multilingual sentiment lexicon.\n",
    "        \n",
    "        Args:\n",
    "            lexicon_path (str): Path to lexicon CSV/Excel file\n",
    "            target_languages (list): List of language columns to extract\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (texts_list, labels_list, languages_list)\n",
    "        \"\"\"\n",
    "        # Default languages if none specified\n",
    "        if target_languages is None:\n",
    "            target_languages = ['zulu', 'xhosa', 'sepedi', 'shona', 'afrikaans', 'english']\n",
    "            \n",
    "        # Read lexicon file\n",
    "        if lexicon_path.endswith('.csv'):\n",
    "            df = pd.read_csv(lexicon_path)\n",
    "        else:\n",
    "            df = pd.read_excel(lexicon_path)\n",
    "            \n",
    "        texts, labels, languages = [], [], []\n",
    "        \n",
    "        # Map sentiment labels to integers\n",
    "        sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        \n",
    "        # Process each language\n",
    "        for lang in target_languages:\n",
    "            if lang not in df.columns:\n",
    "                print(f\"Warning: Language '{lang}' not found in lexicon\")\n",
    "                continue\n",
    "                \n",
    "            # Get non-empty entries\n",
    "            lang_data = df[[lang, 'sentiment']].dropna()\n",
    "            lang_data = lang_data[lang_data[lang].str.strip() != '']\n",
    "            \n",
    "            if len(lang_data) == 0:\n",
    "                print(f\"Warning: No valid entries found for '{lang}'\")\n",
    "                continue\n",
    "                \n",
    "            # Add to lists\n",
    "            texts.extend(lang_data[lang].tolist())\n",
    "            labels.extend([sentiment_map.get(s.lower(), 1) for s in lang_data['sentiment']])\n",
    "            languages.extend([lang] * len(lang_data))\n",
    "            \n",
    "        print(f\"\\nLoaded from lexicon:\")\n",
    "        print(f\"Total entries: {len(texts)}\")\n",
    "        print(\"\\nLanguage distribution:\")\n",
    "        for lang in set(languages):\n",
    "            count = languages.count(lang)\n",
    "            print(f\"{lang}: {count} entries\")\n",
    "            \n",
    "        return texts, labels, languages\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_from_corpus(corpus_path, text_column='text', label_column='sentiment', language_column=None):\n",
    "        \"\"\"\n",
    "        Load data from a sentiment corpus file.\n",
    "        \n",
    "        Args:\n",
    "            corpus_path (str): Path to corpus CSV/Excel file\n",
    "            text_column (str): Name of column containing text\n",
    "            label_column (str): Name of column containing sentiment labels\n",
    "            language_column (str): Optional column name for language labels\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (texts, labels, languages)\n",
    "        \"\"\"\n",
    "        # Read corpus file\n",
    "        if corpus_path.endswith('.csv'):\n",
    "            df = pd.read_csv(corpus_path)\n",
    "        else:\n",
    "            df = pd.read_excel(corpus_path)\n",
    "            \n",
    "        # Verify required columns exist\n",
    "        if text_column not in df.columns or label_column not in df.columns:\n",
    "            raise ValueError(f\"Required columns not found. Available columns: {df.columns.tolist()}\")\n",
    "            \n",
    "        # Map sentiment labels to integers\n",
    "        sentiment_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "        \n",
    "        # Extract data\n",
    "        texts = df[text_column].tolist()\n",
    "        labels = [sentiment_map.get(str(s).lower(), 1) for s in df[label_column]]\n",
    "        \n",
    "        # Get languages if column specified\n",
    "        languages = None\n",
    "        if language_column and language_column in df.columns:\n",
    "            languages = df[language_column].tolist()\n",
    "            \n",
    "        print(f\"\\nLoaded from corpus:\")\n",
    "        print(f\"Total entries: {len(texts)}\")\n",
    "        if languages:\n",
    "            print(\"\\nLanguage distribution:\")\n",
    "            lang_counts = pd.Series(languages).value_counts()\n",
    "            for lang, count in lang_counts.items():\n",
    "                print(f\"{lang}: {count} entries\")\n",
    "                \n",
    "        return texts, labels, languages\n",
    "    \n",
    "    @staticmethod\n",
    "    def combine_datasets(lexicon_data, corpus_data):\n",
    "        \"\"\"\n",
    "        Combine data from lexicon and corpus sources.\n",
    "        \n",
    "        Args:\n",
    "            lexicon_data (tuple): (texts, labels, languages) from lexicon\n",
    "            corpus_data (tuple): (texts, labels, languages) from corpus\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (combined_texts, combined_labels, combined_languages)\n",
    "        \"\"\"\n",
    "        # Unpack data\n",
    "        lex_texts, lex_labels, lex_langs = lexicon_data\n",
    "        corp_texts, corp_labels, corp_langs = corpus_data\n",
    "        \n",
    "        # Combine texts and labels\n",
    "        texts = lex_texts + corp_texts\n",
    "        labels = lex_labels + corp_labels\n",
    "        \n",
    "        # Handle languages (might be None from corpus)\n",
    "        if corp_langs is None:\n",
    "            corp_langs = ['unknown'] * len(corp_texts)\n",
    "        languages = lex_langs + corp_langs\n",
    "        \n",
    "        # Create DataFrame for easy manipulation\n",
    "        df = pd.DataFrame({\n",
    "            'text': texts,\n",
    "            'label': labels,\n",
    "            'language': languages\n",
    "        })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['text'])\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nCombined Dataset Statistics:\")\n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        print(df['label'].value_counts().sort_index())\n",
    "        print(\"\\nLanguage distribution:\")\n",
    "        print(df['language'].value_counts())\n",
    "        \n",
    "        return df['text'].tolist(), df['label'].tolist(), df['language'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd1eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, train_texts, train_labels, validation_texts=None, validation_labels=None,\n",
    "            languages=None, batch_size=16, num_epochs=3, learning_rate=2e-5):\n",
    "        \"\"\"\n",
    "        Train the model with enhanced logging and language-aware evaluation.\n",
    "        \n",
    "        Args:\n",
    "            train_texts (list): Training texts\n",
    "            train_labels (list): Training labels\n",
    "            validation_texts (list): Optional validation texts\n",
    "            validation_labels (list): Optional validation labels\n",
    "            languages (list): Optional list of language identifiers for texts\n",
    "            batch_size (int): Batch size for training\n",
    "            num_epochs (int): Number of training epochs\n",
    "            learning_rate (float): Learning rate for optimization\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"AfroXLMR Training Configuration\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Model: {self.model.config._name_or_path}\")\n",
    "        print(f\"Training samples: {len(train_texts)}\")\n",
    "        print(f\"Validation samples: {len(validation_texts) if validation_texts else 'None'}\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Epochs: {num_epochs}\")\n",
    "        print(f\"Learning rate: {learning_rate}\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        \n",
    "        if languages:\n",
    "            print(\"\\nLanguage Distribution:\")\n",
    "            lang_counts = pd.Series(languages).value_counts()\n",
    "            for lang, count in lang_counts.items():\n",
    "                print(f\"{lang}: {count} samples\")\n",
    "                \n",
    "        print(\"\\nLabel Distribution:\")\n",
    "        label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"{self.id2label[label]}: {count} samples\")\n",
    "            \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Create PyTorch datasets\n",
    "        train_dataset = SentimentDataset(train_texts, train_labels, self.tokenizer)\n",
    "        if validation_texts:\n",
    "            val_dataset = SentimentDataset(validation_texts, validation_labels, self.tokenizer)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./results',\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=100,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./logs',\n",
    "            logging_steps=10,\n",
    "            eval_steps=100,\n",
    "            save_steps=100,\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset if validation_texts else None,\n",
    "            compute_metrics=lambda p: {\n",
    "                'accuracy': accuracy_score(p.label_ids, p.predictions.argmax(-1))\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        trainer.train()\n",
    "        \n",
    "        # Final evaluation\n",
    "        if validation_texts:\n",
    "            print(\"\\nFinal Validation Results:\")\n",
    "            self.evaluate_detailed(validation_texts, validation_labels)\n",
    "            \n",
    "            if languages:\n",
    "                print(\"\\nPer-Language Validation Results:\")\n",
    "                self.evaluate_per_language(validation_texts, validation_labels, \n",
    "                                        languages[-len(validation_texts):])\n",
    "                \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Training completed!\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15669ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Comprehensive Example Usage\n",
    "# ================================\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = AfroXLMRSentimentClassifier()\n",
    "\n",
    "# 1. Load data from lexicon and corpus\n",
    "print(\"Loading data...\")\n",
    "lexicon_data = AfroXLMRSentimentClassifier.load_from_lexicon(\n",
    "    'corrected_lexicon.xlsx',\n",
    "    target_languages=['zulu', 'xhosa', 'sepedi', 'afrikaans', 'english']\n",
    ")\n",
    "\n",
    "corpus_data = AfroXLMRSentimentClassifier.load_from_corpus(\n",
    "    'french_test_corpus.xlsx',\n",
    "    text_column='text',\n",
    "    label_column='sentiment',\n",
    "    language_column='language'\n",
    ")\n",
    "\n",
    "# 2. Combine datasets\n",
    "texts, labels, languages = AfroXLMRSentimentClassifier.combine_datasets(lexicon_data, corpus_data)\n",
    "\n",
    "# 3. Split into train/validation\n",
    "X_train, X_val, y_train, y_val, langs_train, langs_val = train_test_split(\n",
    "    texts, labels, languages, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "classifier.train(\n",
    "    train_texts=X_train,\n",
    "    train_labels=y_train,\n",
    "    validation_texts=X_val,\n",
    "    validation_labels=y_val,\n",
    "    languages=langs_train,\n",
    "    batch_size=16,\n",
    "    num_epochs=3\n",
    ")\n",
    "\n",
    "# 5. XAI Example - Visualize attention\n",
    "example_text = \"I am very happy with the results\"\n",
    "classifier.visualize_attention(\n",
    "    example_text,\n",
    "    layer=-1,  # last layer\n",
    "    head=0,    # first attention head\n",
    "    save_path='attention_viz.png'\n",
    ")\n",
    "\n",
    "# 6. Analyze important words\n",
    "important_words = classifier.analyze_important_words(example_text, top_k=3)\n",
    "print(\"\\nImportant words analysis:\")\n",
    "print(f\"Text: {important_words['text']}\")\n",
    "print(f\"Sentiment: {important_words['sentiment']}\")\n",
    "print(\"Top words:\")\n",
    "for word in important_words['top_words']:\n",
    "    print(f\"- {word['token']}: {word['importance']:.3f}\")\n",
    "\n",
    "# 7. Ensemble prediction example\n",
    "texts_to_predict = [\n",
    "    \"This is excellent work\",\n",
    "    \"I'm not sure about this\",\n",
    "    \"This is terrible\"\n",
    "]\n",
    "\n",
    "# Get probabilities for ensemble methods\n",
    "probs = classifier.predict_with_probabilities(texts_to_predict)\n",
    "print(\"\\nPrediction probabilities:\")\n",
    "for text, prob in zip(texts_to_predict, probs):\n",
    "    print(f\"\\nText: {text}\")\n",
    "    for i, p in enumerate(prob):\n",
    "        print(f\"{classifier.id2label[i]}: {p:.3f}\")\n",
    "\n",
    "# 8. Detailed evaluation\n",
    "test_texts = [\n",
    "    \"The results are amazing\",\n",
    "    \"I don't like this approach\",\n",
    "    \"This seems okay to me\"\n",
    "]\n",
    "test_labels = [2, 0, 1]  # positive, negative, neutral\n",
    "\n",
    "print(\"\\nDetailed Evaluation:\")\n",
    "eval_results = classifier.evaluate_detailed(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    save_confusion_matrix=True\n",
    ")\n",
    "\n",
    "# 9. Per-language evaluation\n",
    "test_languages = ['english', 'zulu', 'english']\n",
    "print(\"\\nPer-Language Evaluation:\")\n",
    "lang_results = classifier.evaluate_per_language(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    test_languages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232fdfa",
   "metadata": {},
   "source": [
    "# Initializing the AfroXLMR Sentiment Classifier\n",
    "\n",
    "The classifier can be initialized with the following parameters:\n",
    "\n",
    "1. `model_name`: The HuggingFace model name (default: \"Davlan/afro-xlmr-base\")\n",
    "2. `num_labels`: Number of sentiment classes (default: 3 for negative/neutral/positive)\n",
    "3. `device`: Computing device (default: will auto-detect CUDA/CPU)\n",
    "\n",
    "The initialization will:\n",
    "- Load the model and tokenizer\n",
    "- Enable attention outputs for XAI features\n",
    "- Set up sentiment label mappings\n",
    "- Move the model to the appropriate device (GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier = AfroXLMRSentimentClassifier()\n",
    "\n",
    "# You can also specify parameters explicitly:\n",
    "# classifier = AfroXLMRSentimentClassifier(\n",
    "#     model_name=\"Davlan/afro-xlmr-base\",\n",
    "#     num_labels=3,\n",
    "#     device=\"cuda\"  # or \"cpu\"\n",
    "# )\n",
    "\n",
    "# Test the classifier with a simple example\n",
    "text = \"I am very happy with the results!\"\n",
    "attention_data = classifier.get_attention_weights(text)\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"Predicted sentiment: {attention_data['sentiment']}\")\n",
    "\n",
    "# Visualize attention for the example\n",
    "classifier.visualize_attention(text, save_path='example_attention.png')\n",
    "\n",
    "# Analyze important words\n",
    "important_words = classifier.analyze_important_words(text, top_k=3)\n",
    "print(\"\\nMost important words:\")\n",
    "for word in important_words['top_words']:\n",
    "    print(f\"- {word['token']}: importance = {word['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BELOW I WILL VISUALISE AfroXLMR, the issue is the data is very biased mostly positive so the model will definetely reflect that:\n",
    "#Positive (label 2): 8,599 examples (83.4%)\n",
    "#Negative (label 0): 980 examples (9.5%)\n",
    "#Neutral (label 1): 875 examples (8.5%)\n",
    "\n",
    "#Maybe we use class weights, or use data augmentation to balance the classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
