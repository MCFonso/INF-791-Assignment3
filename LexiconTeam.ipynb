{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101456ae",
   "metadata": {},
   "source": [
    "# Lexicon Team - Multilingual Sentiment Lexicon Development\n",
    "\n",
    "This notebook consolidates all lexicon processing work, including:\n",
    "- Loading and cleaning the original French lexicon\n",
    "- Creating unique entries and deduplication\n",
    "- GCP Translation API integration (French ‚Üí English)\n",
    "- Quality control and validation\n",
    "- Final enriched lexicon generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8b7b6",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097ef3d",
   "metadata": {},
   "source": [
    "## Step 0: Google Cloud Platform (GCP) Setup\n",
    "\n",
    "**IMPORTANT**: Configure GCP credentials before running translation cells.\n",
    "\n",
    "This notebook uses Google Cloud Translation API for translating:\n",
    "- French ‚Üí English (lexicon)\n",
    "- French ‚Üí Afrikaans, Tsonga, Zulu (corpus)\n",
    "\n",
    "**Requirements:**\n",
    "- Google Cloud project with Translation API enabled\n",
    "- Service account JSON key file\n",
    "- Project ID from GCP Console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b375ae7",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Run this cell first to install the Google Cloud Translation library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b4f545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-translate\n",
      "  Obtaining dependency information for google-cloud-translate from https://files.pythonhosted.org/packages/89/f9/f32ffce070e56bd455f70953df65ed0ea87d6d35cb3a6b18385a8c31ca9e/google_cloud_translate-3.22.0-py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_translate-3.22.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting python-dotenv\n",
      "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/14/1b/a298b06749107c305e1fe0f814c6c74aea7b2f1e10989cb30f544a1b3253/python_dotenv-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-cloud-translate)\n",
      "  Obtaining dependency information for google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 from https://files.pythonhosted.org/packages/ed/d4/90197b416cb61cefd316964fd9e7bd8324bcbafabf40eef14a9f20b81974/google_api_core-2.28.1-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-translate)\n",
      "  Obtaining dependency information for google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 from https://files.pythonhosted.org/packages/92/05/adeb6c495aec4f9d93f9e2fc29eeef6e14d452bba11d15bdb874ce1d5b10/google_auth-2.42.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.42.1-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=1.4.4 (from google-cloud-translate)\n",
      "  Obtaining dependency information for google-cloud-core<3.0.0,>=1.4.4 from https://files.pythonhosted.org/packages/89/20/bfa472e327c8edee00f04beecc80baeddd2ab33ee0e86fd7654da49d45e9/google_cloud_core-2.5.0-py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\python312\\lib\\site-packages (from google-cloud-translate) (1.75.1)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-translate)\n",
      "  Obtaining dependency information for proto-plus<2.0.0,>=1.22.3 from https://files.pythonhosted.org/packages/4e/6d/280c4c2ce28b1593a19ad5239c8b826871fc6ec275c21afc8e1820108039/proto_plus-1.26.1-py3-none-any.whl.metadata\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\python312\\lib\\site-packages (from google-cloud-translate) (6.32.1)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-translate)\n",
      "  Obtaining dependency information for grpc-google-iam-v1<1.0.0,>=0.14.0 from https://files.pythonhosted.org/packages/4a/bd/330a1bbdb1afe0b96311249e699b6dc9cfc17916394fd4503ac5aca2514b/grpc_google_iam_v1-0.14.3-py3-none-any.whl.metadata\n",
      "  Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate)\n",
      "  Obtaining dependency information for googleapis-common-protos<2.0.0,>=1.56.2 from https://files.pythonhosted.org/packages/25/e8/eba9fece11d57a71e3e22ea672742c8f3cf23b35730c9e96db768b295216/googleapis_common_protos-1.71.0-py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.71.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.32.5)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate)\n",
      "  Obtaining dependency information for grpcio-status<2.0.0,>=1.33.2 from https://files.pythonhosted.org/packages/8c/cc/27ba60ad5a5f2067963e6a858743500df408eb5855e98be778eaef8c9b02/grpcio_status-1.76.0-py3-none-any.whl.metadata\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate)\n",
      "  Obtaining dependency information for cachetools<7.0,>=2.0.0 from https://files.pythonhosted.org/packages/96/c5/1e741d26306c42e2bf6ab740b2202872727e0f606033c9dd713f8b93f5a8/cachetools-6.2.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-6.2.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate)\n",
      "  Obtaining dependency information for pyasn1-modules>=0.2.1 from https://files.pythonhosted.org/packages/47/8d/d529b5d697919ba8c11ad626e835d4039be708a35b0d22de83a269a6682c/pyasn1_modules-0.4.2-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/64/8d/0133e4eb4beed9e425d9a98ed6e081a55d195481b7632472be1af08d2f6b/rsa-4.9.1-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in c:\\python312\\lib\\site-packages (from grpcio<2.0.0,>=1.33.2->google-cloud-translate) (4.15.0)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-cloud-translate)\n",
      "  Obtaining dependency information for grpcio<2.0.0,>=1.33.2 from https://files.pythonhosted.org/packages/9e/00/7bd478cbb851c04a48baccaa49b75abaa8e4122f7d86da797500cccdd771/grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate)\n",
      "  Obtaining dependency information for pyasn1<0.7.0,>=0.6.1 from https://files.pythonhosted.org/packages/c8/f1/d6a797abb14f6283c0ddff96bbdd46937f64122b8c925cab503dd37f8214/pyasn1-0.6.1-py3-none-any.whl.metadata\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2025.8.3)\n",
      "Downloading google_cloud_translate-3.22.0-py3-none-any.whl (204 kB)\n",
      "   ---------------------------------------- 0.0/204.4 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/204.4 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/204.4 kB 660.6 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 41.0/204.4 kB 495.5 kB/s eta 0:00:01\n",
      "   ------- ------------------------------- 41.0/204.4 kB 495.5 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 71.7/204.4 kB 302.7 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 92.2/204.4 kB 374.1 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 153.6/204.4 kB 482.7 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 153.6/204.4 kB 482.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 194.6/204.4 kB 471.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 204.4/204.4 kB 496.7 kB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading google_auth-2.42.1-py2.py3-none-any.whl (222 kB)\n",
      "   ---------------------------------------- 0.0/222.6 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 112.6/222.6 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 222.6/222.6 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl (32 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "Downloading cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "   ---------------------------------------- 0.0/173.7 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 163.8/173.7 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 173.7/173.7 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.71.0-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.6 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 225.3/294.6 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.6/294.6 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.76.0-cp312-cp312-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/4.7 MB 6.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/4.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/4.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/4.7 MB 6.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.7/4.7 MB 3.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.1/4.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.5/4.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.6/4.7 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.7/4.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.2/4.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.5/4.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.8/4.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.9/4.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.9/4.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.9/4.7 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.9/4.7 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.2/4.7 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.2/4.7 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.2/4.7 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.2/4.7 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.3/4.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.6/4.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.7/4.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.7/4.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.7/4.7 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.9/4.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.0/4.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.0/4.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.0/4.7 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.2/4.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.4/4.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.4/4.7 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 4.4/4.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.6/4.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.7/4.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.3/181.3 kB 11.4 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.1/83.1 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: python-dotenv, pyasn1, proto-plus, grpcio, googleapis-common-protos, cachetools, rsa, pyasn1-modules, grpcio-status, grpc-google-iam-v1, google-auth, google-api-core, google-cloud-core, google-cloud-translate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ensorflow (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ensorflow (C:\\Python312\\Lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python312\\\\Scripts\\\\dotenv.exe' -> 'C:\\\\Python312\\\\Scripts\\\\dotenv.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install Google Cloud Translation library\n",
    "!pip install google-cloud-translate python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7268b6",
   "metadata": {},
   "source": [
    "### Configure GCP Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995fc76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GCP CONFIGURATION\n",
      "======================================================================\n",
      "‚úÖ Service Account Key: C:\\Users\\User\\Downloads\\multilingualtranlation-c44ef17df27a.json\n",
      "‚úÖ Project ID: multilingualtranlation\n",
      "\n",
      "üîÑ Testing connection to GCP Translation API...\n",
      "‚úÖ GCP Translation API is working!\n",
      "   Test: 'Bonjour' ‚Üí 'Good morning'\n",
      "\n",
      "======================================================================\n",
      "‚úÖ GCP READY - You can now run translation cells\n",
      "======================================================================\n",
      "‚úÖ GCP Translation API is working!\n",
      "   Test: 'Bonjour' ‚Üí 'Good morning'\n",
      "\n",
      "======================================================================\n",
      "‚úÖ GCP READY - You can now run translation cells\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE GCP CREDENTIALS\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# Set your GCP credentials\n",
    "SERVICE_ACCOUNT_KEY = r\"C:\\Users\\User\\Downloads\\multilingualtranlation-c44ef17df27a.json\"\n",
    "PROJECT_ID = \"multilingualtranlation\"  # Update this with your actual GCP Project ID\n",
    "\n",
    "# Configure environment variables\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = SERVICE_ACCOUNT_KEY\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GCP CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úÖ Service Account Key: {SERVICE_ACCOUNT_KEY}\")\n",
    "print(f\"‚úÖ Project ID: {PROJECT_ID}\")\n",
    "print()\n",
    "print(\"üîÑ Testing connection to GCP Translation API...\")\n",
    "\n",
    "# Test the connection\n",
    "try:\n",
    "    from google.cloud import translate_v2 as translate\n",
    "    \n",
    "    client = translate.Client()\n",
    "    \n",
    "    # Test translation\n",
    "    test_result = client.translate(\"Bonjour\", target_language=\"en\")\n",
    "    \n",
    "    print(f\"‚úÖ GCP Translation API is working!\")\n",
    "    print(f\"   Test: 'Bonjour' ‚Üí '{test_result['translatedText']}'\")\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ GCP READY - You can now run translation cells\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not connect to GCP Translation API\")\n",
    "    print(f\"   Error details: {str(e)}\")\n",
    "    print()\n",
    "    print(\"üìã Troubleshooting:\")\n",
    "    print(\"   1. Verify your JSON file path is correct\")\n",
    "    print(\"   2. Check that Translation API is enabled in GCP Console\")\n",
    "    print(\"   3. Verify your Project ID is correct\")\n",
    "    print(\"   4. Install required package: pip install google-cloud-translate\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f928c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2936816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Output paths configured\n",
      "üìÇ Base directory: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\n",
      "\n",
      "üìÑ Lexicon files:\n",
      "   - lexicon_french_cleaned.csv\n",
      "   - lexicon_french_unique.csv\n",
      "   - lexicon_french_enriched.csv\n",
      "   - lexicon_complete_multilingual.csv\n",
      "\n",
      "üìÑ Corpus files:\n",
      "   - corpus_french.csv\n",
      "   - multilingual_corpus.csv\n",
      "   - multilingual_corpus_standard.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OUTPUT PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "# Define all output paths in one place for consistency\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Base directory for outputs\n",
    "BASE_DIR = Path(r\"C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\")\n",
    "\n",
    "# Lexicon output paths\n",
    "LEXICON_CLEANED = BASE_DIR / \"lexicon_french_cleaned.csv\"\n",
    "LEXICON_UNIQUE = BASE_DIR / \"lexicon_french_unique.csv\"\n",
    "LEXICON_ENRICHED = BASE_DIR / \"lexicon_french_enriched.csv\"\n",
    "LEXICON_COMPLETE = BASE_DIR / \"lexicon_complete_multilingual.csv\"\n",
    "\n",
    "# Corpus output paths\n",
    "CORPUS_FRENCH = BASE_DIR / \"corpus_french.csv\"\n",
    "CORPUS_MULTILINGUAL = BASE_DIR / \"multilingual_corpus.csv\"\n",
    "CORPUS_STANDARD = BASE_DIR / \"multilingual_corpus_standard.csv\"\n",
    "CORPUS_TRAIN = BASE_DIR / \"multilingual_train.csv\"\n",
    "CORPUS_TEST = BASE_DIR / \"multilingual_test.csv\"\n",
    "\n",
    "# QC output paths\n",
    "CORPUS_COVERAGE = BASE_DIR / \"corpus_coverage.txt\"\n",
    "OOV_CANDIDATES = BASE_DIR / \"oov_candidates.csv\"\n",
    "CANDIDATE_ENRICHMENT = BASE_DIR / \"candidate_enrichment.csv\"\n",
    "QC_SUMMARY = BASE_DIR / \"corpus_qc_summary.txt\"\n",
    "\n",
    "print(\"‚úÖ Output paths configured\")\n",
    "print(f\"üìÇ Base directory: {BASE_DIR}\")\n",
    "print(f\"\\nüìÑ Lexicon files:\")\n",
    "print(f\"   - {LEXICON_CLEANED.name}\")\n",
    "print(f\"   - {LEXICON_UNIQUE.name}\")\n",
    "print(f\"   - {LEXICON_ENRICHED.name}\")\n",
    "print(f\"   - {LEXICON_COMPLETE.name}\")\n",
    "print(f\"\\nüìÑ Corpus files:\")\n",
    "print(f\"   - {CORPUS_FRENCH.name}\")\n",
    "print(f\"   - {CORPUS_MULTILINGUAL.name}\")\n",
    "print(f\"   - {CORPUS_STANDARD.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8571b",
   "metadata": {},
   "source": [
    "## Step 2: Load Original Lexicon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71d9dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 6963 rows from original lexicon\n",
      "\n",
      "Columns: ['CILUBA', 'French', 'Score', 'Sentiment', 'Nature', 'Unnamed: 5']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CILUBA</th>\n",
       "      <th>French</th>\n",
       "      <th>Score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Nature</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akaja</td>\n",
       "      <td>Arrange</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Akajilula</td>\n",
       "      <td>Rearrange</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akula</td>\n",
       "      <td>Parle</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akulula</td>\n",
       "      <td>Reparle</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aluja</td>\n",
       "      <td>Remet</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amba</td>\n",
       "      <td>Dis</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ambakaja</td>\n",
       "      <td>Supperpose</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ambula</td>\n",
       "      <td>Ramasse</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ambuluja</td>\n",
       "      <td>Depeche</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ambulula</td>\n",
       "      <td>Repete</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Positif</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CILUBA      French  Score Sentiment Nature  Unnamed: 5\n",
       "0      Akaja     Arrange    1.0   Positif  Verbe         NaN\n",
       "1  Akajilula   Rearrange    1.0   Positif  Verbe         NaN\n",
       "2      Akula       Parle    2.0   Positif  Verbe         NaN\n",
       "3    Akulula     Reparle    2.0   Positif  Verbe         NaN\n",
       "4      Aluja       Remet    3.0   Positif  Verbe         NaN\n",
       "5       Amba         Dis    3.0   Positif  Verbe         NaN\n",
       "6   Ambakaja  Supperpose    3.0   Positif  Verbe         NaN\n",
       "7     Ambula     Ramasse    4.0   Positif  Verbe         NaN\n",
       "8   Ambuluja     Depeche    4.0   Positif  Verbe         NaN\n",
       "9   Ambulula      Repete    9.0   Positif  Verbe         NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD ORIGINAL LEXICON\n",
    "# ============================================================================\n",
    "# Load the original lexicon file (6000 words)\n",
    "# This contains French words with sentiment scores\n",
    "\n",
    "lexicon_path = r\"C:\\Users\\User\\Desktop\\Assignment 3\\lexicon_original.csv\"\n",
    "\n",
    "if not Path(lexicon_path).exists():\n",
    "    # Fallback to xlsx if CSV not available\n",
    "    lexicon_path = \"lexicon_6000 words.xlsx\"\n",
    "    df_original = pd.read_excel(lexicon_path)\n",
    "else:\n",
    "    # Try different encodings for French text files\n",
    "    try:\n",
    "        df_original = pd.read_csv(lexicon_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df_original = pd.read_csv(lexicon_path, encoding='latin-1')\n",
    "        except UnicodeDecodeError:\n",
    "            df_original = pd.read_csv(lexicon_path, encoding='cp1252')\n",
    "\n",
    "print(f\"üìä Loaded {len(df_original)} rows from original lexicon\")\n",
    "print(f\"\\nColumns: {df_original.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af786d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Data Quality Summary:\n",
      "============================================================\n",
      "\n",
      "Total entries: 6963\n",
      "\n",
      "Missing values per column:\n",
      "CILUBA           2\n",
      "French           9\n",
      "Score            5\n",
      "Sentiment        3\n",
      "Nature           1\n",
      "Unnamed: 5    6963\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "CILUBA         object\n",
      "French         object\n",
      "Score         float64\n",
      "Sentiment      object\n",
      "Nature         object\n",
      "Unnamed: 5    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIAL DATA QUALITY CHECK\n",
    "# ============================================================================\n",
    "print(\"üìã Data Quality Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal entries: {len(df_original)}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df_original.isnull().sum())\n",
    "print(f\"\\nData types:\")\n",
    "print(df_original.dtypes)\n",
    "\n",
    "# Check for duplicates\n",
    "if 'french' in df_original.columns:\n",
    "    duplicates = df_original['french'].duplicated().sum()\n",
    "    print(f\"\\nüîç Duplicate French entries: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55275496",
   "metadata": {},
   "source": [
    "## Step 3: Clean French Entries\n",
    "\n",
    "This step normalizes the French text:\n",
    "- Removes extra whitespace\n",
    "- Fixes encoding issues\n",
    "- Standardizes sentiment labels\n",
    "- Validates part-of-speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6c91d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned French column\n",
      "‚úÖ Standardized sentiment labels\n",
      "\n",
      "üìä After cleaning: 6954 entries\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    4574\n",
      "neutral     1242\n",
      "negative    1138\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLEAN FRENCH COLUMN\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text entries\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Lowercase for consistency\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def fix_sentiment(s):\n",
    "    \"\"\"Standardize sentiment labels\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return \"neutral\"\n",
    "    s = str(s).strip().lower()\n",
    "    if s in ['positif', 'positive', 'pos', '+']:\n",
    "        return \"positive\"\n",
    "    elif s in ['negatif', 'negative', 'n√©gatif', 'neg', '-']:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply cleaning\n",
    "df_clean = df_original.copy()\n",
    "\n",
    "# Normalize column names to lowercase\n",
    "df_clean.columns = df_clean.columns.str.lower().str.strip()\n",
    "\n",
    "# Clean French column\n",
    "if 'french' in df_clean.columns:\n",
    "    df_clean['french_original'] = df_clean['french']\n",
    "    df_clean['french'] = df_clean['french'].apply(clean_text)\n",
    "    print(f\"‚úÖ Cleaned French column\")\n",
    "\n",
    "# Standardize sentiment\n",
    "if 'sentiment' in df_clean.columns:\n",
    "    df_clean['sentiment_original'] = df_clean['sentiment']\n",
    "    df_clean['sentiment'] = df_clean['sentiment'].apply(fix_sentiment)\n",
    "    print(f\"‚úÖ Standardized sentiment labels\")\n",
    "\n",
    "# Remove empty French entries\n",
    "df_clean = df_clean[df_clean['french'].str.len() > 0]\n",
    "\n",
    "print(f\"\\nüìä After cleaning: {len(df_clean)} entries\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_clean['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e88c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved cleaned lexicon to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\lexicon_french_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned version\n",
    "LEXICON_CLEANED.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_clean.to_csv(LEXICON_CLEANED, index=False)\n",
    "print(f\"üíæ Saved cleaned lexicon to: {LEXICON_CLEANED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91062a86",
   "metadata": {},
   "source": [
    "## Step 4: Create Unique Entries\n",
    "\n",
    "Deduplicate the lexicon by:\n",
    "- Grouping by normalized French text\n",
    "- Keeping the best representative for each group\n",
    "- Prioritizing entries with complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2660035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Identifying duplicates...\n",
      "‚úÖ 'french' column verified\n",
      "\n",
      "üìä Deduplication results:\n",
      "   Original entries: 6954\n",
      "   Unique entries: 3651\n",
      "   Removed duplicates: 3303\n",
      "\n",
      "üìã Columns in df_unique: ['ciluba', 'french', 'score', 'sentiment', 'nature', 'unnamed: 5', 'french_original', 'sentiment_original']\n",
      "\n",
      "üîç Example deduplicated entries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ciluba</th>\n",
       "      <th>french</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>nature</th>\n",
       "      <th>unnamed: 5</th>\n",
       "      <th>french_original</th>\n",
       "      <th>sentiment_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tshishikula tshimue ne mbombo inayi ne binunu ...</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tshiendelele</td>\n",
       "      <td>a jamais</td>\n",
       "      <td>9.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Jamais</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kupuekela</td>\n",
       "      <td>abaissement</td>\n",
       "      <td>3.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abaissement</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kulekela</td>\n",
       "      <td>abandon</td>\n",
       "      <td>3.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abandon</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kulekela</td>\n",
       "      <td>abandonner</td>\n",
       "      <td>4.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abandonner</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mukumike</td>\n",
       "      <td>abattu</td>\n",
       "      <td>3.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abattu</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lubulubulu</td>\n",
       "      <td>abeille</td>\n",
       "      <td>5.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abeille</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kubenga</td>\n",
       "      <td>abhorrer</td>\n",
       "      <td>4.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abhorrer</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kunianga</td>\n",
       "      <td>abimer</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Verbe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abimer</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tshidi tshinyanguka</td>\n",
       "      <td>abim√©</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Mot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abim√©</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              ciluba       french  score  \\\n",
       "0  tshishikula tshimue ne mbombo inayi ne binunu ...          144    0.0   \n",
       "1                                       Tshiendelele     a jamais    9.0   \n",
       "2                                          Kupuekela  abaissement    3.0   \n",
       "3                                           Kulekela      abandon    3.0   \n",
       "4                                           kulekela   abandonner    4.0   \n",
       "5                                           Mukumike       abattu    3.0   \n",
       "6                                         Lubulubulu      abeille    5.0   \n",
       "7                                            kubenga     abhorrer    4.0   \n",
       "8                                           Kunianga       abimer   -4.0   \n",
       "9                                tshidi tshinyanguka        abim√©    0.0   \n",
       "\n",
       "  sentiment nature  unnamed: 5 french_original sentiment_original  \n",
       "0   neutral    Mot         NaN             144             Neutre  \n",
       "1  positive    Mot         NaN        A Jamais            Positif  \n",
       "2  positive    Mot         NaN     abaissement            Positif  \n",
       "3  positive    Mot         NaN         abandon            Positif  \n",
       "4  positive  Verbe         NaN      Abandonner            Positif  \n",
       "5  positive    Mot         NaN          abattu            Positif  \n",
       "6  positive    Mot         NaN         Abeille            Positif  \n",
       "7  positive  Verbe         NaN        Abhorrer            Positif  \n",
       "8  negative  Verbe         NaN          abimer            Negatif  \n",
       "9   neutral    Mot         NaN           abim√©             Neutre  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE UNIQUE ENTRIES\n",
    "# ============================================================================\n",
    "\n",
    "def priority_from_score(score):\n",
    "    \"\"\"Assign priority based on sentiment score strength\"\"\"\n",
    "    try:\n",
    "        score = float(score)\n",
    "        if abs(score) >= 0.5:\n",
    "            return 1  # Strong sentiment\n",
    "        elif abs(score) >= 0.2:\n",
    "            return 2  # Medium sentiment\n",
    "        else:\n",
    "            return 3  # Weak sentiment\n",
    "    except:\n",
    "        return 4  # No valid score\n",
    "\n",
    "def pick_representative(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Select the best representative entry from a group of duplicates\"\"\"\n",
    "    # Priority rules:\n",
    "    # 1. Has sentiment score\n",
    "    # 2. Has English translation\n",
    "    # 3. Has nature/POS tag\n",
    "    # 4. Strongest sentiment score\n",
    "    \n",
    "    if len(group) == 1:\n",
    "        return group.iloc[0]\n",
    "    \n",
    "    # Add priority column\n",
    "    if 'score' in group.columns:\n",
    "        group = group.copy()\n",
    "        group['_priority'] = group['score'].apply(priority_from_score)\n",
    "        # Sort by priority, then by completeness\n",
    "        group = group.sort_values('_priority')\n",
    "    \n",
    "    return group.iloc[0]\n",
    "\n",
    "# Group by French word and pick best representative\n",
    "print(\"üîç Identifying duplicates...\")\n",
    "\n",
    "# Use drop_duplicates with a custom key function approach\n",
    "# First, add priority scores to help with deduplication\n",
    "df_clean_copy = df_clean.copy()\n",
    "if 'score' in df_clean_copy.columns:\n",
    "    df_clean_copy['_priority'] = df_clean_copy['score'].apply(priority_from_score)\n",
    "else:\n",
    "    df_clean_copy['_priority'] = 4\n",
    "\n",
    "# Sort by french word and priority\n",
    "df_clean_copy = df_clean_copy.sort_values(['french', '_priority'])\n",
    "\n",
    "# Keep first occurrence (best priority) for each french word\n",
    "df_unique = df_clean_copy.drop_duplicates(subset=['french'], keep='first').copy()\n",
    "\n",
    "# Remove temporary priority column\n",
    "if '_priority' in df_unique.columns:\n",
    "    df_unique = df_unique.drop(columns=['_priority'])\n",
    "\n",
    "# Reset index\n",
    "df_unique = df_unique.reset_index(drop=True)\n",
    "\n",
    "# Verify the french column still exists\n",
    "if 'french' not in df_unique.columns:\n",
    "    print(\"‚ö†Ô∏è ERROR: 'french' column missing after deduplication!\")\n",
    "    print(f\"   Available columns: {df_unique.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"‚úÖ 'french' column verified\")\n",
    "\n",
    "print(f\"\\nüìä Deduplication results:\")\n",
    "print(f\"   Original entries: {len(df_clean)}\")\n",
    "print(f\"   Unique entries: {len(df_unique)}\")\n",
    "print(f\"   Removed duplicates: {len(df_clean) - len(df_unique)}\")\n",
    "\n",
    "# Verify we have the expected columns\n",
    "print(f\"\\nüìã Columns in df_unique: {df_unique.columns.tolist()}\")\n",
    "\n",
    "# Show examples of deduplicated groups\n",
    "print(f\"\\nüîç Example deduplicated entries:\")\n",
    "df_unique.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6886816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved unique lexicon to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\lexicon_french_unique.csv\n"
     ]
    }
   ],
   "source": [
    "# Save unique version\n",
    "df_unique.to_csv(LEXICON_UNIQUE, index=False)\n",
    "print(f\"üíæ Saved unique lexicon to: {LEXICON_UNIQUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec88efd",
   "metadata": {},
   "source": [
    "## Step 5: GCP Translation (French ‚Üí English)\n",
    "\n",
    "**Note**: This step requires Google Cloud Platform credentials.  \n",
    "If you don't have GCP access, skip to Step 6 which uses pre-translated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "158b3100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GCP credentials found - translation available\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GCP TRANSLATION SETUP (OPTIONAL)\n",
    "# ============================================================================\n",
    "# This cell checks if GCP credentials are available\n",
    "# If not available, we'll use pre-translated data\n",
    "\n",
    "GCP_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv())\n",
    "    \n",
    "    if os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\") and os.getenv(\"GOOGLE_CLOUD_PROJECT\"):\n",
    "        from google.cloud import translate\n",
    "        GCP_AVAILABLE = True\n",
    "        print(\"‚úÖ GCP credentials found - translation available\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GCP credentials not found - will use pre-translated data\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è GCP not available: {e}\")\n",
    "    print(\"   Will use pre-translated data instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c394481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available columns in df_unique: ['ciluba', 'french', 'score', 'sentiment', 'nature', 'unnamed: 5', 'french_original', 'sentiment_original']\n",
      "\n",
      "üåç Translating 3651 entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:17<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Translation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRANSLATE FRENCH ‚Üí ENGLISH (if GCP available)\n",
    "# ============================================================================\n",
    "\n",
    "if GCP_AVAILABLE:\n",
    "    from google.cloud import translate\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Idiom overrides for better translation\n",
    "    POST_OVERRIDE = {\n",
    "        \"pas mal\": \"pretty good\",\n",
    "        \"pas mauvais\": \"not bad (pretty good)\",\n",
    "        \"pas terrible\": \"not great\",\n",
    "        \"au top\": \"top-notch\",\n",
    "        \"trop bien\": \"awesome\",\n",
    "        \"bof\": \"meh\",\n",
    "        \"nickel\": \"spotless\",\n",
    "    }\n",
    "    \n",
    "    def translate_batch(client, parent, texts, batch_size=100):\n",
    "        \"\"\"Translate a list of texts using GCP Translation API\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                resp = client.translate_text(\n",
    "                    request={\n",
    "                        \"parent\": parent,\n",
    "                        \"contents\": batch,\n",
    "                        \"mime_type\": \"text/plain\",\n",
    "                        \"source_language_code\": \"fr\",\n",
    "                        \"target_language_code\": \"en\",\n",
    "                    }\n",
    "                )\n",
    "                results.extend([t.translated_text for t in resp.translations])\n",
    "            except Exception as e:\n",
    "                print(f\"Error translating batch {i}: {e}\")\n",
    "                results.extend([\"\"] * len(batch))\n",
    "            \n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Setup client\n",
    "    project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "    parent = f\"projects/{project_id}/locations/global\"\n",
    "    client = translate.TranslationServiceClient()\n",
    "    \n",
    "    # Translate entries that need it\n",
    "    df_translated = df_unique.copy()\n",
    "    \n",
    "    # Debug: Check what columns we have\n",
    "    print(f\"üìã Available columns in df_unique: {df_translated.columns.tolist()}\")\n",
    "    \n",
    "    # Determine which column has the French text\n",
    "    french_col = None\n",
    "    if 'french' in df_translated.columns:\n",
    "        french_col = 'french'\n",
    "    elif 'French' in df_translated.columns:\n",
    "        french_col = 'French'\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ERROR: No French column found!\")\n",
    "        print(f\"   Available columns: {df_translated.columns.tolist()}\")\n",
    "        raise ValueError(\"Cannot find French column in dataframe\")\n",
    "    \n",
    "    if 'english' not in df_translated.columns:\n",
    "        df_translated['english'] = \"\"\n",
    "    \n",
    "    # Find entries needing translation\n",
    "    needs_translation = df_translated['english'].fillna(\"\").str.strip() == \"\"\n",
    "    to_translate = df_translated.loc[needs_translation, french_col].tolist()\n",
    "    \n",
    "    print(f\"\\nüåç Translating {len(to_translate)} entries...\")\n",
    "    \n",
    "    if len(to_translate) > 0:\n",
    "        translations = translate_batch(client, parent, to_translate)\n",
    "        \n",
    "        # Apply translations\n",
    "        df_translated.loc[needs_translation, 'english'] = translations\n",
    "        \n",
    "        # Apply idiom overrides\n",
    "        for french, english in POST_OVERRIDE.items():\n",
    "            mask = df_translated[french_col] == french\n",
    "            df_translated.loc[mask, 'english'] = english\n",
    "        \n",
    "        print(f\"‚úÖ Translation complete\")\n",
    "    \n",
    "    df_enriched = df_translated\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GCP translation - will load pre-translated data in next step\")\n",
    "    df_enriched = df_unique.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09f4b3",
   "metadata": {},
   "source": [
    "## Step 6: Load or Verify Enriched Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e10ed0",
   "metadata": {},
   "source": [
    "## Step 5b: Translate English ‚Üí Afrikaans (Optional)\n",
    "\n",
    "**Note**: This step translates the English translations to Afrikaans.  \n",
    "Requires GCP credentials. If not available, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f872cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Starting English ‚Üí Afrikaans translation...\n",
      "üî§ Found 3651 English words to translate to Afrikaans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating to Afrikaans: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [00:33<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Afrikaans translation complete!\n",
      "\n",
      "üìã Sample Translation Chain (French ‚Üí English ‚Üí Afrikaans):\n",
      "----------------------------------------------------------------------\n",
      "  144             ‚Üí 144                  ‚Üí 144\n",
      "  a jamais        ‚Üí forever              ‚Üí vir altyd\n",
      "  abaissement     ‚Üí lowering             ‚Üí verlaging\n",
      "  abandon         ‚Üí abandon              ‚Üí laat vaar\n",
      "  abandonner      ‚Üí give up              ‚Üí gee op\n",
      "  abattu          ‚Üí shot                 ‚Üí skoot\n",
      "  abeille         ‚Üí bee                  ‚Üí by\n",
      "  abhorrer        ‚Üí abhor                ‚Üí verafsku\n",
      "  abimer          ‚Üí damage               ‚Üí skade\n",
      "  abim√©           ‚Üí abyss                ‚Üí afgrond\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRANSLATE ENGLISH ‚Üí AFRIKAANS (if GCP available)\n",
    "# ============================================================================\n",
    "\n",
    "if GCP_AVAILABLE:\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(\"\\nüåç Starting English ‚Üí Afrikaans translation...\")\n",
    "    \n",
    "    # Check if we have English translations\n",
    "    if 'english' not in df_enriched.columns:\n",
    "        print(\"‚ö†Ô∏è No English column found. Please complete French ‚Üí English translation first.\")\n",
    "    else:\n",
    "        # Get English words that need translation\n",
    "        english_words = df_enriched['english'].dropna().tolist()\n",
    "        english_words = [w for w in english_words if str(w).strip() != \"\"]\n",
    "        \n",
    "        print(f\"üî§ Found {len(english_words)} English words to translate to Afrikaans\")\n",
    "        \n",
    "        if len(english_words) > 0:\n",
    "            all_translations = []\n",
    "            batch_size = 50  # Translate 50 words at a time\n",
    "            total_words = len(english_words)\n",
    "            \n",
    "            # Get the translate client if not already initialized\n",
    "            from google.cloud import translate\n",
    "            project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "            client = translate.TranslationServiceClient()\n",
    "            \n",
    "            for i in tqdm(range(0, total_words, batch_size), desc=\"Translating to Afrikaans\"):\n",
    "                batch_words = english_words[i:i + batch_size]\n",
    "                \n",
    "                try:\n",
    "                    resp = client.translate_text(\n",
    "                        request={\n",
    "                            \"parent\": parent,\n",
    "                            \"contents\": batch_words,\n",
    "                            \"mime_type\": \"text/plain\",\n",
    "                            \"source_language_code\": \"en\",\n",
    "                            \"target_language_code\": \"af\",  # Afrikaans\n",
    "                        }\n",
    "                    )\n",
    "                    all_translations.extend([t.translated_text for t in resp.translations])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error translating batch {i}: {e}\")\n",
    "                    all_translations.extend([\"\"] * len(batch_words))\n",
    "                \n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "            print(f\"‚úÖ Afrikaans translation complete!\")\n",
    "            \n",
    "            # Add Afrikaans column to dataframe\n",
    "            mask = df_enriched['english'].notna() & (df_enriched['english'].str.strip() != \"\")\n",
    "            df_enriched.loc[mask, 'afrikaans'] = all_translations\n",
    "            \n",
    "            # Show sample translations\n",
    "            print(f\"\\nüìã Sample Translation Chain (French ‚Üí English ‚Üí Afrikaans):\")\n",
    "            print(\"-\" * 70)\n",
    "            sample_df = df_enriched[df_enriched['afrikaans'].notna()].head(10)\n",
    "            for idx, row in sample_df.iterrows():\n",
    "                french_word = row['french']\n",
    "                english_word = row['english']\n",
    "                afrikaans_word = row['afrikaans']\n",
    "                print(f\"  {french_word:15} ‚Üí {english_word:20} ‚Üí {afrikaans_word}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No English words available for translation\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Afrikaans translation - GCP not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a23a23",
   "metadata": {},
   "source": [
    "## Step 5c: Translate English ‚Üí Tsonga (Optional)\n",
    "\n",
    "**Note**: This step translates the English translations to Tsonga (Xitsonga).  \n",
    "Requires GCP credentials. If not available, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18556fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Starting English ‚Üí Tsonga translation...\n",
      "üî§ Found 3651 English words to translate to Tsonga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating to Tsonga: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [00:35<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tsonga translation complete!\n",
      "\n",
      "üìã Sample Translation Chain (French ‚Üí English ‚Üí Tsonga):\n",
      "----------------------------------------------------------------------\n",
      "  144             ‚Üí 144                  ‚Üí 144. 144\n",
      "  a jamais        ‚Üí forever              ‚Üí hilaha ku nga heriki\n",
      "  abaissement     ‚Üí lowering             ‚Üí ku ehlisa ehansi\n",
      "  abandon         ‚Üí abandon              ‚Üí lan'wa\n",
      "  abandonner      ‚Üí give up              ‚Üí lan'wa\n",
      "  abattu          ‚Üí shot                 ‚Üí baleserile\n",
      "  abeille         ‚Üí bee                  ‚Üí nyoxi\n",
      "  abhorrer        ‚Üí abhor                ‚Üí nyenyetsa\n",
      "  abimer          ‚Üí damage               ‚Üí onhaka\n",
      "  abim√©           ‚Üí abyss                ‚Üí xidziva xa le hansi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRANSLATE ENGLISH ‚Üí TSONGA (if GCP available)\n",
    "# ============================================================================\n",
    "\n",
    "if GCP_AVAILABLE:\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(\"\\nüåç Starting English ‚Üí Tsonga translation...\")\n",
    "    \n",
    "    # Check if we have English translations\n",
    "    if 'english' not in df_enriched.columns:\n",
    "        print(\"‚ö†Ô∏è No English column found. Please complete French ‚Üí English translation first.\")\n",
    "    else:\n",
    "        # Get English words that need translation\n",
    "        english_words = df_enriched['english'].dropna().tolist()\n",
    "        english_words = [w for w in english_words if str(w).strip() != \"\"]\n",
    "        \n",
    "        print(f\"üî§ Found {len(english_words)} English words to translate to Tsonga\")\n",
    "        \n",
    "        if len(english_words) > 0:\n",
    "            all_translations = []\n",
    "            batch_size = 50  # Translate 50 words at a time\n",
    "            total_words = len(english_words)\n",
    "            \n",
    "            # Get the translate client if not already initialized\n",
    "            from google.cloud import translate\n",
    "            project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "            client = translate.TranslationServiceClient()\n",
    "            \n",
    "            for i in tqdm(range(0, total_words, batch_size), desc=\"Translating to Tsonga\"):\n",
    "                batch_words = english_words[i:i + batch_size]\n",
    "                \n",
    "                try:\n",
    "                    resp = client.translate_text(\n",
    "                        request={\n",
    "                            \"parent\": parent,\n",
    "                            \"contents\": batch_words,\n",
    "                            \"mime_type\": \"text/plain\",\n",
    "                            \"source_language_code\": \"en\",\n",
    "                            \"target_language_code\": \"ts\",  # Tsonga\n",
    "                        }\n",
    "                    )\n",
    "                    all_translations.extend([t.translated_text for t in resp.translations])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error translating batch {i}: {e}\")\n",
    "                    all_translations.extend([\"\"] * len(batch_words))\n",
    "                \n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "            print(f\"‚úÖ Tsonga translation complete!\")\n",
    "            \n",
    "            # Add Tsonga column to dataframe\n",
    "            mask = df_enriched['english'].notna() & (df_enriched['english'].str.strip() != \"\")\n",
    "            df_enriched.loc[mask, 'tsonga'] = all_translations\n",
    "            \n",
    "            # Show sample translations\n",
    "            print(f\"\\nüìã Sample Translation Chain (French ‚Üí English ‚Üí Tsonga):\")\n",
    "            print(\"-\" * 70)\n",
    "            sample_df = df_enriched[df_enriched['tsonga'].notna()].head(10)\n",
    "            for idx, row in sample_df.iterrows():\n",
    "                french_word = row['french']\n",
    "                english_word = row['english']\n",
    "                tsonga_word = row['tsonga']\n",
    "                print(f\"  {french_word:15} ‚Üí {english_word:20} ‚Üí {tsonga_word}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No English words available for translation\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Tsonga translation - GCP not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99e246",
   "metadata": {},
   "source": [
    "## Step 5d: Translate English ‚Üí Zulu (Optional)\n",
    "\n",
    "**Note**: This step translates the English translations to Zulu (isiZulu).  \n",
    "Requires GCP credentials. If not available, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff35aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåç Starting English ‚Üí Zulu translation...\n",
      "üî§ Found 3651 English words to translate to Zulu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating to Zulu: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74/74 [00:39<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Zulu translation complete!\n",
      "\n",
      "üìã Sample Translation Chain (French ‚Üí English ‚Üí Zulu):\n",
      "----------------------------------------------------------------------\n",
      "  144             ‚Üí 144                  ‚Üí 144\n",
      "  a jamais        ‚Üí forever              ‚Üí kuze kube phakade\n",
      "  abaissement     ‚Üí lowering             ‚Üí ukwehlisa\n",
      "  abandon         ‚Üí abandon              ‚Üí lahla\n",
      "  abandonner      ‚Üí give up              ‚Üí Yeka\n",
      "  abattu          ‚Üí shot                 ‚Üí ukudubula\n",
      "  abeille         ‚Üí bee                  ‚Üí inyosi\n",
      "  abhorrer        ‚Üí abhor                ‚Üí nyanya\n",
      "  abimer          ‚Üí damage               ‚Üí umonakalo\n",
      "  abim√©           ‚Üí abyss                ‚Üí kwalasha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TRANSLATE ENGLISH ‚Üí ZULU (if GCP available)\n",
    "# ============================================================================\n",
    "\n",
    "if GCP_AVAILABLE:\n",
    "    import time\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    print(\"\\nüåç Starting English ‚Üí Zulu translation...\")\n",
    "    \n",
    "    # Check if we have English translations\n",
    "    if 'english' not in df_enriched.columns:\n",
    "        print(\"‚ö†Ô∏è No English column found. Please complete French ‚Üí English translation first.\")\n",
    "    else:\n",
    "        # Get English words that need translation\n",
    "        english_words = df_enriched['english'].dropna().tolist()\n",
    "        english_words = [w for w in english_words if str(w).strip() != \"\"]\n",
    "        \n",
    "        print(f\"üî§ Found {len(english_words)} English words to translate to Zulu\")\n",
    "        \n",
    "        if len(english_words) > 0:\n",
    "            all_translations = []\n",
    "            batch_size = 50  # Translate 50 words at a time\n",
    "            total_words = len(english_words)\n",
    "            \n",
    "            # Get the translate client if not already initialized\n",
    "            from google.cloud import translate\n",
    "            project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "            parent = f\"projects/{project_id}/locations/global\"\n",
    "            client = translate.TranslationServiceClient()\n",
    "            \n",
    "            for i in tqdm(range(0, total_words, batch_size), desc=\"Translating to Zulu\"):\n",
    "                batch_words = english_words[i:i + batch_size]\n",
    "                \n",
    "                try:\n",
    "                    resp = client.translate_text(\n",
    "                        request={\n",
    "                            \"parent\": parent,\n",
    "                            \"contents\": batch_words,\n",
    "                            \"mime_type\": \"text/plain\",\n",
    "                            \"source_language_code\": \"en\",\n",
    "                            \"target_language_code\": \"zu\",  # Zulu\n",
    "                        }\n",
    "                    )\n",
    "                    all_translations.extend([t.translated_text for t in resp.translations])\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error translating batch {i}: {e}\")\n",
    "                    all_translations.extend([\"\"] * len(batch_words))\n",
    "                \n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "            print(f\"‚úÖ Zulu translation complete!\")\n",
    "            \n",
    "            # Add Zulu column to dataframe\n",
    "            mask = df_enriched['english'].notna() & (df_enriched['english'].str.strip() != \"\")\n",
    "            df_enriched.loc[mask, 'zulu'] = all_translations\n",
    "            \n",
    "            # Show sample translations\n",
    "            print(f\"\\nüìã Sample Translation Chain (French ‚Üí English ‚Üí Zulu):\")\n",
    "            print(\"-\" * 70)\n",
    "            sample_df = df_enriched[df_enriched['zulu'].notna()].head(10)\n",
    "            for idx, row in sample_df.iterrows():\n",
    "                french_word = row['french']\n",
    "                english_word = row['english']\n",
    "                zulu_word = row['zulu']\n",
    "                print(f\"  {french_word:15} ‚Üí {english_word:20} ‚Üí {zulu_word}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No English words available for translation\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Zulu translation - GCP not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8209187",
   "metadata": {},
   "source": [
    "## Step 5f: Save Complete Multilingual Lexicon\n",
    "\n",
    "Save the final lexicon with all language translations combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3172ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving complete multilingual lexicon...\n",
      "‚úÖ Saved complete multilingual lexicon to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\lexicon_complete_multilingual.csv\n",
      "\n",
      "üìä Lexicon Statistics:\n",
      "   Total entries: 3651\n",
      "   Columns: ['ciluba', 'french', 'score', 'sentiment', 'nature', 'unnamed: 5', 'french_original', 'sentiment_original', 'english', 'afrikaans', 'tsonga', 'zulu']\n",
      "   English: 3651 (100.0%)\n",
      "   Afrikaans: 3651 (100.0%)\n",
      "   Tsonga: 3651 (100.0%)\n",
      "   Zulu: 3651 (100.0%)\n",
      "\n",
      "üìã Sample Multilingual Entries:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Entry 1:\n",
      "  French      : 144\n",
      "  English     : 144\n",
      "  Afrikaans   : 144\n",
      "  Tsonga      : 144. 144\n",
      "  Zulu        : 144\n",
      "\n",
      "Entry 2:\n",
      "  French      : a jamais\n",
      "  English     : forever\n",
      "  Afrikaans   : vir altyd\n",
      "  Tsonga      : hilaha ku nga heriki\n",
      "  Zulu        : kuze kube phakade\n",
      "\n",
      "Entry 3:\n",
      "  French      : abaissement\n",
      "  English     : lowering\n",
      "  Afrikaans   : verlaging\n",
      "  Tsonga      : ku ehlisa ehansi\n",
      "  Zulu        : ukwehlisa\n",
      "\n",
      "Entry 4:\n",
      "  French      : abandon\n",
      "  English     : abandon\n",
      "  Afrikaans   : laat vaar\n",
      "  Tsonga      : lan'wa\n",
      "  Zulu        : lahla\n",
      "\n",
      "Entry 5:\n",
      "  French      : abandonner\n",
      "  English     : give up\n",
      "  Afrikaans   : gee op\n",
      "  Tsonga      : lan'wa\n",
      "  Zulu        : Yeka\n",
      "\n",
      "====================================================================================================\n",
      "üéâ COMPLETE MULTILINGUAL LEXICON READY!\n",
      "üìÇ Location: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\lexicon_complete_multilingual.csv\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE COMPLETE MULTILINGUAL LEXICON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üíæ Saving complete multilingual lexicon...\")\n",
    "\n",
    "# Save the complete lexicon with all languages\n",
    "df_enriched.to_csv(LEXICON_COMPLETE, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved complete multilingual lexicon to: {LEXICON_COMPLETE}\")\n",
    "print(f\"\\nüìä Lexicon Statistics:\")\n",
    "print(f\"   Total entries: {len(df_enriched)}\")\n",
    "print(f\"   Columns: {list(df_enriched.columns)}\")\n",
    "\n",
    "# Count translations for each language\n",
    "translation_stats = {}\n",
    "for lang in ['english', 'afrikaans', 'tsonga', 'zulu']:\n",
    "    if lang in df_enriched.columns:\n",
    "        count = df_enriched[lang].notna().sum()\n",
    "        pct = (count / len(df_enriched)) * 100\n",
    "        translation_stats[lang] = {'count': count, 'percentage': pct}\n",
    "        print(f\"   {lang.capitalize()}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Show sample of complete multilingual entries\n",
    "print(f\"\\nüìã Sample Multilingual Entries:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Select columns that exist\n",
    "available_cols = ['french'] + [lang for lang in ['english', 'afrikaans', 'tsonga', 'zulu'] \n",
    "                                if lang in df_enriched.columns]\n",
    "\n",
    "if len(available_cols) > 1:\n",
    "    sample = df_enriched[available_cols].head(5)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"\\nEntry {idx + 1}:\")\n",
    "        for col in available_cols:\n",
    "            if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                print(f\"  {col.capitalize():12s}: {row[col]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"üéâ COMPLETE MULTILINGUAL LEXICON READY!\")\n",
    "print(f\"üìÇ Location: {LEXICON_COMPLETE}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f73cdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Corpus Creation from Lexicon\n",
    "\n",
    "Create a synthetic French corpus by wrapping lexicon terms in sentence templates based on their sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f81b3016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available columns in df_enriched: ['CILUBA', 'French', 'French_norm', 'Score', 'Score_int', 'Sentiment', 'Sentiment_std', 'Nature', 'Nature_std', 'qa_status', 'english', 'notes', 'source', 'qa_status_translate']\n",
      "üìä Shape: (6963, 14)\n",
      "\n",
      "‚úì Created corpus with 5,000 synthetic French sentences\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "positive    3532\n",
      "neutral      815\n",
      "negative     653\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 examples:\n",
      " id                            text    label lang\n",
      "  1         C'est Arrange, j'adore. positive   fr\n",
      "  2       Rearrange ‚Äî c'est super ! positive   fr\n",
      "  3 Franchement, Parle, c'est bien. positive   fr\n",
      "  4         C'est Reparle, j'adore. positive   fr\n",
      "  5           C'est Remet, j'adore. positive   fr\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Sentence templates for each sentiment\n",
    "TEMPLATES = {\n",
    "    \"positive\": [\n",
    "        \"C'est {t}, j'adore.\",\n",
    "        \"{t} ‚Äî c'est super !\",\n",
    "        \"Franchement, {t}, c'est bien.\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"C'est {t}, je d√©teste.\",\n",
    "        \"{t} ‚Äî c'est nul.\",\n",
    "        \"Honn√™tement, {t}, c'est mauvais.\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"√Ä propos de {t}.\",\n",
    "        \"On parle de {t}.\",\n",
    "        \"{t} est mentionn√©.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "def create_sentence(term, label, nature):\n",
    "    \"\"\"Generate a French sentence using the term and its sentiment.\"\"\"\n",
    "    cand = TEMPLATES.get(label, TEMPLATES[\"neutral\"])\n",
    "    \n",
    "    # Convert term to string and handle NaN/None\n",
    "    t = str(term) if pd.notna(term) else \"\"\n",
    "    if not t or t == \"nan\":\n",
    "        return \"Texte indisponible.\"\n",
    "    \n",
    "    # Special handling for verbs (rough heuristic)\n",
    "    if nature in {\"verb\", \"verbe\"} or t.endswith((\"er\", \"ir\", \"re\")):\n",
    "        pos_tpl = [\n",
    "            \"Il faut {t}, c'est utile.\",\n",
    "            \"{t}, c'est une bonne id√©e.\",\n",
    "            \"On aime {t}.\"\n",
    "        ]\n",
    "        neg_tpl = [\n",
    "            \"√âviter de {t}, ce n'est pas bien.\",\n",
    "            \"{t}, ce n'est pas recommand√©.\",\n",
    "            \"On n'aime pas {t}.\"\n",
    "        ]\n",
    "        neu_tpl = [\n",
    "            \"On discute de {t}.\",\n",
    "            \"{t} est √† consid√©rer.\",\n",
    "            \"{t} est cit√©.\"\n",
    "        ]\n",
    "        if label == \"positive\":\n",
    "            cand = pos_tpl\n",
    "        elif label == \"negative\":\n",
    "            cand = neg_tpl\n",
    "        else:\n",
    "            cand = neu_tpl\n",
    "    \n",
    "    return random.choice(cand).format(t=term)\n",
    "\n",
    "# Load the enriched lexicon\n",
    "df_corpus_source = df_enriched.copy()\n",
    "\n",
    "# Debug: Print available columns\n",
    "print(f\"üìã Available columns in df_enriched: {df_corpus_source.columns.tolist()}\")\n",
    "print(f\"üìä Shape: {df_corpus_source.shape}\")\n",
    "print()\n",
    "\n",
    "# Normalize sentiment labels - use 'sentiment' column if available\n",
    "sentiment_col = None\n",
    "if 'sentiment_std' in df_corpus_source.columns:\n",
    "    sentiment_col = 'sentiment_std'\n",
    "elif 'sentiment' in df_corpus_source.columns:\n",
    "    sentiment_col = 'sentiment'\n",
    "elif 'Sentiment' in df_corpus_source.columns:\n",
    "    sentiment_col = 'Sentiment'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: No sentiment column found, using neutral for all\")\n",
    "    df_corpus_source['sentiment'] = 'neutral'\n",
    "    sentiment_col = 'sentiment'\n",
    "\n",
    "sentiment_map = {\n",
    "    \"positif\": \"positive\", \"positive\": \"positive\", \"pos\": \"positive\",\n",
    "    \"negatif\": \"negative\", \"n√©gatif\": \"negative\", \"negative\": \"negative\", \"neg\": \"negative\",\n",
    "    \"neutral\": \"neutral\", \"neutre\": \"neutral\", \"neu\": \"neutral\",\n",
    "}\n",
    "\n",
    "df_corpus_source[\"label\"] = (\n",
    "    df_corpus_source[sentiment_col]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .map(sentiment_map)\n",
    "    .fillna(\"neutral\")\n",
    ")\n",
    "\n",
    "# Determine which French column to use\n",
    "french_col = None\n",
    "if 'french_norm' in df_corpus_source.columns:\n",
    "    french_col = 'french_norm'\n",
    "elif 'french' in df_corpus_source.columns:\n",
    "    french_col = 'french'\n",
    "elif 'French' in df_corpus_source.columns:\n",
    "    french_col = 'French'\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Available columns: {df_corpus_source.columns.tolist()}\")\n",
    "    raise ValueError(\"No French column found in dataframe!\")\n",
    "\n",
    "# Keep only non-empty French terms\n",
    "df_corpus_source = df_corpus_source[\n",
    "    df_corpus_source[french_col].astype(str).str.strip() != \"\"\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Determine nature column\n",
    "nature_col = 'nature_std' if 'nature_std' in df_corpus_source.columns else None\n",
    "\n",
    "# Generate synthetic corpus (limit to 5000 for manageable size)\n",
    "corpus_rows = []\n",
    "for i, row in df_corpus_source.iterrows():\n",
    "    nature = row.get(nature_col, \"other\") if nature_col else \"other\"\n",
    "    text = create_sentence(\n",
    "        row[french_col],\n",
    "        row[\"label\"],\n",
    "        nature\n",
    "    )\n",
    "    corpus_rows.append({\n",
    "        \"id\": i + 1,\n",
    "        \"text\": text,\n",
    "        \"label\": row[\"label\"],\n",
    "        \"lang\": \"fr\"\n",
    "    })\n",
    "    if len(corpus_rows) >= 5000:\n",
    "        break\n",
    "\n",
    "df_corpus = pd.DataFrame(corpus_rows)\n",
    "\n",
    "print(f\"‚úì Created corpus with {len(df_corpus):,} synthetic French sentences\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_corpus[\"label\"].value_counts())\n",
    "print(f\"\\nFirst 5 examples:\")\n",
    "print(df_corpus.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96d5ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved French corpus to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\corpus_french.csv\n"
     ]
    }
   ],
   "source": [
    "# Save corpus to CSV\n",
    "df_corpus.to_csv(CORPUS_FRENCH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úì Saved French corpus to: {CORPUS_FRENCH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9f9e7",
   "metadata": {},
   "source": [
    "### Step 6b: Corpus Coverage & Quality Analysis\n",
    "\n",
    "Analyze how much of the corpus vocabulary is covered by the lexicon and identify OOV (Out-of-Vocabulary) candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b96e049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORPUS COVERAGE ANALYSIS\n",
      "============================================================\n",
      "Total tokens in corpus:     18,813\n",
      "Tokens in lexicon:          8,789\n",
      "Coverage:                   46.72%\n",
      "Unique OOV tokens:          98\n",
      "\n",
      "Top 20 Out-of-Vocabulary (OOV) tokens:\n",
      "  c'est                 3560\n",
      "  j'adore                796\n",
      "  on                     748\n",
      "  franchement            726\n",
      "  une                    428\n",
      "  id√©e                   428\n",
      "  utile                  380\n",
      "  faut                   379\n",
      "  aime                   372\n",
      "  est                    351\n",
      "  pas                    252\n",
      "  √†                      244\n",
      "  mentionn√©              193\n",
      "  ce                     170\n",
      "  n'est                  169\n",
      "  nul                    149\n",
      "  honn√™tement            134\n",
      "  d√©teste                118\n",
      "  recommand√©              86\n",
      "  √©viter                  83\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenization pattern (matches French words)\n",
    "TOKEN_PATTERN = re.compile(r\"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''-]+\")\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize French text into lowercase words.\"\"\"\n",
    "    return [t.lower().strip(\"'-\") for t in TOKEN_PATTERN.findall(text or \"\") if t.strip(\"'-\")]\n",
    "\n",
    "# Build lexicon vocabulary - use the correct column name\n",
    "french_vocab_col = None\n",
    "if 'french_norm' in df_enriched.columns:\n",
    "    french_vocab_col = 'french_norm'\n",
    "elif 'French_norm' in df_enriched.columns:\n",
    "    french_vocab_col = 'French_norm'\n",
    "elif 'french' in df_enriched.columns:\n",
    "    french_vocab_col = 'french'\n",
    "elif 'French' in df_enriched.columns:\n",
    "    french_vocab_col = 'French'\n",
    "else:\n",
    "    raise ValueError(f\"No French column found. Available: {df_enriched.columns.tolist()}\")\n",
    "\n",
    "lexicon_vocab = set(df_enriched[french_vocab_col].astype(str).str.strip().str.lower())\n",
    "lexicon_vocab.discard(\"\")  # Remove empty strings\n",
    "\n",
    "# Analyze corpus coverage\n",
    "total_tokens = 0\n",
    "in_lexicon = 0\n",
    "oov_counter = Counter()\n",
    "\n",
    "for text in df_corpus[\"text\"]:\n",
    "    tokens = tokenize(text)\n",
    "    total_tokens += len(tokens)\n",
    "    for token in tokens:\n",
    "        if token in lexicon_vocab:\n",
    "            in_lexicon += 1\n",
    "        else:\n",
    "            oov_counter[token] += 1\n",
    "\n",
    "coverage_pct = (in_lexicon / total_tokens * 100.0) if total_tokens > 0 else 0.0\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORPUS COVERAGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total tokens in corpus:     {total_tokens:,}\")\n",
    "print(f\"Tokens in lexicon:          {in_lexicon:,}\")\n",
    "print(f\"Coverage:                   {coverage_pct:.2f}%\")\n",
    "print(f\"Unique OOV tokens:          {len(oov_counter):,}\")\n",
    "print()\n",
    "\n",
    "# Show top OOV words\n",
    "if oov_counter:\n",
    "    print(\"Top 20 Out-of-Vocabulary (OOV) tokens:\")\n",
    "    for token, count in oov_counter.most_common(20):\n",
    "        print(f\"  {token:20s} {count:5d}\")\n",
    "else:\n",
    "    print(\"No OOV tokens found (100% coverage!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a79d3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved coverage report to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\corpus_coverage.txt\n",
      "‚úì Saved 98 OOV candidates to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\oov_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "# Save coverage report\n",
    "coverage_text = f\"\"\"Tokens: {total_tokens:,}\n",
    "In-lex: {in_lexicon:,}\n",
    "Coverage: {coverage_pct:.2f}%\n",
    "Unique OOV: {len(oov_counter):,}\n",
    "\"\"\"\n",
    "\n",
    "with open(CORPUS_COVERAGE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(coverage_text)\n",
    "print(f\"‚úì Saved coverage report to: {CORPUS_COVERAGE}\")\n",
    "\n",
    "# Save OOV candidates\n",
    "if oov_counter:\n",
    "    df_oov = pd.DataFrame(oov_counter.most_common(), columns=[\"token\", \"count\"])\n",
    "    df_oov.to_csv(OOV_CANDIDATES, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úì Saved {len(df_oov):,} OOV candidates to: {OOV_CANDIDATES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00137edb",
   "metadata": {},
   "source": [
    "### Step 6c: Candidate Enrichment Analysis\n",
    "\n",
    "Identify frequent OOV terms and suggest sentiment labels based on their distribution in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "397b46e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Identified 27 candidate terms for lexicon enrichment\n",
      "  (Terms appearing in at least 3 documents)\n",
      "\n",
      "Top 10 candidates by sentiment score:\n",
      "French_norm Nature_std suggested_sentiment  suggested_score  df_pos  df_neg  df_neu\n",
      "     n'aime      other            negative             10.0       0      83       0\n",
      "        pas      other            negative             10.0       0     252       0\n",
      "        nul      other            negative             10.0       0     149       0\n",
      " recommand√©      other            negative             10.0       0      86       0\n",
      "     √©viter      other            negative             10.0       0      83       0\n",
      "honn√™tement      other            negative             10.0       0     134       0\n",
      "         ce      other            negative             10.0       0     170       0\n",
      "      n'est      other            negative             10.0       0     169       0\n",
      "    d√©teste      other            negative             10.0       0     118       0\n",
      "    discute      other             neutral              0.0       0       0      71\n"
     ]
    }
   ],
   "source": [
    "# Count OOV term occurrences by sentiment label\n",
    "sentiment_counts = {label: Counter() for label in [\"positive\", \"negative\", \"neutral\"]}\n",
    "\n",
    "for _, row in df_corpus.iterrows():\n",
    "    token_set = set(tokenize(row[\"text\"]))\n",
    "    for token in token_set:\n",
    "        if token not in lexicon_vocab:\n",
    "            sentiment_counts[row[\"label\"]][token] += 1\n",
    "\n",
    "# Build candidate enrichment list\n",
    "min_doc_freq = 3  # Minimum number of documents a term must appear in\n",
    "candidates = []\n",
    "\n",
    "all_oov_tokens = set()\n",
    "for label_counts in sentiment_counts.values():\n",
    "    all_oov_tokens.update(label_counts.keys())\n",
    "\n",
    "for token in all_oov_tokens:\n",
    "    pos_count = sentiment_counts[\"positive\"][token]\n",
    "    neg_count = sentiment_counts[\"negative\"][token]\n",
    "    neu_count = sentiment_counts[\"neutral\"][token]\n",
    "    total_count = pos_count + neg_count + neu_count\n",
    "    \n",
    "    if total_count < min_doc_freq:\n",
    "        continue\n",
    "    \n",
    "    # Determine suggested sentiment (highest count)\n",
    "    suggested_sentiment = max(\n",
    "        [(\"positive\", pos_count), (\"negative\", neg_count), (\"neutral\", neu_count)],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    # Calculate polarity score (higher = stronger sentiment)\n",
    "    if suggested_sentiment != \"neutral\":\n",
    "        suggested_score = round(10 * abs(pos_count - neg_count) / max(1, total_count), 2)\n",
    "    else:\n",
    "        suggested_score = 0.0\n",
    "    \n",
    "    candidates.append({\n",
    "        \"French_norm\": token,\n",
    "        \"Nature_std\": \"other\",\n",
    "        \"suggested_sentiment\": suggested_sentiment,\n",
    "        \"suggested_score\": suggested_score,\n",
    "        \"df_pos\": pos_count,\n",
    "        \"df_neg\": neg_count,\n",
    "        \"df_neu\": neu_count\n",
    "    })\n",
    "\n",
    "df_candidates = pd.DataFrame(candidates).sort_values(\n",
    "    [\"suggested_sentiment\", \"suggested_score\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Identified {len(df_candidates):,} candidate terms for lexicon enrichment\")\n",
    "print(f\"  (Terms appearing in at least {min_doc_freq} documents)\")\n",
    "print()\n",
    "print(\"Top 10 candidates by sentiment score:\")\n",
    "print(df_candidates.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15994f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved 27 enrichment candidates to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\candidate_enrichment.csv\n"
     ]
    }
   ],
   "source": [
    "# Save candidate enrichment list\n",
    "if len(df_candidates) > 0:\n",
    "    df_candidates.to_csv(CANDIDATE_ENRICHMENT, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"‚úì Saved {len(df_candidates):,} enrichment candidates to: {CANDIDATE_ENRICHMENT}\")\n",
    "else:\n",
    "    print(\"No enrichment candidates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbb0ba",
   "metadata": {},
   "source": [
    "### Step 6d: Generate QC Summary Report\n",
    "\n",
    "Create a comprehensive quality control summary combining coverage stats and top enrichment candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5d9db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved QC summary to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\corpus_qc_summary.txt\n",
      "\n",
      "============================================================\n",
      "CORPUS QC SUMMARY\n",
      "============================================================\n",
      "\n",
      "Tokens: 18,813\n",
      "In-lex: 8,789\n",
      "Coverage: 46.72%\n",
      "Unique OOV: 98\n",
      "\n",
      "Top 10 Enrichment Candidates by Suggested Score:\n",
      "\n",
      "French_norm Nature_std suggested_sentiment  suggested_score  df_pos  df_neg  df_neu\n",
      "     n'aime      other            negative             10.0       0      83       0\n",
      "        pas      other            negative             10.0       0     252       0\n",
      "        nul      other            negative             10.0       0     149       0\n",
      " recommand√©      other            negative             10.0       0      86       0\n",
      "     √©viter      other            negative             10.0       0      83       0\n",
      "honn√™tement      other            negative             10.0       0     134       0\n",
      "         ce      other            negative             10.0       0     170       0\n",
      "      n'est      other            negative             10.0       0     169       0\n",
      "    d√©teste      other            negative             10.0       0     118       0\n",
      "       faut      other            positive             10.0     379       0       0\n"
     ]
    }
   ],
   "source": [
    "# Build QC summary report\n",
    "qc_summary = [\"=\" * 60]\n",
    "qc_summary.append(\"CORPUS QC SUMMARY\")\n",
    "qc_summary.append(\"=\" * 60)\n",
    "qc_summary.append(\"\")\n",
    "qc_summary.append(coverage_text.strip())\n",
    "qc_summary.append(\"\")\n",
    "\n",
    "if len(df_candidates) > 0:\n",
    "    qc_summary.append(\"Top 10 Enrichment Candidates by Suggested Score:\")\n",
    "    qc_summary.append(\"\")\n",
    "    top_10 = df_candidates.sort_values(\"suggested_score\", ascending=False).head(10)\n",
    "    qc_summary.append(top_10.to_string(index=False))\n",
    "else:\n",
    "    qc_summary.append(\"No enrichment candidates found.\")\n",
    "\n",
    "qc_summary_text = \"\\n\".join(qc_summary)\n",
    "\n",
    "# Save QC summary\n",
    "with open(QC_SUMMARY, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(qc_summary_text)\n",
    "\n",
    "print(f\"‚úì Saved QC summary to: {QC_SUMMARY}\")\n",
    "print()\n",
    "print(qc_summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c08bc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Multilingual Corpus Creation\n",
    "\n",
    "Translate a subset of the French corpus into African languages for multilingual model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2746a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Selected 375 French sentences for translation\n",
      "\n",
      "Sentiment distribution:\n",
      "label\n",
      "positive    125\n",
      "negative    125\n",
      "neutral     125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample sentences:\n",
      "  [positive] C'est Arrange, j'adore.\n",
      "  [positive] Rearrange ‚Äî c'est super !\n",
      "  [positive] Franchement, Parle, c'est bien.\n"
     ]
    }
   ],
   "source": [
    "# Select a subset of French sentences for translation (125 per sentiment class)\n",
    "# This creates a manageable multilingual corpus for model training\n",
    "\n",
    "# Balance the corpus by sentiment\n",
    "samples_per_sentiment = 125\n",
    "\n",
    "french_subset = []\n",
    "for sentiment in [\"positive\", \"negative\", \"neutral\"]:\n",
    "    sentiment_samples = df_corpus[df_corpus[\"label\"] == sentiment].head(samples_per_sentiment)\n",
    "    french_subset.append(sentiment_samples)\n",
    "\n",
    "df_french_subset = pd.concat(french_subset, ignore_index=True)\n",
    "\n",
    "print(f\"‚úì Selected {len(df_french_subset)} French sentences for translation\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_french_subset[\"label\"].value_counts())\n",
    "print(f\"\\nSample sentences:\")\n",
    "for i in range(3):\n",
    "    row = df_french_subset.iloc[i]\n",
    "    print(f\"  [{row['label']}] {row['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82883b8c",
   "metadata": {},
   "source": [
    "### Step 7a: Translate to Afrikaans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d96f1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating to Afrikaans...\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Afrikaans\n",
      "\n",
      "Sample Afrikaans translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  AF: Dis Arrange, ek is mal daaroor.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  AF: Herrangskik ‚Äì dis wonderlik!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  AF: Eerlikwaar, om jou stem te laat hoor is goed.\n",
      "  Label: positive\n",
      "\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Afrikaans\n",
      "\n",
      "Sample Afrikaans translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  AF: Dis Arrange, ek is mal daaroor.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  AF: Herrangskik ‚Äì dis wonderlik!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  AF: Eerlikwaar, om jou stem te laat hoor is goed.\n",
      "  Label: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if GCP_AVAILABLE:\n",
    "    from google.cloud import translate_v2 as translate\n",
    "    import time\n",
    "    \n",
    "    # Initialize GCP Translation client\n",
    "    translate_client = translate.Client()\n",
    "    \n",
    "    # Translate French sentences to Afrikaans\n",
    "    afrikaans_translations = []\n",
    "    batch_size = 50\n",
    "    total_batches = (len(df_french_subset) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(\"Translating to Afrikaans...\")\n",
    "    for i in range(0, len(df_french_subset), batch_size):\n",
    "        batch = df_french_subset.iloc[i:i+batch_size]\n",
    "        batch_texts = batch[\"text\"].tolist()\n",
    "        \n",
    "        try:\n",
    "            # Translate batch\n",
    "            results = translate_client.translate(\n",
    "                batch_texts,\n",
    "                source_language=\"fr\",\n",
    "                target_language=\"af\"\n",
    "            )\n",
    "            \n",
    "            # Extract translations\n",
    "            for idx, result in enumerate(results):\n",
    "                row_idx = i + idx\n",
    "                afrikaans_translations.append({\n",
    "                    \"id\": df_french_subset.iloc[row_idx][\"id\"],\n",
    "                    \"source_text\": df_french_subset.iloc[row_idx][\"text\"],\n",
    "                    \"translated_text\": result[\"translatedText\"],\n",
    "                    \"label\": df_french_subset.iloc[row_idx][\"label\"],\n",
    "                    \"source_lang\": \"fr\",\n",
    "                    \"target_lang\": \"af\"\n",
    "                })\n",
    "            \n",
    "            print(f\"  Batch {i//batch_size + 1}/{total_batches} complete ({len(afrikaans_translations)} total)\")\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    df_afrikaans = pd.DataFrame(afrikaans_translations)\n",
    "    print(f\"‚úì Translated {len(df_afrikaans)} sentences to Afrikaans\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample Afrikaans translations:\")\n",
    "    for i in range(3):\n",
    "        row = df_afrikaans.iloc[i]\n",
    "        print(f\"  FR: {row['source_text']}\")\n",
    "        print(f\"  AF: {row['translated_text']}\")\n",
    "        print(f\"  Label: {row['label']}\\n\")\n",
    "else:\n",
    "    print(\"‚ö† GCP Translation not available. Skipping Afrikaans translation.\")\n",
    "    df_afrikaans = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b50e11a",
   "metadata": {},
   "source": [
    "### Step 7b: Translate to Tsonga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e82f5ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating to Tsonga...\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Tsonga\n",
      "\n",
      "Sample Tsonga translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  TS: I Arrange, ndza swi tsakela.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  TS: Hlela nakambe ‚Äî sweswo i swinene!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  TS: Ku vula ntiyiso, ku vulavula i swinene.\n",
      "  Label: positive\n",
      "\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Tsonga\n",
      "\n",
      "Sample Tsonga translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  TS: I Arrange, ndza swi tsakela.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  TS: Hlela nakambe ‚Äî sweswo i swinene!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  TS: Ku vula ntiyiso, ku vulavula i swinene.\n",
      "  Label: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if GCP_AVAILABLE:\n",
    "    # Translate French sentences to Tsonga\n",
    "    tsonga_translations = []\n",
    "    \n",
    "    print(\"Translating to Tsonga...\")\n",
    "    for i in range(0, len(df_french_subset), batch_size):\n",
    "        batch = df_french_subset.iloc[i:i+batch_size]\n",
    "        batch_texts = batch[\"text\"].tolist()\n",
    "        \n",
    "        try:\n",
    "            # Translate batch\n",
    "            results = translate_client.translate(\n",
    "                batch_texts,\n",
    "                source_language=\"fr\",\n",
    "                target_language=\"ts\"\n",
    "            )\n",
    "            \n",
    "            # Extract translations\n",
    "            for idx, result in enumerate(results):\n",
    "                row_idx = i + idx\n",
    "                tsonga_translations.append({\n",
    "                    \"id\": df_french_subset.iloc[row_idx][\"id\"],\n",
    "                    \"source_text\": df_french_subset.iloc[row_idx][\"text\"],\n",
    "                    \"translated_text\": result[\"translatedText\"],\n",
    "                    \"label\": df_french_subset.iloc[row_idx][\"label\"],\n",
    "                    \"source_lang\": \"fr\",\n",
    "                    \"target_lang\": \"ts\"\n",
    "                })\n",
    "            \n",
    "            print(f\"  Batch {i//batch_size + 1}/{total_batches} complete ({len(tsonga_translations)} total)\")\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    df_tsonga = pd.DataFrame(tsonga_translations)\n",
    "    print(f\"‚úì Translated {len(df_tsonga)} sentences to Tsonga\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample Tsonga translations:\")\n",
    "    for i in range(3):\n",
    "        row = df_tsonga.iloc[i]\n",
    "        print(f\"  FR: {row['source_text']}\")\n",
    "        print(f\"  TS: {row['translated_text']}\")\n",
    "        print(f\"  Label: {row['label']}\\n\")\n",
    "else:\n",
    "    print(\"‚ö† GCP Translation not available. Skipping Tsonga translation.\")\n",
    "    df_tsonga = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a621f36",
   "metadata": {},
   "source": [
    "### Step 7c: Translate to Zulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9d7ade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating to Zulu...\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 1/8 complete (50 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 2/8 complete (100 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 3/8 complete (150 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 4/8 complete (200 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 5/8 complete (250 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 6/8 complete (300 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 7/8 complete (350 total)\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Zulu\n",
      "\n",
      "Sample Zulu translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  ZU: I-Hlela, ngiyayithanda.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  ZU: Hlela kabusha ‚Äî kuhle lokho!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  ZU: Eqinisweni, ukukhuluma kahle kuhle.\n",
      "  Label: positive\n",
      "\n",
      "  Batch 8/8 complete (375 total)\n",
      "‚úì Translated 375 sentences to Zulu\n",
      "\n",
      "Sample Zulu translations:\n",
      "  FR: C'est Arrange, j'adore.\n",
      "  ZU: I-Hlela, ngiyayithanda.\n",
      "  Label: positive\n",
      "\n",
      "  FR: Rearrange ‚Äî c'est super !\n",
      "  ZU: Hlela kabusha ‚Äî kuhle lokho!\n",
      "  Label: positive\n",
      "\n",
      "  FR: Franchement, Parle, c'est bien.\n",
      "  ZU: Eqinisweni, ukukhuluma kahle kuhle.\n",
      "  Label: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if GCP_AVAILABLE:\n",
    "    # Translate French sentences to Zulu\n",
    "    zulu_translations = []\n",
    "    \n",
    "    print(\"Translating to Zulu...\")\n",
    "    for i in range(0, len(df_french_subset), batch_size):\n",
    "        batch = df_french_subset.iloc[i:i+batch_size]\n",
    "        batch_texts = batch[\"text\"].tolist()\n",
    "        \n",
    "        try:\n",
    "            # Translate batch\n",
    "            results = translate_client.translate(\n",
    "                batch_texts,\n",
    "                source_language=\"fr\",\n",
    "                target_language=\"zu\"\n",
    "            )\n",
    "            \n",
    "            # Extract translations\n",
    "            for idx, result in enumerate(results):\n",
    "                row_idx = i + idx\n",
    "                zulu_translations.append({\n",
    "                    \"id\": df_french_subset.iloc[row_idx][\"id\"],\n",
    "                    \"source_text\": df_french_subset.iloc[row_idx][\"text\"],\n",
    "                    \"translated_text\": result[\"translatedText\"],\n",
    "                    \"label\": df_french_subset.iloc[row_idx][\"label\"],\n",
    "                    \"source_lang\": \"fr\",\n",
    "                    \"target_lang\": \"zu\"\n",
    "                })\n",
    "            \n",
    "            print(f\"  Batch {i//batch_size + 1}/{total_batches} complete ({len(zulu_translations)} total)\")\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch {i//batch_size + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    df_zulu = pd.DataFrame(zulu_translations)\n",
    "    print(f\"‚úì Translated {len(df_zulu)} sentences to Zulu\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample Zulu translations:\")\n",
    "    for i in range(3):\n",
    "        row = df_zulu.iloc[i]\n",
    "        print(f\"  FR: {row['source_text']}\")\n",
    "        print(f\"  ZU: {row['translated_text']}\")\n",
    "        print(f\"  Label: {row['label']}\\n\")\n",
    "else:\n",
    "    print(\"‚ö† GCP Translation not available. Skipping Zulu translation.\")\n",
    "    df_zulu = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170b9c1",
   "metadata": {},
   "source": [
    "### Step 7d: Combine Multilingual Corpus\n",
    "\n",
    "Combine all language translations into a single multilingual corpus for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "caad184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTILINGUAL CORPUS SUMMARY\n",
      "============================================================\n",
      "Total sentences: 1,500\n",
      "\n",
      "Language distribution:\n",
      "target_lang\n",
      "fr    375\n",
      "af    375\n",
      "ts    375\n",
      "zu    375\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "label\n",
      "positive    500\n",
      "negative    500\n",
      "neutral     500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Breakdown by language and sentiment:\n",
      "label        negative  neutral  positive\n",
      "target_lang                             \n",
      "af                125      125       125\n",
      "fr                125      125       125\n",
      "ts                125      125       125\n",
      "zu                125      125       125\n"
     ]
    }
   ],
   "source": [
    "# Create French source entries\n",
    "df_french_source = df_french_subset.copy()\n",
    "df_french_source[\"source_text\"] = df_french_source[\"text\"]\n",
    "df_french_source[\"translated_text\"] = df_french_source[\"text\"]\n",
    "df_french_source[\"source_lang\"] = \"fr\"\n",
    "df_french_source[\"target_lang\"] = \"fr\"\n",
    "df_french_source = df_french_source[[\"id\", \"source_text\", \"translated_text\", \"label\", \"source_lang\", \"target_lang\"]]\n",
    "\n",
    "# Combine all language versions\n",
    "multilingual_dfs = [df_french_source]\n",
    "\n",
    "if len(df_afrikaans) > 0:\n",
    "    multilingual_dfs.append(df_afrikaans)\n",
    "if len(df_tsonga) > 0:\n",
    "    multilingual_dfs.append(df_tsonga)\n",
    "if len(df_zulu) > 0:\n",
    "    multilingual_dfs.append(df_zulu)\n",
    "\n",
    "df_multilingual = pd.concat(multilingual_dfs, ignore_index=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTILINGUAL CORPUS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total sentences: {len(df_multilingual):,}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df_multilingual[\"target_lang\"].value_counts())\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_multilingual[\"label\"].value_counts())\n",
    "print(f\"\\nBreakdown by language and sentiment:\")\n",
    "print(df_multilingual.groupby([\"target_lang\", \"label\"]).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5ac3baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Saved multilingual corpus to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\multilingual_corpus.csv\n",
      "‚úì Saved standard format corpus to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\multilingual_corpus_standard.csv\n"
     ]
    }
   ],
   "source": [
    "# Save multilingual corpus\n",
    "df_multilingual.to_csv(CORPUS_MULTILINGUAL, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n‚úì Saved multilingual corpus to: {CORPUS_MULTILINGUAL}\")\n",
    "\n",
    "# Also save in standard corpus format (text, label, lang)\n",
    "df_corpus_format = pd.DataFrame({\n",
    "    \"id\": df_multilingual[\"id\"],\n",
    "    \"text\": df_multilingual[\"translated_text\"],\n",
    "    \"label\": df_multilingual[\"label\"],\n",
    "    \"lang\": df_multilingual[\"target_lang\"]\n",
    "})\n",
    "\n",
    "df_corpus_format.to_csv(CORPUS_STANDARD, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"‚úì Saved standard format corpus to: {CORPUS_STANDARD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4d63d",
   "metadata": {},
   "source": [
    "### Step 7f: Enhanced Corpus Format with Sentiment Scoring\n",
    "\n",
    "Add comprehensive sentiment analysis including:\n",
    "- Word-level sentiment scores from lexicon\n",
    "- VADER sentiment analysis\n",
    "- Custom sentiment scoring\n",
    "- Detailed word score breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5d4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì VADER already installed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHECK VADER INSTALLATION\n",
    "# ============================================================================\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"‚úì VADER already installed\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è VADER not found. Please install using: !pip install vaderSentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695da3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lexicon data...\n",
      "‚úì Loaded 3651 lexicon entries\n",
      "Loading multilingual corpus...\n",
      "‚úì Loaded 1500 corpus entries\n",
      "\n",
      "üìä Data Summary:\n",
      "   Lexicon columns: ['ciluba', 'french', 'score', 'sentiment', 'nature', 'unnamed: 5', 'french_original', 'sentiment_original', 'english', 'afrikaans', 'tsonga', 'zulu']\n",
      "   Corpus columns: ['id', 'source_text', 'translated_text', 'label', 'source_lang', 'target_lang']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA IF NOT ALREADY IN MEMORY (after kernel restart)\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths if not already defined\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR = Path(r\"C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\")\n",
    "    LEXICON_COMPLETE = BASE_DIR / \"lexicon_complete_multilingual.csv\"\n",
    "    CORPUS_MULTILINGUAL = BASE_DIR / \"multilingual_corpus.csv\"\n",
    "\n",
    "# Load lexicon if not in memory\n",
    "try:\n",
    "    df_enriched\n",
    "    print(\"‚úì df_enriched already loaded\")\n",
    "except NameError:\n",
    "    print(\"Loading lexicon data...\")\n",
    "    df_enriched = pd.read_csv(LEXICON_COMPLETE, encoding='utf-8-sig')\n",
    "    print(f\"‚úì Loaded {len(df_enriched)} lexicon entries\")\n",
    "\n",
    "# Load multilingual corpus if not in memory\n",
    "try:\n",
    "    df_multilingual\n",
    "    print(\"‚úì df_multilingual already loaded\")\n",
    "except NameError:\n",
    "    print(\"Loading multilingual corpus...\")\n",
    "    df_multilingual = pd.read_csv(CORPUS_MULTILINGUAL, encoding='utf-8-sig')\n",
    "    print(f\"‚úì Loaded {len(df_multilingual)} corpus entries\")\n",
    "\n",
    "print(\"\\nüìä Data Summary:\")\n",
    "print(f\"   Lexicon columns: {df_enriched.columns.tolist()}\")\n",
    "print(f\"   Corpus columns: {df_multilingual.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2a89fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Built lexicon lookups for 5 languages\n",
      "   french: 3651 entries\n",
      "   english: 3091 entries\n",
      "   afrikaans: 2885 entries\n",
      "   tsonga: 2609 entries\n",
      "   zulu: 2659 entries\n",
      "\n",
      "üîÑ Processing multilingual corpus with sentiment scoring...\n",
      "‚úì Enhanced corpus created with 1500 entries\n",
      "   Entries with source word scores: 1496\n",
      "   Entries with target word scores: 1249\n",
      "\n",
      "üìä Sample of enhanced corpus:\n",
      "source_language target_language                        sentence                 translated_text  total_score_avg          word_scores_avg sentiment_avg  total_score_v2           word_scores_v2 sentiment_v2  vader_positive  vader_negative  vader_neutral  vader_compound vader_sentiment  custom_sentiment_numeric  vader_sentiment_numeric\n",
      "             fr              fr         C'est Arrange, j'adore.         C'est Arrange, j'adore.              0.3              arrange:1.0       neutral             0.3              arrange:1.0      neutral           0.000             0.0          1.000           0.000         neutral                         1                        0\n",
      "             fr              fr       Rearrange ‚Äî c'est super !       Rearrange ‚Äî c'est super !              2.7 rearrange:1.0; super:7.0      positive             2.7 rearrange:1.0; super:7.0     positive           0.512             0.0          0.488           0.636        positive                         1                        1\n",
      "             fr              fr Franchement, Parle, c'est bien. Franchement, Parle, c'est bien.              0.8      parle:2.0; bien:1.0      positive             0.8      parle:2.0; bien:1.0     positive           0.000             0.0          1.000           0.000         neutral                         1                        0\n",
      "\n",
      "üìã Columns in enhanced corpus:\n",
      "   1. source_language\n",
      "   2. target_language\n",
      "   3. sentence\n",
      "   4. translated_text\n",
      "   5. total_score_avg\n",
      "   6. word_scores_avg\n",
      "   7. sentiment_avg\n",
      "   8. total_score_v2\n",
      "   9. word_scores_v2\n",
      "   10. sentiment_v2\n",
      "   11. vader_positive\n",
      "   12. vader_negative\n",
      "   13. vader_neutral\n",
      "   14. vader_compound\n",
      "   15. vader_sentiment\n",
      "   16. custom_sentiment_numeric\n",
      "   17. vader_sentiment_numeric\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE ENHANCED CORPUS FORMAT WITH SENTIMENT SCORING\n",
    "# ============================================================================\n",
    "import sys\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Map corpus language codes to lexicon column names\n",
    "lang_mapping = {\n",
    "    'fr': 'french',\n",
    "    'en': 'english',\n",
    "    'af': 'afrikaans',\n",
    "    'ts': 'tsonga',\n",
    "    'zu': 'zulu'\n",
    "}\n",
    "\n",
    "# Build lexicon lookup dictionaries\n",
    "lexicon_lookup = {}\n",
    "for lang_col in ['french', 'english', 'afrikaans', 'tsonga', 'zulu']:\n",
    "    if lang_col in df_enriched.columns:\n",
    "        lexicon_lookup[lang_col] = {}\n",
    "        for _, row in df_enriched.iterrows():\n",
    "            word = str(row.get(lang_col, '')).strip().lower()\n",
    "            if word and word != 'nan':\n",
    "                score = row.get('score', 0)\n",
    "                sentiment = row.get('sentiment', 'neutral')\n",
    "                try:\n",
    "                    score = float(score) if pd.notna(score) else 0\n",
    "                except:\n",
    "                    score = 0\n",
    "                lexicon_lookup[lang_col][word] = {\n",
    "                    'score': score,\n",
    "                    'sentiment': sentiment\n",
    "                }\n",
    "\n",
    "print(f\"‚úì Built lexicon lookups for {len(lexicon_lookup)} languages\")\n",
    "for lang, lookup in lexicon_lookup.items():\n",
    "    print(f\"   {lang}: {len(lookup)} entries\")\n",
    "\n",
    "def tokenize_simple(text):\n",
    "    \"\"\"Simple tokenization for word analysis\"\"\"\n",
    "    return [w.lower().strip() for w in re.findall(r\"[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø''-]+\", str(text)) if w.strip()]\n",
    "\n",
    "def get_word_scores(text, lang_code):\n",
    "    \"\"\"Get word-level sentiment scores from lexicon\"\"\"\n",
    "    # Map language code to lexicon column name\n",
    "    lang_col = lang_mapping.get(lang_code, lang_code)\n",
    "    \n",
    "    tokens = tokenize_simple(text)\n",
    "    word_scores = {}\n",
    "    total_score = 0\n",
    "    \n",
    "    lookup = lexicon_lookup.get(lang_col, {})\n",
    "    for token in tokens:\n",
    "        if token in lookup:\n",
    "            score = lookup[token]['score']\n",
    "            word_scores[token] = score\n",
    "            total_score += score\n",
    "    \n",
    "    return word_scores, total_score\n",
    "\n",
    "def format_word_scores(word_scores):\n",
    "    \"\"\"Format word scores as 'word1:score1; word2:score2'\"\"\"\n",
    "    if not word_scores:\n",
    "        return \"\"\n",
    "    return \"; \".join([f\"{word}:{score}\" for word, score in word_scores.items()])\n",
    "\n",
    "def determine_sentiment_from_score(score):\n",
    "    \"\"\"Determine sentiment label from score\"\"\"\n",
    "    if score > 0.5:\n",
    "        return \"positive\"\n",
    "    elif score < -0.5:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def get_vader_sentiment(text):\n",
    "    \"\"\"Get VADER sentiment scores\"\"\"\n",
    "    scores = vader.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "def sentiment_to_numeric(sentiment):\n",
    "    \"\"\"Convert sentiment label to numeric\"\"\"\n",
    "    mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    return mapping.get(str(sentiment).lower(), 0)\n",
    "\n",
    "# Process multilingual corpus\n",
    "print(\"\\nüîÑ Processing multilingual corpus with sentiment scoring...\")\n",
    "\n",
    "enhanced_rows = []\n",
    "for _, row in df_multilingual.iterrows():\n",
    "    source_text = row['source_text']\n",
    "    translated_text = row['translated_text']\n",
    "    source_lang = row['source_lang']\n",
    "    target_lang = row['target_lang']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Get word scores for both source and target\n",
    "    source_word_scores, source_total = get_word_scores(source_text, source_lang)\n",
    "    target_word_scores, target_total = get_word_scores(translated_text, target_lang)\n",
    "    \n",
    "    # Calculate averages\n",
    "    source_tokens = tokenize_simple(source_text)\n",
    "    target_tokens = tokenize_simple(translated_text)\n",
    "    source_avg = source_total / len(source_tokens) if source_tokens else 0\n",
    "    target_avg = target_total / len(target_tokens) if target_tokens else 0\n",
    "    \n",
    "    # Determine sentiments from scores\n",
    "    source_sentiment = determine_sentiment_from_score(source_avg)\n",
    "    target_sentiment = determine_sentiment_from_score(target_avg)\n",
    "    \n",
    "    # Get VADER scores for target text\n",
    "    vader_scores = get_vader_sentiment(translated_text)\n",
    "    \n",
    "    # Determine VADER sentiment\n",
    "    compound = vader_scores['compound']\n",
    "    if compound >= 0.05:\n",
    "        vader_sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        vader_sentiment = 'negative'\n",
    "    else:\n",
    "        vader_sentiment = 'neutral'\n",
    "    \n",
    "    enhanced_rows.append({\n",
    "        'source_language': source_lang,\n",
    "        'target_language': target_lang,\n",
    "        'sentence': source_text,\n",
    "        'translated_text': translated_text,\n",
    "        'total_score_avg': round(source_avg, 1),\n",
    "        'word_scores_avg': format_word_scores(source_word_scores),\n",
    "        'sentiment_avg': source_sentiment,\n",
    "        'total_score_v2': round(target_avg, 1),\n",
    "        'word_scores_v2': format_word_scores(target_word_scores),\n",
    "        'sentiment_v2': target_sentiment,\n",
    "        'vader_positive': round(vader_scores['pos'], 3),\n",
    "        'vader_negative': round(vader_scores['neg'], 3),\n",
    "        'vader_neutral': round(vader_scores['neu'], 3),\n",
    "        'vader_compound': round(vader_scores['compound'], 3),\n",
    "        'vader_sentiment': vader_sentiment,\n",
    "        'custom_sentiment_numeric': sentiment_to_numeric(label),\n",
    "        'vader_sentiment_numeric': sentiment_to_numeric(vader_sentiment)\n",
    "    })\n",
    "\n",
    "df_enhanced = pd.DataFrame(enhanced_rows)\n",
    "\n",
    "print(f\"‚úì Enhanced corpus created with {len(df_enhanced)} entries\")\n",
    "\n",
    "# Count how many entries have word scores\n",
    "with_source_scores = (df_enhanced['word_scores_avg'] != '').sum()\n",
    "with_target_scores = (df_enhanced['word_scores_v2'] != '').sum()\n",
    "print(f\"   Entries with source word scores: {with_source_scores}\")\n",
    "print(f\"   Entries with target word scores: {with_target_scores}\")\n",
    "\n",
    "print(f\"\\nüìä Sample of enhanced corpus:\")\n",
    "print(df_enhanced.head(3).to_string(index=False))\n",
    "\n",
    "# Show column names\n",
    "print(f\"\\nüìã Columns in enhanced corpus:\")\n",
    "for i, col in enumerate(df_enhanced.columns, 1):\n",
    "    print(f\"   {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc7f014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved enhanced corpus to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\multilingual_corpus_enhanced_final.csv\n",
      "\n",
      "üìä Enhanced Corpus Statistics:\n",
      "   Total entries: 1,500\n",
      "   Languages: ['fr', 'af', 'ts', 'zu']\n",
      "   Entries with source scores: 1496\n",
      "   Entries with target scores: 1249\n",
      "\n",
      "   Sentiment distribution (custom):\n",
      "      negative (-1): 500\n",
      "      neutral (0): 500\n",
      "      positive (1): 500\n",
      "\n",
      "   Sentiment distribution (VADER):\n",
      "vader_sentiment\n",
      "neutral     1461\n",
      "negative      20\n",
      "positive      19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã Sample Enhanced Corpus Entry:\n",
      "   source_language               : fr\n",
      "   target_language               : fr\n",
      "   sentence                      : Rearrange ‚Äî c'est super !\n",
      "   translated_text               : Rearrange ‚Äî c'est super !\n",
      "   total_score_avg               : 2.7\n",
      "   word_scores_avg               : rearrange:1.0; super:7.0\n",
      "   sentiment_avg                 : positive\n",
      "   total_score_v2                : 2.7\n",
      "   word_scores_v2                : rearrange:1.0; super:7.0\n",
      "   sentiment_v2                  : positive\n",
      "   vader_positive                : 0.512\n",
      "   vader_negative                : 0.0\n",
      "   vader_neutral                 : 0.488\n",
      "   vader_compound                : 0.636\n",
      "   vader_sentiment               : positive\n",
      "   custom_sentiment_numeric      : 1\n",
      "   vader_sentiment_numeric       : 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE ENHANCED MULTILINGUAL CORPUS\n",
    "# ============================================================================\n",
    "\n",
    "# Define output path for enhanced corpus\n",
    "CORPUS_ENHANCED = BASE_DIR / \"multilingual_corpus_enhanced_final.csv\"\n",
    "\n",
    "# Save the enhanced corpus\n",
    "df_enhanced.to_csv(CORPUS_ENHANCED, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"‚úì Saved enhanced corpus to: {CORPUS_ENHANCED}\")\n",
    "print(f\"\\nüìä Enhanced Corpus Statistics:\")\n",
    "print(f\"   Total entries: {len(df_enhanced):,}\")\n",
    "print(f\"   Languages: {df_enhanced['target_language'].unique().tolist()}\")\n",
    "print(f\"   Entries with source scores: {(df_enhanced['word_scores_avg'] != '').sum()}\")\n",
    "print(f\"   Entries with target scores: {(df_enhanced['word_scores_v2'] != '').sum()}\")\n",
    "print(f\"\\n   Sentiment distribution (custom):\")\n",
    "for val, count in df_enhanced['custom_sentiment_numeric'].value_counts().sort_index().items():\n",
    "    label = {1: 'positive', 0: 'neutral', -1: 'negative'}.get(val, val)\n",
    "    print(f\"      {label} ({val}): {count}\")\n",
    "print(f\"\\n   Sentiment distribution (VADER):\")\n",
    "print(df_enhanced['vader_sentiment'].value_counts())\n",
    "\n",
    "# Display first few rows in full detail\n",
    "print(f\"\\nüìã Sample Enhanced Corpus Entry:\")\n",
    "sample_row = df_enhanced.iloc[1]  # Show the one with word scores\n",
    "for col in df_enhanced.columns:\n",
    "    print(f\"   {col:30s}: {sample_row[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a986869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED CORPUS FORMAT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "üìã Column Order (matches required format):\n",
      "    1. ‚úì source_language\n",
      "    2. ‚úì target_language\n",
      "    3. ‚úì sentence\n",
      "    4. ‚úì translated_text\n",
      "    5. ‚úì total_score_avg\n",
      "    6. ‚úì word_scores_avg\n",
      "    7. ‚úì sentiment_avg\n",
      "    8. ‚úì total_score_v2\n",
      "    9. ‚úì word_scores_v2\n",
      "   10. ‚úì sentiment_v2\n",
      "   11. ‚úì vader_positive\n",
      "   12. ‚úì vader_negative\n",
      "   13. ‚úì vader_neutral\n",
      "   14. ‚úì vader_compound\n",
      "   15. ‚úì vader_sentiment\n",
      "   16. ‚úì custom_sentiment_numeric\n",
      "   17. ‚úì vader_sentiment_numeric\n",
      "\n",
      "üìä Sample Entries with Word-Level Scores:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Entry 1:\n",
      "  Source (fr): C'est Arrange, j'adore.\n",
      "  Target (fr): C'est Arrange, j'adore.\n",
      "  Total Score (avg): 0.3\n",
      "  Word Scores (avg): arrange:1.0\n",
      "  Sentiment (avg): neutral\n",
      "  Total Score (v2): 0.3\n",
      "  Word Scores (v2): arrange:1.0\n",
      "  Sentiment (v2): neutral\n",
      "  VADER: pos=0.0, neg=0.0, neu=1.0, compound=0.0\n",
      "  VADER Sentiment: neutral\n",
      "\n",
      "Entry 2:\n",
      "  Source (fr): Rearrange ‚Äî c'est super !\n",
      "  Target (fr): Rearrange ‚Äî c'est super !\n",
      "  Total Score (avg): 2.7\n",
      "  Word Scores (avg): rearrange:1.0; super:7.0\n",
      "  Sentiment (avg): positive\n",
      "  Total Score (v2): 2.7\n",
      "  Word Scores (v2): rearrange:1.0; super:7.0\n",
      "  Sentiment (v2): positive\n",
      "  VADER: pos=0.512, neg=0.0, neu=0.488, compound=0.636\n",
      "  VADER Sentiment: positive\n",
      "\n",
      "Entry 3:\n",
      "  Source (fr): Franchement, Parle, c'est bien.\n",
      "  Target (fr): Franchement, Parle, c'est bien.\n",
      "  Total Score (avg): 0.8\n",
      "  Word Scores (avg): parle:2.0; bien:1.0\n",
      "  Sentiment (avg): positive\n",
      "  Total Score (v2): 0.8\n",
      "  Word Scores (v2): parle:2.0; bien:1.0\n",
      "  Sentiment (v2): positive\n",
      "  VADER: pos=0.0, neg=0.0, neu=1.0, compound=0.0\n",
      "  VADER Sentiment: neutral\n",
      "\n",
      "Entry 4:\n",
      "  Source (fr): C'est Reparle, j'adore.\n",
      "  Target (fr): C'est Reparle, j'adore.\n",
      "  Total Score (avg): 0.7\n",
      "  Word Scores (avg): reparle:2.0\n",
      "  Sentiment (avg): positive\n",
      "  Total Score (v2): 0.7\n",
      "  Word Scores (v2): reparle:2.0\n",
      "  Sentiment (v2): positive\n",
      "  VADER: pos=0.0, neg=0.0, neu=1.0, compound=0.0\n",
      "  VADER Sentiment: neutral\n",
      "\n",
      "Entry 5:\n",
      "  Source (fr): C'est Remet, j'adore.\n",
      "  Target (fr): C'est Remet, j'adore.\n",
      "  Total Score (avg): 1.0\n",
      "  Word Scores (avg): remet:3.0\n",
      "  Sentiment (avg): positive\n",
      "  Total Score (v2): 1.0\n",
      "  Word Scores (v2): remet:3.0\n",
      "  Sentiment (v2): positive\n",
      "  VADER: pos=0.0, neg=0.0, neu=1.0, compound=0.0\n",
      "  VADER Sentiment: neutral\n",
      "\n",
      "üìà Statistics by Target Language:\n",
      "--------------------------------------------------------------------------------\n",
      "                total_score_v2           vader_compound                \\\n",
      "                          mean  min  max           mean    min    max   \n",
      "target_language                                                         \n",
      "af                       0.786 -2.7  6.5         -0.011 -0.636  0.000   \n",
      "fr                       0.323 -3.2  5.3          0.017 -0.572  0.636   \n",
      "ts                       0.291 -4.3  3.7          0.001 -0.542  0.459   \n",
      "zu                       0.498 -4.5  5.7         -0.000 -0.026  0.000   \n",
      "\n",
      "                custom_sentiment_numeric  \n",
      "                                    mean  \n",
      "target_language                           \n",
      "af                                   0.0  \n",
      "fr                                   0.0  \n",
      "ts                                   0.0  \n",
      "zu                                   0.0  \n",
      "\n",
      "================================================================================\n",
      "‚úÖ CORPUS SUCCESSFULLY FORMATTED\n",
      "üìÇ Saved to: C:\\Users\\User\\Desktop\\Assignment 3\\Code\\INF-791-Assignment3\\multilingual_corpus_enhanced.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FORMAT VERIFICATION AND STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENHANCED CORPUS FORMAT VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show the exact column order\n",
    "print(f\"\\nüìã Column Order (matches required format):\")\n",
    "required_cols = [\n",
    "    'source_language', 'target_language', 'sentence', 'translated_text',\n",
    "    'total_score_avg', 'word_scores_avg', 'sentiment_avg',\n",
    "    'total_score_v2', 'word_scores_v2', 'sentiment_v2',\n",
    "    'vader_positive', 'vader_negative', 'vader_neutral', 'vader_compound',\n",
    "    'vader_sentiment', 'custom_sentiment_numeric', 'vader_sentiment_numeric'\n",
    "]\n",
    "\n",
    "for i, col in enumerate(required_cols, 1):\n",
    "    status = \"‚úì\" if col in df_enhanced.columns else \"‚úó\"\n",
    "    print(f\"   {i:2d}. {status} {col}\")\n",
    "\n",
    "# Show sample with word scores\n",
    "print(f\"\\nüìä Sample Entries with Word-Level Scores:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Find entries that have word scores\n",
    "with_scores = df_enhanced[df_enhanced['word_scores_avg'].notna() & (df_enhanced['word_scores_avg'] != '')]\n",
    "if len(with_scores) > 0:\n",
    "    for idx, row in with_scores.head(5).iterrows():\n",
    "        print(f\"\\nEntry {idx + 1}:\")\n",
    "        print(f\"  Source ({row['source_language']}): {row['sentence']}\")\n",
    "        print(f\"  Target ({row['target_language']}): {row['translated_text']}\")\n",
    "        print(f\"  Total Score (avg): {row['total_score_avg']}\")\n",
    "        print(f\"  Word Scores (avg): {row['word_scores_avg']}\")\n",
    "        print(f\"  Sentiment (avg): {row['sentiment_avg']}\")\n",
    "        print(f\"  Total Score (v2): {row['total_score_v2']}\")\n",
    "        print(f\"  Word Scores (v2): {row['word_scores_v2']}\")\n",
    "        print(f\"  Sentiment (v2): {row['sentiment_v2']}\")\n",
    "        print(f\"  VADER: pos={row['vader_positive']}, neg={row['vader_negative']}, neu={row['vader_neutral']}, compound={row['vader_compound']}\")\n",
    "        print(f\"  VADER Sentiment: {row['vader_sentiment']}\")\n",
    "else:\n",
    "    print(\"  No entries with word scores found in sample.\")\n",
    "\n",
    "# Statistics by language\n",
    "print(f\"\\nüìà Statistics by Target Language:\")\n",
    "print(\"-\"*80)\n",
    "lang_stats = df_enhanced.groupby('target_language').agg({\n",
    "    'total_score_v2': ['mean', 'min', 'max'],\n",
    "    'vader_compound': ['mean', 'min', 'max'],\n",
    "    'custom_sentiment_numeric': 'mean'\n",
    "}).round(3)\n",
    "print(lang_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CORPUS SUCCESSFULLY FORMATTED\")\n",
    "print(f\"üìÇ Saved to: {BASE_DIR / 'multilingual_corpus_enhanced.csv'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5188c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Diagnostic: Checking word matching between corpus and lexicon\n",
      "================================================================================\n",
      "\n",
      "Sentence: C'est Arrange, j'adore.\n",
      "Language: fr\n",
      "Tokens: [\"c'est\", 'arrange', \"j'adore\"]\n",
      "No lexicon available for language: fr\n",
      "\n",
      "Sentence: Rearrange ‚Äî c'est super !\n",
      "Language: fr\n",
      "Tokens: ['rearrange', \"c'est\", 'super']\n",
      "No lexicon available for language: fr\n",
      "\n",
      "Sentence: Franchement, Parle, c'est bien.\n",
      "Language: fr\n",
      "Tokens: ['franchement', 'parle', \"c'est\", 'bien']\n",
      "No lexicon available for language: fr\n",
      "\n",
      "================================================================================\n",
      "Checking lexicon entries for 'french'...\n",
      "Total entries: 3651\n",
      "Sample entries: ['144', 'a jamais', 'abaissement', 'abandon', 'abandonner', 'abattu', 'abeille', 'abhorrer', 'abimer', 'abim√©', 'abolir', 'abolition', 'abominable', 'abondamment', 'abondance', 'abonder', 'aborder', 'abrogation', 'abr√©ger', 'absence']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: Check Word Matching\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç Diagnostic: Checking word matching between corpus and lexicon\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sample a few sentences\n",
    "sample_sentences = df_multilingual.head(10)\n",
    "\n",
    "for idx, row in sample_sentences.iterrows():\n",
    "    text = row['translated_text']\n",
    "    lang = row['target_lang']\n",
    "    \n",
    "    print(f\"\\nSentence: {text}\")\n",
    "    print(f\"Language: {lang}\")\n",
    "    \n",
    "    tokens = tokenize_simple(text)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    if lang in lexicon_lookup:\n",
    "        print(f\"Lexicon has {len(lexicon_lookup[lang])} entries for {lang}\")\n",
    "        matched = [t for t in tokens if t in lexicon_lookup[lang]]\n",
    "        print(f\"Matched tokens: {matched}\")\n",
    "        if matched:\n",
    "            for token in matched[:3]:  # Show first 3 matches\n",
    "                info = lexicon_lookup[lang][token]\n",
    "                print(f\"  - {token}: score={info['score']}, sentiment={info['sentiment']}\")\n",
    "    else:\n",
    "        print(f\"No lexicon available for language: {lang}\")\n",
    "    \n",
    "    if idx >= 2:  # Just show first 3\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Checking lexicon entries for 'french'...\")\n",
    "if 'french' in lexicon_lookup:\n",
    "    print(f\"Total entries: {len(lexicon_lookup['french'])}\")\n",
    "    print(f\"Sample entries: {list(lexicon_lookup['french'].keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abdc8e",
   "metadata": {},
   "source": [
    "## ‚úÖ Enhanced Corpus Creation Complete!\n",
    "\n",
    "The multilingual corpus has been successfully formatted with all required columns:\n",
    "\n",
    "### üìã Column Structure:\n",
    "1. **source_language** - Source language code (fr, en, af, ts, zu)\n",
    "2. **target_language** - Target language code  \n",
    "3. **sentence** - Original source text\n",
    "4. **translated_text** - Translated text in target language\n",
    "5. **total_score_avg** - Average sentiment score from lexicon (source)\n",
    "6. **word_scores_avg** - Individual word scores from lexicon (source) in format: `word1:score1; word2:score2`\n",
    "7. **sentiment_avg** - Sentiment label derived from score (source): positive/negative/neutral\n",
    "8. **total_score_v2** - Average sentiment score from lexicon (target)\n",
    "9. **word_scores_v2** - Individual word scores from lexicon (target)\n",
    "10. **sentiment_v2** - Sentiment label derived from score (target)\n",
    "11. **vader_positive** - VADER positive sentiment score (0-1)\n",
    "12. **vader_negative** - VADER negative sentiment score (0-1)\n",
    "13. **vader_neutral** - VADER neutral sentiment score (0-1)\n",
    "14. **vader_compound** - VADER compound score (-1 to 1)\n",
    "15. **vader_sentiment** - VADER sentiment classification\n",
    "16. **custom_sentiment_numeric** - Numeric encoding of original label (1=positive, 0=neutral, -1=negative)\n",
    "17. **vader_sentiment_numeric** - Numeric encoding of VADER sentiment\n",
    "\n",
    "### üìä Statistics:\n",
    "- **Total entries**: 1,500\n",
    "- **Languages**: French (fr), Afrikaans (af), Tsonga (ts), Zulu (zu)\n",
    "- **Entries with word-level scores**: 1,496 source / 1,249 target\n",
    "- **Balanced sentiment**: 500 positive, 500 neutral, 500 negative\n",
    "\n",
    "### üìÇ Output File:\n",
    "`multilingual_corpus_enhanced_final.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94e968de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "FORMAT COMPARISON: Required Format vs. Generated Output\n",
      "====================================================================================================\n",
      "\n",
      "üìå YOUR EXAMPLE FORMAT:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "source_language: french\n",
      "target_language: english\n",
      "sentence: Arrange pagne proteger Comportement Seulement\n",
      "translated_text: arrange loincloth protect behavior only\n",
      "total_score_avg: 10.6\n",
      "word_scores_avg: arrange:1.0; pagne:2.0; proteger:3.0; comportement:1.6; seulement:3.0\n",
      "sentiment_avg: positive\n",
      "total_score_v2: 11.6\n",
      "word_scores_v2: arrange:1; pagne:3; proteger:3; comportement:1.6; seulement:3.0\n",
      "sentiment_v2: positive\n",
      "vader_positive: 0\n",
      "vader_negative: 0\n",
      "vader_neutral: 1\n",
      "vader_compound: 0\n",
      "vader_sentiment: neutral\n",
      "custom_sentiment_numeric: 1\n",
      "vader_sentiment_numeric: 0\n",
      "\n",
      "\n",
      "üìå OUR GENERATED FORMAT (Sample Entry):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "source_language: fr\n",
      "target_language: af\n",
      "sentence: C'est Arrange, j'adore.\n",
      "translated_text: Dis Arrange, ek is mal daaroor.\n",
      "total_score_avg: 0.3\n",
      "word_scores_avg: arrange:1.0\n",
      "sentiment_avg: neutral\n",
      "total_score_v2: 2.0\n",
      "word_scores_v2: ek:9.0; mal:3.0\n",
      "sentiment_v2: positive\n",
      "vader_positive: 0.0\n",
      "vader_negative: 0.0\n",
      "vader_neutral: 1.0\n",
      "vader_compound: 0.0\n",
      "vader_sentiment: neutral\n",
      "custom_sentiment_numeric: 1\n",
      "vader_sentiment_numeric: 0\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ ALL COLUMNS PRESENT AND CORRECTLY FORMATTED!\n",
      "====================================================================================================\n",
      "\n",
      "üìÇ Final Output File: multilingual_corpus_enhanced_final.csv\n",
      "üìä Total Rows: 1,500\n",
      "üåç Languages: fr, af, ts, zu\n",
      "üíæ Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL COMPARISON: Your Example vs. Our Output\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"FORMAT COMPARISON: Required Format vs. Generated Output\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìå YOUR EXAMPLE FORMAT:\")\n",
    "print(\"-\"*100)\n",
    "example = \"\"\"\n",
    "source_language: french\n",
    "target_language: english\n",
    "sentence: Arrange pagne proteger Comportement Seulement\n",
    "translated_text: arrange loincloth protect behavior only\n",
    "total_score_avg: 10.6\n",
    "word_scores_avg: arrange:1.0; pagne:2.0; proteger:3.0; comportement:1.6; seulement:3.0\n",
    "sentiment_avg: positive\n",
    "total_score_v2: 11.6\n",
    "word_scores_v2: arrange:1; pagne:3; proteger:3; comportement:1.6; seulement:3.0\n",
    "sentiment_v2: positive\n",
    "vader_positive: 0\n",
    "vader_negative: 0\n",
    "vader_neutral: 1\n",
    "vader_compound: 0\n",
    "vader_sentiment: neutral\n",
    "custom_sentiment_numeric: 1\n",
    "vader_sentiment_numeric: 0\n",
    "\"\"\"\n",
    "print(example)\n",
    "\n",
    "print(\"\\nüìå OUR GENERATED FORMAT (Sample Entry):\")\n",
    "print(\"-\"*100)\n",
    "# Get an entry with word scores from different languages\n",
    "sample = df_enhanced[\n",
    "    (df_enhanced['word_scores_avg'] != '') & \n",
    "    (df_enhanced['target_language'] == 'af')\n",
    "].iloc[0] if len(df_enhanced[df_enhanced['target_language'] == 'af']) > 0 else df_enhanced.iloc[1]\n",
    "\n",
    "for col in df_enhanced.columns:\n",
    "    print(f\"{col}: {sample[col]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"‚úÖ ALL COLUMNS PRESENT AND CORRECTLY FORMATTED!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nüìÇ Final Output File: multilingual_corpus_enhanced_final.csv\")\n",
    "print(f\"üìä Total Rows: {len(df_enhanced):,}\")\n",
    "print(f\"üåç Languages: {', '.join(df_enhanced['target_language'].unique())}\")\n",
    "print(f\"üíæ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb7d46",
   "metadata": {},
   "source": [
    "### Step 7e: Train/Test Split for Model Training\n",
    "\n",
    "Split the multilingual corpus into training and test sets with stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96281c40",
   "metadata": {},
   "source": [
    "## Step 7: Quality Control Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f96cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEXICON QUALITY CONTROL REPORT\n",
      "======================================================================\n",
      "\n",
      "üìä Basic Statistics:\n",
      "   Total entries: 6963\n",
      "   Unique French words: 4406\n",
      "\n",
      "üåç Translation Coverage:\n",
      "   Entries with English: 6786 (97.5%)\n",
      "   Missing English: 177\n",
      "\n",
      "‚ùì Missing Data:\n",
      "   CILUBA              :     2 (  0.0%)\n",
      "   French              :     9 (  0.1%)\n",
      "   French_norm         :     9 (  0.1%)\n",
      "   Score               :     5 (  0.1%)\n",
      "   Score_int           :     5 (  0.1%)\n",
      "   Sentiment           :     3 (  0.0%)\n",
      "   Sentiment_std       :     3 (  0.0%)\n",
      "   Nature              :     1 (  0.0%)\n",
      "   Nature_std          :     1 (  0.0%)\n",
      "   english             :   177 (  2.5%)\n",
      "   notes               :   177 (  2.5%)\n",
      "   source              :   177 (  2.5%)\n",
      "   qa_status_translate :   177 (  2.5%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# QUALITY CONTROL REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LEXICON QUALITY CONTROL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìä Basic Statistics:\")\n",
    "print(f\"   Total entries: {len(df_enriched)}\")\n",
    "print(f\"   Unique French words: {df_enriched['French'].nunique()}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "if 'sentiment' in df_enriched.columns:\n",
    "    print(f\"\\nüòä Sentiment Distribution:\")\n",
    "    sentiment_counts = df_enriched['sentiment'].value_counts()\n",
    "    for sent, count in sentiment_counts.items():\n",
    "        pct = (count / len(df_enriched)) * 100\n",
    "        print(f\"   {sent:12s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Translation coverage\n",
    "if 'english' in df_enriched.columns:\n",
    "    has_english = df_enriched['english'].notna() & (df_enriched['english'].str.strip() != \"\")\n",
    "    coverage = has_english.sum() / len(df_enriched) * 100\n",
    "    print(f\"\\nüåç Translation Coverage:\")\n",
    "    print(f\"   Entries with English: {has_english.sum()} ({coverage:.1f}%)\")\n",
    "    print(f\"   Missing English: {(~has_english).sum()}\")\n",
    "\n",
    "# Score distribution\n",
    "if 'score' in df_enriched.columns:\n",
    "    print(f\"\\nüìà Score Distribution:\")\n",
    "    scores = pd.to_numeric(df_enriched['score'], errors='coerce')\n",
    "    print(f\"   Mean score: {scores.mean():.3f}\")\n",
    "    print(f\"   Median score: {scores.median():.3f}\")\n",
    "    print(f\"   Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "    print(f\"   Strong sentiment (|score| > 0.5): {(scores.abs() > 0.5).sum()}\")\n",
    "\n",
    "# Missing data analysis\n",
    "print(f\"\\n‚ùì Missing Data:\")\n",
    "for col in df_enriched.columns:\n",
    "    missing = df_enriched[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        pct = (missing / len(df_enriched)) * 100\n",
    "        print(f\"   {col:20s}: {missing:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de96ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABW4AAAPdCAYAAAAauvH/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAn5hJREFUeJzs3QecXGW5P/Bn03vvIRB6700QFDWKgnixgY2miBUV9C9gISAoisLlSlUsoFeKBZALCFJEBBGkSQ+QEBII6aQnm2R3/p/3XCd3d7Ob7G5295zd+X4/n4HM7JTnzJndeeY373nfqlKpVAoAAAAAAAqjW94FAAAAAABQn+AWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgswe39913XxxxxBExbty4qKqqiptuummjt7n33ntjr732it69e8c222wTV111VWvrBQCAVtPLAgDQZYPb5cuXx+677x6XXnpps67/8ssvx+GHHx5ve9vb4oknnoivfOUrceKJJ8Ydd9zRmnoBAKDV9LIAAHQWVaVSqdTqG1dVxY033hhHHnlkk9c57bTT4tZbb42nn3563WUf+chHYtGiRXH77be39qEBAGCT6GUBACiyHu39AA8++GBMmjSp3mWHHnpoNlqhKdXV1dmprLa2NhYuXBjDhw/PGmwAADq/NH5g6dKl2bQF3boVc+kFvSwAAHn1s+0e3M6ePTtGjx5d77J0fsmSJbFy5cro27fverc577zz4uyzz27v0gAAKICZM2fGZpttFkWklwUAIK9+tt2D29Y444wz4tRTT113fvHixbH55ptnT8KgQYNyrQ0AgLaRws8JEybEwIEDoyvRywIAVIYl7dzPtntwO2bMmJgzZ069y9L51LQ2NkIhSSv2plND6TaaXQCArqXI0wfoZQEAyKufbffJxA444IC4++6761125513ZpcDAECR6WUBAMhLi4PbZcuWxRNPPJGdkpdffjn794wZM9YdGnbssceuu/5nP/vZmDZtWnz961+P559/Pi677LL47W9/G6ecckpbbgcAAGyUXhYAgC4b3D7yyCOx5557Zqckzd+V/n3mmWdm519//fV1jW+y5ZZbxq233pqNTNh9993jggsuiJ/97GfZarwAANCR9LIAAHQWVaVSqRSdYKLfwYMHZws7mBcMAKBrqJQer1K2EwCg0ixp5z6v3ee4BQAAAACgZQS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAAOgKwe2ll14aEydOjD59+sT+++8fDz/88Aavf9FFF8X2228fffv2jQkTJsQpp5wSq1atam3NAADQanpZAAC6ZHB7/fXXx6mnnhqTJ0+Oxx57LHbfffc49NBDY+7cuY1e/5prronTTz89u/5zzz0XP//5z7P7+MY3vtEW9QMAQLPpZQEA6CyqSqVSqSU3SKMS9t1337jkkkuy87W1tdnIg5NPPjlrahv64he/mDW5d99997rLvvrVr8ZDDz0U999/f6OPUV1dnZ3KlixZkj3G4sWLY9CgQS0pFwCAgko93uDBgzu0x9PLAgDQWfrZFo24Xb16dTz66KMxadKk/7uDbt2y8w8++GCjtznwwAOz25QPQZs2bVrcdtttcdhhhzX5OOedd1620eVTanQBAGBT6GUBAOhMerTkyvPnz4+ampoYPXp0vcvT+eeff77R23zsYx/LbnfQQQdFGty7du3a+OxnP7vBw8vOOOOM7BC2hqMUAACgtfSyAAB0+cXJWuLee++N733ve3HZZZdl84jdcMMNceutt8Y555zT5G169+6dDS+uewIAgI6mlwUAoFOMuB0xYkR079495syZU+/ydH7MmDGN3ubb3/52HHPMMXHiiSdm53fddddYvnx5nHTSSfHNb34zOzwNAADam14WAIDOpEWdZq9evWLvvfeutzhDWtAhnT/ggAMavc2KFSvWa2hTw5y0cF00AABoNb0sAABddsRtkubrOu6442KfffaJ/fbbLy666KJs1MEJJ5yQ/fzYY4+N8ePHZ4syJEcccURceOGFseeee2ar+L700kvZyIV0ebnpBQCAjqCXBQCgywa3Rx99dMybNy/OPPPMmD17duyxxx5x++23r1vkYcaMGfVGJXzrW9+Kqqqq7P+vvfZajBw5Mmt0v/vd77btlgAAwEboZQEA6CyqSp3gGK+0Eu/gwYNj8eLFFncAAOgiKqXHq5TtBACoNEvauc+zmgIAAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAADQFYLbSy+9NCZOnBh9+vSJ/fffPx5++OENXn/RokXxhS98IcaOHRu9e/eO7bbbLm677bbW1gwAAK2mlwUAoDPo0dIbXH/99XHqqafGFVdckTW6F110URx66KExZcqUGDVq1HrXX716dbzzne/Mfvb73/8+xo8fH6+88koMGTKkrbYBAACaRS8LAEBnUVUqlUotuUFqcPfdd9+45JJLsvO1tbUxYcKEOPnkk+P0009f7/qpKf7hD38Yzz//fPTs2bNZj1FdXZ2dypYsWZI9xuLFi2PQoEEtKRcAgIJKPd7gwYM7tMfTywIA0Fn62RZNlZBGHDz66KMxadKk/7uDbt2y8w8++GCjt7n55pvjgAMOyA4vGz16dOyyyy7xve99L2pqapp8nPPOOy/b6PIpNboAALAp9LIAAHQmLQpu58+fnzWpqWmtK52fPXt2o7eZNm1adlhZul2aC+zb3/52XHDBBXHuuec2+ThnnHFGllSXTzNnzmxJmQAAsB69LAAAXXqO25ZKh5+lOcF++tOfRvfu3WPvvfeO1157LTvkbPLkyY3eJi36kE4AAJAnvSwAAJ0iuB0xYkTWsM6ZM6fe5en8mDFjGr1NWn03zQeWble24447ZqMa0uFqvXr1am3tAADQbHpZAAC67FQJqTFNowzuvvvueqMQ0vk091dj3vzmN8dLL72UXa/shRdeyJpgjS4AAB1FLwsAQJcNbpNTTz01rrzyyrj66qvjueeei8997nOxfPnyOOGEE7KfH3vssdm8XmXp5wsXLowvf/nLWZN76623Zgs6pAUeAACgI+llAQDosnPcHn300TFv3rw488wzs0PE9thjj7j99tvXLfIwY8aMbHXesrSK7h133BGnnHJK7LbbbjF+/Pis8T3ttNPadksAAGAj9LIAAHQWVaVSqRQFt2TJkhg8eHC2Ku+gQYPyLgcAgDZQKT1epWwnAEClWdLOfV6Lp0oAAAAAAKB9CW4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAA0BWC20svvTQmTpwYffr0if333z8efvjhZt3uuuuui6qqqjjyyCNb87AAALDJ9LIAAHTJ4Pb666+PU089NSZPnhyPPfZY7L777nHooYfG3LlzN3i76dOnx9e+9rU4+OCDN6VeAABoNb0sAABdNri98MIL49Of/nSccMIJsdNOO8UVV1wR/fr1i1/84hdN3qampiY+/vGPx9lnnx1bbbXVRh+juro6lixZUu8EAACbSi8LAECXDG5Xr14djz76aEyaNOn/7qBbt+z8gw8+2OTtvvOd78SoUaPiU5/6VLMe57zzzovBgwevO02YMKElZQIAwHr0sgAAdNngdv78+dmIg9GjR9e7PJ2fPXt2o7e5//774+c//3lceeWVzX6cM844IxYvXrzuNHPmzJaUCQAA69HLAgDQmfRozztfunRpHHPMMVmjO2LEiGbfrnfv3tkJAADyopcFAKDTBLepYe3evXvMmTOn3uXp/JgxY9a7/tSpU7OFHI444oh1l9XW1v7vA/foEVOmTImtt9669dUDAEAz6WUBAOiyUyX06tUr9t5777j77rvrNa/p/AEHHLDe9XfYYYd46qmn4oknnlh3et/73hdve9vbsn+b7wsAgI6ilwUAoEtPlXDqqafGcccdF/vss0/st99+cdFFF8Xy5cuzlXmTY489NsaPH58tytCnT5/YZZdd6t1+yJAh2f8bXg4AAO1NLwsAQJcNbo8++uiYN29enHnmmdkiDnvssUfcfvvt6xZ5mDFjRrY6LwAAFI1eFgCAzqKqVCqVouCWLFkSgwcPzlblHTRoUN7lAADQBiqlx6uU7QQAqDRL2rnPM5wAAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAA6ArB7aWXXhoTJ06MPn36xP777x8PP/xwk9e98sor4+CDD46hQ4dmp0mTJm3w+gAA0J70sgAAdMng9vrrr49TTz01Jk+eHI899ljsvvvuceihh8bcuXMbvf69994bH/3oR+Mvf/lLPPjggzFhwoR417veFa+99lpb1A8AAM2mlwUAoLOoKpVKpZbcII1K2HfffeOSSy7JztfW1mYN7Mknnxynn376Rm9fU1OTjVZItz/22GMbvU51dXV2KluyZEn2GIsXL45Bgwa1pFwAAAoq9XiDBw/u0B5PLwsAQGfpZ1s04nb16tXx6KOPZoeIrbuDbt2y82kEQnOsWLEi1qxZE8OGDWvyOuedd1620eVTanQBAGBT6GUBAOhMWhTczp8/PxtlMHr06HqXp/OzZ89u1n2cdtppMW7cuHoNc0NnnHFGllSXTzNnzmxJmQAAsB69LAAAnUmPjnyw73//+3Hddddlc4WlxSCa0rt37+wEAABFoZcFAKCwwe2IESOie/fuMWfOnHqXp/NjxozZ4G1/9KMfZc3uXXfdFbvttlvrqgUAgFbSywIA0GWnSujVq1fsvffecffdd6+7LC3okM4fcMABTd7u/PPPj3POOSduv/322GeffTatYgAAaAW9LAAAXXqqhFNPPTWOO+64rGndb7/94qKLLorly5fHCSeckP08ra47fvz4bFGG5Ac/+EGceeaZcc0118TEiRPXzR82YMCA7AQAAB1FLwsAQJcNbo8++uiYN29e1sCmxnWPPfbIRh+UF3mYMWNGtjpv2eWXX56t4PuhD32o3v1Mnjw5zjrrrLbYBgAAaBa9LAAAnUVVqVQqRcEtWbIkBg8enK3KO2jQoLzLAQCgDVRKj1cp2wkAUGmWtHOf16I5bgEAAAAAaH+CWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAB0heD20ksvjYkTJ0afPn1i//33j4cffniD1//d734XO+ywQ3b9XXfdNW677bbW1gsAAJtELwsAQJcMbq+//vo49dRTY/LkyfHYY4/F7rvvHoceemjMnTu30ev//e9/j49+9KPxqU99Kh5//PE48sgjs9PTTz/dFvUDAECz6WUBAOgsqkqlUqklN0ijEvbdd9+45JJLsvO1tbUxYcKEOPnkk+P0009f7/pHH310LF++PG655ZZ1l73pTW+KPfbYI6644opGH6O6ujo7lS1evDg233zzmDlzZgwaNKgl5QIAUFBLlizJ+shFixbF4MGDO+Qx9bIAAHSWfrZHS668evXqePTRR+OMM85Yd1m3bt1i0qRJ8eCDDzZ6m3R5GtVQVxrVcNNNNzX5OOedd16cffbZ612enggAALqWBQsWdEhwq5cFAKAz9bMtCm7nz58fNTU1MXr06HqXp/PPP/98o7eZPXt2o9dPlzclNdN1G+SUWm+xxRYxY8aMDhuNQXG+tTA6pfLY95XLvq9c9n1lKo9EHTZsWIc8nl6WjuZvW+Wy7yuT/V657PvKtbid+9kWBbcdpXfv3tmpodTo+gWoPGmf2++Vyb6vXPZ95bLvK1Ma9dqV6GVpyN+2ymXfVyb7vXLZ95WrWzv1sy261xEjRkT37t1jzpw59S5P58eMGdPobdLlLbk+AAC0B70sAACdSYuC2169esXee+8dd99997rL0oIO6fwBBxzQ6G3S5XWvn9x5551NXh8AANqDXhYAgM6kxVMlpPm6jjvuuNhnn31iv/32i4suuihbafeEE07Ifn7sscfG+PHjs0UZki9/+cvx1re+NS644II4/PDD47rrrotHHnkkfvrTnzb7MdOhZpMnT270kDO6Lvu9ctn3lcu+r1z2fWXKY7/rZelI9n3lsu8rk/1euez7ytW7nfd9ValUKrX0Rpdcckn88Ic/zBZl2GOPPeLHP/5x7L///tnPDjnkkJg4cWJcddVV667/u9/9Lr71rW/F9OnTY9ttt43zzz8/DjvssLbdEgAAaAa9LAAAnUGrglsAAAAAANpP11rCFwAAAACgCxDcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAVTmOD20ksvzVbw7dOnT7aq78MPP7zB66fVfXfYYYfs+rvuumvcdtttHVYr+ez3K6+8Mg4++OAYOnRodpo0adJGXycUV0t/58uuu+66qKqqiiOPPLLda6QY+37RokXxhS98IcaOHRu9e/eO7bbbzt/8Ctn3F110UWy//fbRt2/fmDBhQpxyyimxatWqDquXTXfffffFEUccEePGjcv+dt90000bvc29994be+21V/b7vs0228RVV10VnYFetnLpZyuXfrYy6WUrl1628txXhF62VADXXXddqVevXqVf/OIXpWeeeab06U9/ujRkyJDSnDlzGr3+Aw88UOrevXvp/PPPLz377LOlb33rW6WePXuWnnrqqQ6vnY7b7x/72MdKl156aenxxx8vPffcc6Xjjz++NHjw4NKrr77a4bXTsfu+7OWXXy6NHz++dPDBB5f+4z/+o8PqJb99X11dXdpnn31Khx12WOn+++/PXgP33ntv6Yknnujw2unYff+b3/ym1Lt37+z/ab/fcccdpbFjx5ZOOeWUDq+d1rvttttK3/zmN0s33HBDKbWdN9544wavP23atFK/fv1Kp556atbjXXzxxVnPd/vtt5eKTC9bufSzlUs/W5n0spVLL1uZbitAL1uI4Ha//fYrfeELX1h3vqampjRu3LjSeeed1+j1jzrqqNLhhx9e77L999+/9JnPfKbdayW//d7Q2rVrSwMHDixdffXV7VglRdn3aX8feOCBpZ/97Gel4447TqNbIfv+8ssvL2211Val1atXd2CVFGHfp+u+/e1vr3dZaoDe/OY3t3uttI/mNLtf//rXSzvvvHO9y44++ujSoYceWioyvWzl0s9WLv1sZdLLVi69LJFTL5v7VAmrV6+ORx99NDtMqKxbt27Z+QcffLDR26TL614/OfTQQ5u8PsXTmv3e0IoVK2LNmjUxbNiwdqyUouz773znOzFq1Kj41Kc+1UGVUoR9f/PNN8cBBxyQHV42evTo2GWXXeJ73/te1NTUdGDl5LHvDzzwwOw25UPQpk2blh1WeNhhh3VY3XS8ztjj6WUrl362culnK5NetnLpZWmu9ujxekTO5s+fn/3RSn/E6krnn3/++UZvM3v27Eavny6nc2jNfm/otNNOy+YZafhLQdfb9/fff3/8/Oc/jyeeeKKDqqQo+z41OPfcc098/OMfzxqdl156KT7/+c9nH3InT57cQZWTx77/2Mc+lt3uoIMOSkcHxdq1a+Ozn/1sfOMb3+igqslDUz3ekiVLYuXKldkccUWjl61c+tnKpZ+tTHrZyqWXJc9eNvcRt9Aa3//+97NJ/W+88cZsYnC6rqVLl8YxxxyTLeYxYsSIvMuhg9XW1mYjU37605/G3nvvHUcffXR885vfjCuuuCLv0mhnaVL/NCLlsssui8ceeyxuuOGGuPXWW+Occ87JuzSANqGfrRz62cqll61celnaSu4jbtMbV/fu3WPOnDn1Lk/nx4wZ0+ht0uUtuT7F05r9XvajH/0oa3Tvuuuu2G233dq5UvLe91OnTo3p06dnKznWbYCSHj16xJQpU2LrrbfugMrJ4/c+rb7bs2fP7HZlO+64Y/ZNZjpkqVevXu1eN/ns+29/+9vZh9wTTzwxO7/rrrvG8uXL46STTso+8KTD0+h6murxBg0aVMjRtoletnLpZyuXfrYy6WUrl16WPHvZ3F8p6Q9V+ubp7rvvrvcmls6nuWAaky6ve/3kzjvvbPL6FE9r9nty/vnnZ99Q3X777bHPPvt0ULXkue932GGHeOqpp7LDysqn973vffG2t70t+/eECRM6eAvoyN/7N7/5zdkhZeUPN8kLL7yQNcEa3a6979O8jw0b2vKHnv9dG4CuqDP2eHrZyqWfrVz62cqkl61celmaq116vFIBXHfddaXevXuXrrrqqtKzzz5bOumkk0pDhgwpzZ49O/v5McccUzr99NPXXf+BBx4o9ejRo/SjH/2o9Nxzz5UmT55c6tmzZ+mpp57KcSto7/3+/e9/v9SrV6/S73//+9Lrr7++7rR06dIct4KO2PcNWYW3cvb9jBkzstW2v/jFL5amTJlSuuWWW0qjRo0qnXvuuTluBR2x79N7e9r31157bWnatGmlP//5z6Wtt966dNRRR+W4FbRUeo9+/PHHs1NqOy+88MLs36+88kr287TP074vS/u6X79+pf/3//5f1uNdeumlpe7du5duv/32UpHpZSuXfrZy6Wcrk162cullK9PSAvSyhQhuk4svvri0+eabZ43MfvvtV/rHP/6x7mdvfetbsze2un7729+Wtttuu+z6O++8c+nWW2/NoWo6cr9vscUW2S9Kw1P6g0jn09Lf+bo0upW17//+97+X9t9//6xR2mqrrUrf/e53S2vXrs2hcjpy369Zs6Z01llnZQ1unz59ShMmTCh9/vOfL73xxhs5VU9r/OUvf2n0vbu8r9P/075veJs99tgje52k3/lf/vKXpc5AL1u59LOVSz9bmfSylUsvW3n+UoBetir9p/XjdQEAAAAAaGu5z3ELAAAAAEB9glsAAAAAgIIR3AIAAAAAFIzgFgAAAACgYAS3AAAAAAAFI7gFAAAAACgYwS0AAAAAQMEIbgEAAAAACkZwCwAAAABQMIJbAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCW+jkrrrqqqiqqlp3quuQQw5Zd/nxxx/f6sdIty3fT7pPiv/8nXXWWesec+LEiR3ymAAAtK3Ux5V7utTfkZ/p06fX+9x17733RiXwGoR8CW5hA9Kbcd0356ZOmxKKVpq6gWJzT57frh/KPvDAA3HSSSfFLrvsEkOGDImePXvGiBEj4uCDD47JkyfH1KlT8y4RADpFr9qrV68YPHhwbLXVVjFp0qQ4++yzY+bMmW36uG01OKA1SqVS3HbbbXHMMcfEdtttF4MGDcr6htGjR8c73vGO+MEPfhCvv/56h9bUmXTFPrKS6aGh6+uRdwFA+/nc5z4X733ve7N/pzdzKse73vWuGDBgQPbv9OGtqN5444345Cc/GTfddNN6P1uwYEHcf//92emvf/1rxYxqAIBNsWbNmuy0ZMmSePnll+Puu++Oc845J7797W9np27dOu/YnRRAf+xjH8t6g4bmzp0b99xzT3Z67rnnsqPSoKvSQ0PlENxCCxx99NGxzz77rHd5UUPRVG+RA8Wyyy+/PKZNm5b9e+jQofGNb3yj2c/v6tWrs5EXvXv3bqeKO6cDDzwwOxXZ8uXLs9fDI488su6yMWPGxJFHHhmbb755LF26NB577LHsA2dX4fUKQEf0qosXL87eQ++4446oqanJTmmk5ezZs7O+qzOaM2dOvPWtb83C6LItt9wy3ve+92WjbVOQ9Y9//KPRUBe6kkrsoaGilYAm/eUvfymlX5Py6Ze//OVGb/Pyyy/Xu026j2uvvba03377lfr27VsaMmRI6UMf+lBpxowZ6912zZo1pR/84AelbbbZptSrV6/SVlttVfrud79bWr16dZN1pH/X/Vldb33rW9ddftxxx9X72X333Vc68sgjS+PGjSv17Nmz1L9//9IWW2xReve7312aPHlyadGiReuum25bvp90n/PmzSt97nOfK40dOzarc4cddij99Kc/beWzXL/OVMOGfp5qeeqpp0r/8R//URo2bFh22eOPP55d7/zzz88u33bbbUtDhw4t9ejRozR48ODSvvvuWzr33HNLy5YtW+++Gz6vf/7zn0uHHHJI9nwMGDAgez6efvrp9W63Kc9fXT//+c9LH/7wh7PncPjw4VnNAwcOLO2+++6lr3/969lz3dTrsbFT+bWRatjQc7pw4cLS2WefXdp7771LgwYNyrYhbcv73//+7DloqOHrbNWqVdlzmp7r9BoYP3586atf/Wp2eXOdfvrp9e4z7bvly5evd73XXnutdMUVV6x3+V133VX64Ac/mD12qiE9b3vuuWfpzDPPLC1YsGDd9RYvXlzq16/fBn+PjzrqqHU/nzRpUr2fTZ06tXTyySdn+yjdT58+fUo77rhj6bTTTqu3fzri9ZrceOON2XVSHaNGjSqdeOKJpblz527w97012wFA5+9Vn3322dKWW25Z7zp/+tOf6vWe3/rWt0rvec97sr4zvQ+l96P0nnXQQQeVfvzjH2d9aFnd/qKpU+qFy7V98pOfzN6bx4wZk71Xp1546623Lh1//PGlJ598skXb+pGPfKTe46ReNNXf0AsvvFD67//+7/Uu//3vf1867LDDSqNHj876ntSTH3DAAaUf/ehHjfYfDZ/XX/3qV1l/lt4/0zZceOGF657Dc845pzRx4sQN9sUN36efe+650gc+8IGsB0jPy5vf/ObSnXfeud7tUh9Xvl16/ht64oknSieccEK2/1JtqSfdY489ss8QdXuJlvSRZTfffHPpfe97X7b/ys/Z2972tuz5ra2tbbPPQC3ZjrLp06eXTjrppOxzU7p+7969s172wAMPLJ1yyinZa7+utG1pH5T77VTPdtttl/WAl156aak5GtvG6667Luun0zaOHDky24bZs2evu80vfvGLdddP16n7GSF54403sue2fJ10f0XpoTf2GtzQ59CkuZ9f03OSetT0Oks9avos9tBDD63rX1OtaX+lz2aHHnpo1lu35WsPik5wC+0c3Kamt7HGKIU1K1eu3GBDWj4dccQRbRrcpjfr7t27b7BxS81kY8Hj9ttvnzWmjd0mhZDtHdympiI1cnUftxyEpUZsQ9u06667lpYuXVrvvuv+PDXMVVVV690u3W8Kxtri+WsY3KZGb0P3kxqq1HS1ZXCbGtnNNttsg/fz5S9/ud5tGr7OmnpdH3PMMc3a5+lDYGoSy7dLjVpTQWVjTj311I0+b3UD91RX+Wfvete76t1Xek2kpq7882uuuWbdz2666aZ6oW9jj9Pwg0F7vl4vv/zyRq+bPuTsvPPOjf6+t3Y7AOgaverDDz9c7zp13wfT+8zGeov0hebatWtbHNymL3Q3dL0UGDUWVDZm1qxZ9Xq0FOjV1NQ067ap9rpf0DZ2Sl9kpseoq+7Pm+rXvv3tb2ehWXP64rr9QfmL84a36datW+m3v/1ts4Pbyy67LAshm9qunXbaqfT666+3uI9Mz23d3qmxUxp4UH5dbOpnoJZsRzJnzpwsJN1QfalnKtvY6zaF+c3RcBsPP/zwJvuy8meHtK11e76GIXHdYDeF+BsbBNHRPXRHBLeN/X6lMP6Pf/zjuoEPG/pstimvPegMTJUALXD77bfH/PnzGz0sbcKECY3eJh2ute+++8ahhx4af/nLX7IJ5JMXX3wxm5PoIx/5SHb+97//fVx33XXrbpcWlEg/mzFjRvzmN79p0+346U9/mh0yl+ywww7x4Q9/OHr06JE91hNPPJEdWtOUKVOmRJ8+fbL5c/v27Zsdbrdy5crsZ+eff34211J7evzxx7Na04IU2267bTz//PNZPclmm20Wb3vb22KLLbbIplxI/UI6nO7666/PDil66qmn4rLLLouvf/3rjd532jfp+fjABz6QPQ9p4YvyPFE///nP4/TTT9/k56+hUaNGxRFHHBFbb711DBs2LLp37x6vvfZaVnN63PTvc889N6s7XeeHP/xh/PnPf44777yz0akl0mttQ9auXRvvf//749VXX83Op8dLz2V67tLr8emnn84u/6//+q/Ya6+94thjj23ydZ3uZ6eddspen2mV3ST9+/vf/36MGzdug3X885//zA7jqvs71L9//2Y9Z7/+9a/jwgsvXHd+5513zmqZNWtWXH311dm+Sc9b2o/PPPNMtm9OOOGE7HZJOmwszYOXnvskbXf5NZwWdUj3laTXzkc/+tF1Pys/Tm1tbbadr7zySvY4H/zgB7PXVnou2/P1mvbZKaecsu6+0/N14oknZnMVptdnmsuwMW2xHQB0Xqk32H333eNf//pXdv6+++7L3ivT3/u0QFXqOd/0pjfF+PHjs/ejND9uer/63e9+l/UNd911V/zhD3+Io446at2UV3WnuUpTM9Sdniv1M+X3qTS1wa677ppdlvrG1Nvceuut2Ry0afqgL33pS/Hss89udBtSD/2/OdD/Ou6445o9V+/3vve9+O1vf7vufNrWtB2phrSNSfr3xz/+8Wx+3MY8+uijccABB8Q73/nO7H069cNJmjs4Sdv5lre8Ja688spsOoqN9cXp/lKvlPrp1A+l9/Hq6ursvTktNJXq29gaBX//+9/ji1/8Ynab8na9+93vzu4v9UPpM0t6blMvl3rHlvSRqfZy35ReI6lHSK+h1FOky9NrJD13e+yxx3pTnLX0M1BLtyNJr8d58+at24bU5w0fPjzrBdNr929/+1u9WupOD5IW7EuL66VeK82ZnOos90gtlV7LqZdLC4Gl7StPTZB+N0477bT4xS9+kfV9n/70p7P+OPnZz34Wn//859fdR/k1mKT5mzc2nVZH99AdIfXL6TlKf1suueSS7PW1atWq+I//+I+shvR8pb8X6blr7LNZa1970GnknRxDkTXnm+l0Stdr6hu/dIhG+RCz9P90aHP5Z+kbz7J02Ef58nQYSN1vERt+S7ypI27TIU/ly9NhJA2lb7TrHm5Td8RoOqXRe2UXXXRRvZ8tWbKkXUfcNnz8htKhNrfddlt2WNAFF1xQ+uEPf1h6y1vesu62b3/72+tdv+79TpgwoV79abRk+WfpULa2eP4ajrhN0nXTKN50WF067C7VXHf0RvrWvq6NTYOwoeukw+zrbnMa4VC2YsWKet+op8MBm3qdfeUrX6l3aFvdn6XD6jYmjSZpalTExqS6yrdLo79T3WVpe+reb9reJB3OV/dQ0YsvvnjdbdJhk3UPuyxLh9mVL0+H0tX9hj6Nyqk76jqNCGjv1+t5553X5KGuDf9W1f19b+12ANB1jg5rOOK0bp9ZHsGY3gPS+2iaOiC9H+2yyy7rrp+mPKhrY9PzlKWRm+mQ56uuuirrGdP9Nhzx15zDl9P0Qk29B25Ievy6I/bS1Ah1R4mmaakaOyomaTjis9zP33HHHfV+lvqS8n2m9/Om+uK6z1k6NL48Mjn5zW9+U+92V1555UZHO6bprcqXp0PL645AbjjK+l//+lez+8h0PyNGjFh3nXT4fFP7Io18LD9uaz8DtWY7Ur9cvuwzn/nMetuQRqDWna6g7ujmuiN3y9Lh+M3RcBvT6PXylBHp/+l83RHl5c8Dr7zySr1+69FHH103dVndaRLKlxeph+6IEbdpirCyj370o/V+lv5mlL3pTW9q9LNZa1970FkYcQvtLI2I69mzZ/bv9P+0iEIa7ZekRRTK6k4u/573vCdGjhy57nz6Fvnss89us5rSt8I333xz9u/jjz8+fvKTn8R2220X22+/fbz5zW+O/fbbL/t2vTFpdED69rMs3aautE0DBw6M9pIWKqv7+GXpW/r0rWsaKZq+kW1KeaRpY9KoyLq1p+ckfQPccF9tyvPXUPrWe/LkybFs2bJW1dxSDz74YL3zdUfUppEwaTRNGo2RPPnkk7FixYro16/fevdTd6RAY6+B9pLqSXWVpdHOqe6621O3trS9aaGGtD/SvkrPdXLttddmozvSN/blUSfl37Wy8rfzyQsvvFDvcRpKo0XS4ijt+Xqt+zci/X1Io1HK0siRiRMnrhv5XFdbbAcAnVvd0ap1pZGG6X3zV7/61boRj23Vi6T319QHpyOSNiTdd1NHrm2qNDJ24cKF685/4hOfqHdkSRq5m0aX1u0b0ijShlJ/VO7n0/ttXWl0Yvk+06jW5vTFqZesez9p1GTqU9JIw/KI3PTcbUjd9/d77713g0fMpPf33XbbLZr7nNU9wvA73/lOdmpM6qNSb5GOQGvtZ6DWbEfqt1Nvl17XqQ9Po1DTUWCpJ00jwNMo2LRgXd3nO42OLfdm+++/f3YkVBpxmq67zTbbRGuk11O550//T6O2y6OCU3+XjmRKj5UWDEv94A033JD9LI3MTqOA0+jP8j5P25WOditaD90R0vNY1vD3K/3ulaXfr7QA4cY+bzT3tQedRfOOLwEyv/zlL7MGoeEphSZNafjmU/fwl7oN8qJFi+qtClpXw/Ob6itf+UoWUqbGKB2WlZqkdPj/V7/61ezwpNQ4vP766y3enmRDTX9baKwxTH784x9ngeOGQrAkbe+m7qtNef7qSs1aus2GQttkY9vUEnU/vKTDkRoeWlW3yU2v7bqvy6aeq9a8BtLhmHWlw9qaIzVbdT981q03SduTtqvu9cvSB6LyYZWpGU0hZzo8rdwwp0a+7lQTdZ+rjSkfrteer9cN/Y1o6rK22g4AOrcUrpWlQ7fTYeXJGWecEVddddVG37s31D81Jh16nUKfjYW2zb3v1vYNDd8DG/YNDc83FerUnQKqV69eTf6s4aHlTT2v5emaylJPWd4nSVP9V0e8v7fkfjd0383tq1uzHWmQRBr8UO750jRl//3f/x3f/va3swEwaTqq1J+XpZA09ejlsDlNh5a+PE/TUqQANwXnrfkM03A/Nnw91d2PaVqQsjSAIAWpdafwaO50c3n00M1V9/5b8jejNb9fG9pfzX3tQWdhxC20s/K3fWVNjcRMc2umRiIpfyNYVp4rq62kN700suKCCy7IvrlO36yn04033pi9Sad5TtNowDTXUWu3p700NYdTmm+s7ht82pY0YiK9+ac5QsujSDekudu2Kc9fUzWnRil9C59GBKQPVGlu0y984QvR1srzziUpME7ze9V9TufMmVNv+9PrcmPPVWteAykgTSNQynN0pcY1zUHX2OjeutI8ZuURFg3rTdL21A3C0/XL0miHt7/97dlcfen2aU7pP/3pT42Otm34XKURGSn4bUoKfdv79Vp3XzT8G7GhvxNtsR0AdF7piI3y/Lbl+VjLX2TWfT9Kc9GmQCmNWky9ThrpVnf+zZb4n//5nyyYKks906c+9als3tY0X2l6P2qJNCqy7vt/6sNSELaxeW7rvgc21jc0PF+3b9hQj1hXa+YBbfg+nuYWLX8OSJrqvxpuW/l+DjrooEaP8Ck78MADm11bw+csjUreUH/QMCRraV/d2u1IAylS8JpGYKb5WNP8pWk9kvT/NGI41Z3m8U/SiO70pf1LL70UDz/8cHadNBr2j3/8YzaPc+pD05FMDXvBlu7Hhq+nuvuxPN9zetzFixdnI4XLc+Km/i+N1i1qD92Uhr9/aQR/uY70HDdXW/9+5f15Fdqa4BYKIh3Wc8cdd2T/Tv9PAWD5DTON9G1LKWRMDUw63Lpuc5SaslNPPTX7d0sW2CqCus1uei7TN/FJmtg+fXgo4vNXt+a0MEha8KL8TXBarK45zUjdD0XN0bBxTx980sIY5War7jf/aRGKjTWBrZW2IR2O9YMf/CA7n0Yop1HMabREw0P506idW265JWvOUz2prrQIXJI+UKZpRMq3Sduzoe1NoxlScJtcccUV60YCpXrS4ze8bWruy/WlBb4ajnJIzX56faXD4Nr79ZqulxbjKDfbabGF9EE2SaNKGpsmob23A4BiSz1Lw4V4yr1Kw/ej9J5SDlTTyMa6IxZb2ovUvd8kBWLlxbbq9hrNNXbs2CxILgfNaSqrL3/5y3HRRRetd2h9CozS+14KwlIInYLB8qjO1Gd85jOfWXebhl+ytyTg3BRp8az0vl0OPdN2lY8ASvbee++N3keqNR29Vf7yNvVJgwYNqned1NulXqnudm1s36XnLI3+Le/DdB9f+9rXGg0t0zQHmzrNRWu2I/WGaR+mUaPpS/l0Kr8uytMNpB4vbUPalvTFRQpN05QIdadFSH18efqz1Lu3NLhNr6fydAkpEK27oHQKY9Nj1nXyySdn25ekRd3K+zwtVDxixIhC99CNafgFQwrR075In2XOO++8Zm0PsHGCW2iB9C1u3TmfylIjmlbC3BTp9uXgNh1WkwKU1KCmpiO9Ebel//zP/8xWFX3HO96RzfmTmp7U0NZ9w27ON/1FkprM8je7qUFJTXk6dDwFoM09hKijn79Uc3l+1TTnVArUdtxxx2wUaHn+psbUDd3SB6vUZKZ5vVLTmEbpbmgO08MPPzx73PJqyKmBTPOCpftMTXN5ZEJyyimnRHv61re+lW1/OeROI47T3FVpddt0iFsaSZB+lkYjpLnMyo1uml6iHLKmDz1p5EHdFXHL0rzDaXvrStdL+yb9jtXd1nS9uvNKl5+bFO6mMDXt3zQiNs0Hlj6gpBEJacRQ+lCb7iutstyckQmb8npN25wa7FRPkg5BTaOXkrSyblPaczsAKGavumTJkizESufTl3NlqU9417veVe/9KB0pVJ53M42gSwFP6nM2dHh93V4kzR2ajjRKwVM6pSM7Gs5/n95n0yHsqd/Z0JfTG+u/Un9Ufv9Oq8+nnimFXuVe7KGHHspC0TRfZwpu0/akfiYdQp+kUZdpVGd6DtL7bd0QOQXXKdjqCCmwS71Nem9P/U7d9/H0uSK9T29M6ofSiNEUGKaRpGkAQZpvNz0XaURnGtn517/+NRtJWXdNg+b0kSnc/+Y3v5ldJz1H06ZNywYYpJGeKVxNo7jTc52ey9RbbYrWbMd9992X7d/0+Kl3TkcvpVHL5Tlky8FpeQBCmgoh3Vfax2n7U5g/derUbMqETfnsk+azTZ8H3vKWt8T999+/bgRt8rGPfWy9ARCp5tNOOy0boFPu55KWBsZ59NCNSV8w1B3Fm/Zb+t1KnzPqzqcLbKK8V0eDzrRSb1OnuiuyNlzVMt1Hc1fh/chHPtLo/b/nPe+pd/7qq69u1mqeTT1WWn11Q9vTrVu3eiuJptuWf5buc0PPUd0Vcpurbp2NrW7bnJWL//a3v5V69Oix3rYMGDAgW3W0qftvarXTDW13Wz1/L774YmngwIHr3T5tx8c//vEm92taDbdfv36NPva8efM2umLws88+W9pss802uA1f+tKX6t2mtavGbsz8+fNL733vezf6O9bwdddwReqGp3HjxpWefvrpRh/zc5/73HrXv/nmmxu9btqP/fv332h9dV/37fl6TSsHN/U3aMcdd1x3/oQTTtjk7QCg6/Sq6T3nnHPOKdXU1NS7/bXXXtvo9ceOHVt65zvf2eT78B//+MdGb7fzzjuvW8l91113bfQ6dfuixnrlDZk+fXrpgAMO2Oj21n3/Xbt2benDH/7wBq+f3kNfe+21ZvU2DXv9uj/bUF9ctz9405veVBo2bFijPWTaJ3Wl9/jyz1N/V9ell17aaD/R8NTSPjK9To455pgW9Web8hmopdvR1Ou27in1imXbb7/9Bq+b9kV6bW1Mw2085JBDGr2/iRMnlubMmdPofXzta19b73ctvUZbqiN76A29Bj/xiU80ej+HHXZYk78nG/pcUfczTMOfNfWZalNee9AZWJwMCiSNbvj+97+ffWOaDoNJh0+lEQJpQv26NnU0bBqll77tTd8Op1F3aU7V9K10+nf6hj99q91Rq4i2lfSNexqxnA7rSRPQp9EKhx12WDYHbcPDlDZVWz1/6VCtNGIgfTOdvpFP89ym+a/St+OTJk1q8nZpZGY6rD19g97UHKobkkYmpEPGzjrrrOxwsvS4af6odBhi+tY9PY9pwYaOkA5fS9uSnrP0vKba0uFx6fC3NBoi7de02nPDw7fSXHlppMEHP/jBbJRF+n1J25FGk6bfmfQtf1Pz5zUc1ZBGdKRRQI1J+zGNREojT9LrKD1GeQGRAw44IP7f//t/2WGCTc3v1tav189+9rPZqIo0bUK6XRrZlEZOpBFEdRdbaPg3or22A4DiSX/f08jIdFRQGg2YjtZIo+vSKL2Gc1KmaRTSiMo00jS9l6b3hTQ6MY1srbsoUEPve9/7shGv6X274WJCSbqve+65Jxt9m+4zvWelkZRpMdfUf7TWFltskb1fpd4hjV5MvVTqhVIfkxaKSv3TpZdemvUOdZ+PtI3p0PD0Xpuul66f3nvTEW5pXvl09NGGtretpRHJaTqHD33oQ9mRLmmUa+oJ0gjQhlNbbEg6ZD6NrE4jKtMoydRPpm1LvU3qKVNPVHd+4+b2kel1knqvNJo69VppFGfaz2k/pn2QRjmnaSrSnMhtoaXbkfqo7373u9mo0PS5Kb3e0/XT0VPpNZ8W3Eu9Ylk6bD/1UGmEaNr+9PpMj5EWkU2P/eijj2bb1VKTJ0/ORqruueee6xb9S3Prpn6u4cJlZWlUc93fwzSKuOF0H0XtoRvzs5/9LJtKI41kTq+RtP/S46ZR1EDbqErpbRvdF7CJ0vxNjR3mnhrjdLhz2WuvvdahzSVQ7L8Rab6yFOamwwSTNMdaOkQPACiGQw45JAvZkhTupXCRypOmSEjhcZq6IUlTdjScWgSgLnPcQoGkkXPV1dXZCMz0rW+ayynN01V33qvyt6NA5UkjldLI/DRCJ40wSaMq0kjaiy++eF1om0bFbOp8cwAAtJ00ij2tJ5BGwJZD2zRCXGgLbIzgFgokLR6RFipKp8aklefTwhFAZUoHyaTD+dKpMemQwnRo2oYWqAMAoGOlKTDqLoybphWoO6UHQFMEt1Ag6bCptDJnWgU0rQicVpxN8xelOYeOOuqobERumr8JqNzDLNN8gWnutDlz5sSyZcuy+czSHG1pnrfPfe5z2bxmAAAUT5qPN82Je+6552b/B9gYc9wCAAAAABRM/WVFAQAAAADInWOuAQCgHdXW1sasWbOyQ2TTlEgAAHQNpVIpli5dmi0i361b24+PFdwCAEA7SqHthAkT8i4DAIB2MnPmzNhss83a/H4FtwAA0I7SSNskrSg+ZMiQvMuhg0dbz5s3L0aOHNkuo3AoLvu+Mtnvlcu+r1yLFi2KLbbYYl2/19YEtwAA0I7K0yMMGjQoO1FZH+RXrVqV7Xcf5CuLfV+Z7PfKZd9X9r5P2ms6LK8mAAAAAICCEdwCAAAAABSM4BYAAAAAoGAEtwAAAAAABSO4BQAAAAAoGMEtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgBLcAAAAAAAUjuAUAAAAAKBjBLQAAAABAwQhuAQAAAAAKRnALAAAAAFAwglsAAAAAgILpkXcBAABQCX590l3Rt0f/vMugI1WVotfYUqx+vSqiVJV3NXQk+74y2e+Vy77v1E685j1RVEbcAlS4Qw45JL7yla9s8v0cf/zxceSRR7ZJTQAAAFDpBLcAXVAKUauqquKzn/3sej/7whe+kP0sXSe54YYb4pxzztnkx/yv//qvuOqqqzb5fgAAAADBLUCXNWHChLjuuuti5cqV6y5btWpVXHPNNbH55puvu2zYsGExcODATX68wYMHx5AhQzb5fgAAAADBLUCXtddee2XhbRpRW5b+nULbPffcs8mpEi677LLYdttto0+fPjF69Oj40Ic+tO5nv//972PXXXeNvn37xvDhw2PSpEmxfPnyRqdKSPf7pS99Kb7+9a9n4fCYMWPirLPOqlfj888/HwcddFD2WDvttFPcdddd2Wjgm266qd2eFwAAAOgMBLcAXdgnP/nJ+OUvf7nu/C9+8Ys44YQTmrz+I488koWt3/nOd2LKlClx++23x1ve8pbsZ6+//np89KMfze7zueeei3vvvTc+8IEPRKlUavL+rr766ujfv3889NBDcf7552f3e+edd2Y/q6mpyYLefv36ZT//6U9/Gt/85jfbdPsBAACgs+qRdwEAtJ9PfOITccYZZ8Qrr7ySnX/ggQey6RNS6NqYGTNmZEHre9/73mz6hC222GLd6NwU3K5duzYLa9PlSRp9uyG77bZbTJ48Oft3GsV7ySWXxN133x3vfOc7swB36tSpWS1pNG7y3e9+N/sZAAAAVDrBLUAXNnLkyDj88MOzRcPSyNj07xEjRjR5/RSaplB2q622ine/+93Z6f3vf382Knb33XePd7zjHVlYe+ihh8a73vWubBqFoUOHbjC4rWvs2LExd+7c7N9pRG+ayqEc2ib77bdfm2w3AAAAdHamSgDo4tLUBim4TdMWpH9vSBpl+9hjj8W1116bhaxnnnlmFtguWrQounfvno2S/dOf/pTNR3vxxRfH9ttvHy+//HKT99ezZ89659P8tbW1tW22bQAAANBVCW4Burg0anb16tWxZs2abKTsxvTo0SNbdCzNSfvkk0/G9OnT45577lkXvL75zW+Os88+Ox5//PHo1atX3Hjjja2qK4W+M2fOjDlz5qy77J///Ger7gsAAAC6GlMlAHRxaaRsWkys/O8NueWWW2LatGnZgmRpCoTbbrstGyGbQta0gFianzZNkTBq1Kjs/Lx582LHHXdsVV1pWoatt946jjvuuCwkXrp0aXzrW99aFxADAABAJRPcAlSAQYMGNet6Q4YMiRtuuCHOOuusWLVqVbagWJo2Yeedd87C3/vuuy8uuuiiWLJkSTYX7gUXXBDvec97WlVTCpFvuummOPHEE2PffffN5tX94Q9/GEcccUT06dOnVfcJAAAAXUVVKa1WAwAF8MADD8RBBx0UL730UjYaF6ArSF92DR48OH784d9F3x798y6HjlRVil5jS7H69aqIkqNJKop9X5ns98pl33dqJ17TusFISVoPJh2tunjx4mYPmGoJI24ByE2aH3fAgAHZyN4U1n75y1/O5tAV2gIAAFDpBLcA5CbNa3vaaafFjBkzYsSIEdmiaGn6BQAAAKh0glsAcnPsscdmJwAAAKC+bg3OAwAAAACQM8EtAAAAAEDBCG4BAAAAAApGcAsAAAAAUDCCWwAAAACAghHcAgAAAAAUjOAWAAAAAKBgeuRdAEBXsrp2bcxfsyTmr136v/9fszTeWLssVtRUx4ra6lj+7/+vrFm97vyq2tVRU6qNUkTURm2c+nzv2Oe2f0Z06xZV3btn/49u3f/33z16RLf+A6Kq/8DoNmBAdBs4KKoGpH8PjG4DB0e3IUOj29Bh0W3o8OjWr3/eTwcAAADQSoJbgBZYWbs6Zq6aHzOq58Ur1fNjxqp58frqN2JeFtIuiSU1Kzf5MWpW9o/aBfM2+X6qevf5d5A7PLqPHBXdx02IHuM2i+7jJ0SP8ZtnYS8AAABQTIJbgEYsr1kVz694LTtNXTU7Xlk1L2ZUz88C2lI2Nrb4StWrombO69lpzfPr/7xq4OAo7bZbXP6R3WLrvmNim75jY+s+o2N0ryF5lAsAAADUIbgFKl6ayuC5LKR9dd3/Z1Yv6DQBbWuVli6OpSsWx00LHq53+bAeA2Ln/pvHLv0mxK7p//03j8E9TLsAAAAAHUlwC1ScmdXz49Gl0+LRZVPjsaXT4tXVC6JSzRuVpktYXe+yhWuXxd8WP5udyib0Hh479/vfEHevAVvFjv3GR7cq61sCAABAexHcAl3etJVzspD20aVT47FlL8ecNYvyLqkwXh7Re73gtjFpBHI63f7G49n5Qd37xj4Dt4n9B24b+w/aNrbsM7oDqgUAAIDKIbgFupwVNdXx0NIX/z1q9LmYu2Zx3iUV1lNDWzcdRFqE7Z5FT2WnZGTPQbFfCnEHbhsHDd4xhve08BkAAABsCsEt0CW8Wr0gC2rvW/xsPLJ0aqwurc27pE7hkUGr2uR+0qJtty58NDt1i6rYtf8W8fYhu8Tbh+wam/cZ2SaPAQAAAJVEcAt06ikQ/vTGY3HXG0/GtFVz8i6n8xk2POb1avuAuzZK8a/l07PTf752S2zVZ3QW4L5tyC6xc78JUVVV1eaPCQAAAF2N4BboVGZVL8zmWb194eMxZeWsvMvp1FaOG9Uhj5NC9Wmz58TPZt8Vo3sOiXcP2yOOGL5PbNt3XIc8PgAAAHRGglug8BasWRp/fuOJLKz91/JXohStm5eV+uaPHhIRHTulRFoY7uo592an7fuOi/cO3ycOH7a3OXEBAACgAcEtUEg1pdpsYbEb5v8j7l/8XNREbd4ldTmvjOgTEctye/w0YnrKqzfHRa/eEgcM2i4LcdN0Cn269cqtJgAAACgKwS1QKK+vfiP+MO8fcdOCh7IFr2g/zw0rxlyzKZS/f8nz2Wlg9z7xvuH7xtEjD4otLGoGAABABRPcArkrlUrx9yXPx/Xz/m50bQf65+DqKJqlNaviN3P/FtfMvT8bhfuRkQfFwYN3jG5V3fIuDQAAADqU4BbITXXtmvjjgn/Gr+f8NWZUz8u7nMoycFC80md1FFWax/jvS6Zkpwm9h8eHR7453j98vxjUo1/epQEAAECHENwCHW7x2uVx3bwH4rq598fCtfnNsVrJqseNic5iZvWCuPDVm+OyWbfHfwzfN44f/bYY13tY3mUBAABAuxLcAh1mVvXC+NWce+OmBQ/HytrijvasBG+MHpKNa+1MVtWujuvnPRB/mPdgHDZ87/jUmHfExD6j8i4LAAAA2oXgFmh3U1fOjp/Nviv+vPCJWGv+2kJ4dWSacmB5dEbpNXTzgn/GLQseiXcM2S0+NfYdsWO/zfIuCwAAANqU4BZoN69WL4jLZ90Rty18NGo72ejOru754Z1/sa/0mrpz0b+y00GDdoiTxr4rdh8wMe+yAAAAoE0IboE2N2f1ovjJ63+OP85/2AjbgnpsyJroSu5f8nx2OmTwzvGl8YfH1n07zxy+AAAA0BjBLdBmFqxZGr+YfXf8dt7fY3Vpbd7l0JQ+feLZvquiK7p38TPxt8XPxRHD94nPj3t3jO6V5vIFAACAzkdwC2yytNDYVbPviavn3GvRsU5g7bixUaqKLqsmarMF8G5f+Hh8ZNRB8akxk2JQj755lwUAAAAtIrgFNsmfFj4eF736PzF7zaK8S6GZFo0ZHpVgVWlNXDXnL3HD/H/EiWMmxcdGHRw9u3nbAwAAoHPwCRZoledXvBY/mHljPLZsWt6l0EKzRvWPiBVRKZbUrIwLX/uf+MP8f8TXJxwZBw3eMe+SAAAAYKMEt0CLvLF2WVz82m1x4/yHojZKeZdDK7w4rHtUoleq58UXXroy3jp45yzA3ax3ZYw8BgAAoHMS3ALNUluqzRYdu2TWn2Jpzcq8y2ETPDGksheO++viZ+KhJS/EiWMnxfGj32b6BAAAAArJp1Vgo15eNSfOmn59PLF8et6lsKl69IzHB6yKSpfmv01fQty68NH4xuYfjP0Gbpt3SQAAAFBPt/pnAf7PmlJN/PT1O+OoZy8Q2nYRNWPHxJpuprgoe3nV3Pj0C5fHd175bSyvEWgDAABQHEbcAo16ZvnMOOuV6+KFla/nXQptaNmYEXmXUEhp4bIHl0yJs7Y4OvYftF3e5QAAAIDgFqhvVe3quGzW7fHfc+6LmqjNuxza2Ouj+kdEdd5lFNKs1W/EZ178SXxoxAFx6mZHRL/uvfMuCQAAgApmqgRgnSkrXouPPvefcfWce4W2XdTUEb3yLqHQSlGK383/e3zw2R/Gw0tfzLscAAAAKpjgFohSqRS/nvPX+MTz/xXTVs3Juxza0VNDBPLNMWv1wjjphSvi+zNuiOraNXmXAwAAQAUyVQJUuAVrlsa3pl8Tf18yJe9SaG/dusU/B67Mu4pONfr22nn3x2PLpsX5Wx0bE/uMyrskAAAAKogRt1DB/rb42fjQsz8U2laI0siRsayHEbctNWXlrGwKkVsWPJJ3KQAAAFQQI26hAq2pXRsXvvY/cc3cv+VdCh1o+VgjRltrRW11fHP6NfHQ0hfjG5t/MPp2M1cwAAAA7cuIW6gwc1YvihNeuERoW4Hmjh6Udwmd3s0L/hkfe+4/48WVr+ddCgAAAF2c4BYqyKNLp2aHfD+1fEbepZCD6SN6511Cl5AW8PvEcxdlIS4AAAC0F8EtVIjfzLkvTnrh8liwdmnepZCTp4ea37atrCqtiW9PvzZ+OPOPUVPyvAIAAND2zHELXdzK2tXxnVd+G7ctfCzvUsjZw4Oq8y6hy/nvuX+Nqatmx/lbHhuDevTNuxwAAAC6ECNuoQt7rXphHPf8j4W2RAwbFvN6rc27ii7pwSVT4uPPXxQvr5qTdykAAAB0IYJb6KKeWv5KfOL5i2LKyll5l0IBrBw7Ou8SurQZ1fPimOf/K+5f/FzepQAAANBFCG6hC7p30dNx4guXx8K1y/IuhYKYP3pw3iV0eUtrVsXJL/0srp5zb96lAAAA0AUIbqGLuWbu3+KUqb+MVbWr8y6FApkx0vyrHaE2SnHhqzfH+TNvjFKplHc5AAAAdGIWJ4MuIoVEP3r15myxJGjouaF5V1BZfjP3b7FgzbI4d+JHo2c3b7UAAAC0nE+T0AVU166Jb7z8m7hr0ZN5l0JBPTJkTd4lVJzb33g8Fq1dHhdufXz0794n73IAAADoZEyVAJ3c0pqV8ZkXrxDa0rQBA+PlPtV5V1GR/rH0hfjUC5fFgjVL8y4FAACATkZwC53YG2uXxadfuDweX/Zy3qVQYKvHj8m7hIr23IpX49jnfxwzVs3LuxQAAAA6EcEtdFJzVy+OT065NAuFYEPeGG2C27y9unpBHD/lkpi6cnbepQAAANBJCG6hE3p99RvxyRcuiWmr5uRdCp3AqyP75V0CEbFg7dI48YXL4iXhLQAAAM0guIVO5tXqBdlI25nVC/IuhU5iyjB/6oti4dplWXj74srX8y4FAACAgvNpHjqRNEfmp6ZcGrNWL8y7FDqRx4asybsE1pub+rJ4YcWsvEsBAACgwAS30ImmR0gLkc1esyjvUuhM+vSJp/utyrsKGnhj7fI46cXLY8qK1/IuBQAAgIIS3EInMH/NkjhJaEsrrB07NkpVeVdBk+HtC1fE88JbAAAAGiG4hYJbvHZ5fOaFK2JG9fy8S6ETWjxmWN4lsAGLapbHZ1/8Sbyyal7epQAAAFAwglsosOU1q+JzL/40XlplFXpaZ9bI/nmXQDPmvP3si1fE3NWL8y4FAACAAhHcQkGtql0dX3rp5/HMipl5l0In9uKIHnmXQDPMWv1GfO7Fn8SStSvyLgUAAICCENxCAa0p1cRXp14VjyybmncpdHL/GlKTdwk0UxpZf/JLP8u+tAEAAADBLRTQd175bdy/5Pm8y6Cz69EjHuu/Mu8qaIEnlk+Pr029OtaWBO4AAACVTnALBfPT1++Mmxf8M+8y6AJqR4+O1d1LeZdBC/1tyXMxefr1USrZdwAAAJVMcAsF8qeFj8dls27Puwy6iKXjRuZdAq10y8JH4rLX/S0AAACoZIJbKIjHl70cZ06/NkphlB1tY/aogXmXwCaOvk9f5gAAAFCZBLdQADOr58cpU38Rq0tr8y6FLuTl4T3zLoFNNHn6tfH08hl5lwEAAEAOBLeQs8Vrl8cXXrwy3li7PO9S6GKeHFKbdwlsourS2vjK1F/E3NWL8y4FAACADia4hRzVlGrj69N+Ha9Uz8u7FLqabt3in4NW5l0FbWDemiVZeLuqdnXepQAAANCBenTkgwH1XTrrT/GPpS/kXQYdbO4vH47Ff3kpqqcvjKrePaL/buNizMkHRZ+Jw5q8zfUz58RX/vVivct6d6uK6Ye9ed35y6e+GpdOfS379xd23yGW9Pi/Ebcrnn49Xvv+PbHNVR+Nqh6+s+tsnlkxM749/bo4f8tjoqqqKu9yAAAA6ACCW8jJXxY9Hb+YfU/eZZCDZY+9GsM/vHv022l0lGpKMfvSB+LlL94Q2//uuOjWt+l5aQf26B73H7L3uvN187tnlyyP86fMiF/vt1O2vN0xjzwbm7+0R/TdZkSU1tbGq9+7Ozb75iShbSf25zeeiO37josTx07KuxQAAAA6gOAWcjBj1bz49vRropRFbFSarS7+QL3zE856Vzz7zp/EiufmxIC9NmvydimnHdWnV6M/e2nZithpUP84aMSQ7PzW40fFsukLs+B23q8fif57jY9+O49p4y2ho1026/bYtf8Wsf+gbfMuBQAAgHZm6BV0sJW1q+PUaVfF0ppVeZdCQdQs+9+5S3sM6rPB6y2vqYl97v5n7H3Xw3H8P5+NKUv/b0G7HQb2j2nLV8arK1fFzBWrYvq8N6LP1sOj+tVFsfB/nokxn/u/KRXovGqiNk5/+dcWKwMAAKgAglvoYN955bfx4srX8y6DgijVlmLWBfdGv93HRZ9tRjR5va0H9I0Ld982rtpnx7h4z+2jtlSKIx54MmatrM5+vt3AfnH6DlvER/7xTHz0oWfiLZ98e/TZcni89r27YuyXDo6lD06PKUf9Kl742H9nUzXQeS1cuyy+/vKvYm2pJu9SAAAAaEeCW+hA1899IG5b+FjeZVAgr/3gnlg1dUFs/r3DNni9fYYOiqM2Gx27DB4QBw4fHD/fZ8cY3qtn/HrG7HXXOW6LsXH/2/bOTjUf2yUW3vJMdOvXK/rtOjZePfeumPijI2LcqW+NGd+4LWpXr+2AraO9PL7s5bh81h15lwEAAEA7EtxCB5m6cnZc8Oof8y6DgoW2S++fFltf8aHoNXpgi27bs1u32GVw/5i+fOX6PxwyNF5bsSTmXvmPGPf/3hYrnp4dvTcfEr03HxoD9pmQLVZWPWNR220IufjF7LvjwSVT8i4DAACAdiK4hQ6wpnZtnPHyf0d1yShHIkqlUhbaLr73pdjq8g9Fr/GDW3wfNaVSPLdkRYzqvf5iZSvHj8mmXxjx0b3+NxCuLWVh7f/duPZ/T3RqtVGKb758TcxfsyTvUgAAAGgHglvoABfPui2mrJyVdxkUxKwf3BNv/On52Pzcw7KpDNbMX56dalf9X7B/8uNT4rvPTV93/sIXZsS9896IV5aviicXL4svPj4lXltZHR/bfMx693/bojeyEbXDj9ojO993p9FR/crCWPLAy7HghicjulVF7y2GddDW0p4WrF0aZ7/y27zLAAAAoB30aI87Bf7Pw0tfjF/P+WveZVAgC37/ZPb/aZ/5Xb3LN5v8rhh2xM7Zv1Mo262qat3PFq1ZG1978qWYV706BvfsEbsNHhA3v3m32H5gv3r3sbKmJr73x3tiswveE1Xd/vf2adRtmjLh1e/8Oap6do8JZx8a3fr4899V3Lf42bhx/kPx/hH7510KAAAAbcgnd2hHS9aujG+9fG12SDOU7fbIKRu9zg0H7lbv/Hd23io7bUzf7t3jCz/7Qlw/Zlm9y4cfuWt2omv64cybYv+B28a43kZSAwAAdBWmSoB2dO6M38WcNRaBomM9Mnh13iXQwZbXVseZr1yXzZ8MAABA1yC4hXbyp4WPxx1vPJF3GVSa/gNial/BbSX659KX4pp5f8u7DAAAANqI4BbaweK1y+P8mTfmXQYVaPW49Rcro3L8+NVbY/qquXmXAQAAQBsQ3EI7uODV/4mFa+vPMQod4Y0xQ/MugRytKq2Jb0+/NmpLtXmXAgAAwCYS3EIbe2jJi/HHBQ/nXQYV6tVR/fMugZw9ufyV+N28B/MuAwAAgE0kuIU2VF27JluQDPLywlB/1om4eNZtsWDN0rzLAAAAYBP4hA9t6IrX74gZ1fPzLoMK9viQNXmXQAEsrVkZF7x6c95lAAAAsAkEt9BGXlgxK341+968y6CS9e4TT/dblXcVFMStCx+Nh5e+mHcZAAAAtJLgFtrIuTN+H2vDgkDkZ+3YMVHjrzp1fG/GH2JN7dq8ywAAAKAVfMSHNnDbwkfjX8un510GFW7J2OF5l0DBvLxqblw9x5EAAAAAnZHgFjbRqtrV8V+v3pp3GRCzRvbPuwQK6MrX74zXqhfmXQYAAAAtJLiFTXTV7L/E7DWL8i4D4qXhPfIugQJaVVoTF7/myyUAAIDORnALm2DO6kVx1Zy/5F0GZP41tCbvEiio2994Ip5dPjPvMgAAAGgBwS1sgh+/dlusrF2ddxkQ0b17PDpgZd5VUFClKMUFr96cdxkAAAC0gOAWWunp5TPi1oWP5l0GZGrHjInqbqW8y6DAHlk2Nf666Jm8ywAAAKCZBLfQSmn0WhrFBkWwdMyIvEugE7jotVuiplSbdxkAAAA0g+AWWuHvS6bEY8um5V0GrDNn9MC8S6ATmLZqTtw0/6G8ywAAAKAZBLfQCpfN+lPeJUA904b3yrsEOonLZt0eK2qq8y4DAACAjRDcQgvdt/jZeGr5jLzLgHqeHmraDppn/tql8bt5f8+7DAAAADZCcAstdPms2/MuAeqrqoqHBq3Muwo6kavn3BuralfnXQYAAAAbILiFFvjLoqfj2RWv5l0G1FMaOTKW9KjJuww6kQVrl8Yf5v8j7zIAAADYAMEtNFOpVDLalkJaMXZU3iXQCV01+y+xunZt3mUAAADQBMEtNNPdi56MKStn5V0GrGfeqEF5l0AnNHfN4rhpwcN5lwEAAEATBLfQTL+YfU/eJUCjpo/onXcJdFK/nH13rCmZZgMAAKCIBLfQDA8vfTGeWTEz7zKgUc8Oy7sCOqtZq9+IWxY8kncZAAAANEJwC82cCxKK6uHBq/IugU7sl7PvyebwBgAAoFgEt7ARL658PR5Y8nzeZUDjhgyN13tZYIrWe6V6nr9xAAAABSS4hY347zl/zbsEaNKqcaPzLoEu4Ddz78u7BAAAABoQ3MIGLFyzLP608LG8y4AmLRg1JO8S6AIeXPJCTF81N+8yAAAAqENwCxvw23kPRHXJYegU18yRffMugS6gFKW4Zu7f8i4DAACAOgS30IS1pZr4/fwH8y4DNuj5YVV5l0AX8T8LHollNRa6AwAAKArBLTThvsXPxrw1S/IuAzbokSGr8y6BLmJFbXXcNP+hvMsAAADg3wS30IQbBBgUXb/+8WLf6ryroAu5bt79USqV8i4DAAAAwS00bs7qRfH3xc/nXQZs0JrxY/MugS5mZvWCeHjpi3mXAQAAgOAWGnfj/IeiJmrzLgM26I3RQ/MugS7o5gWP5F0CAAAAgltYX22pNv644OG8y4CNem1kv7xLoAu6a9GTsdwiZQAAALkT3EIDDy55IWatfiPvMmCjpgzvnncJdEGralfHn9/4V95lAAAAVDzBLTQyTQJ0Bo8PWZt3CXRRNy/4Z94lAAAAVDzBLdSxtGZl3Lv46bzLgI3r1Sue6rcy7yrooh5f9nLMrJ6fdxkAAAAVTXALdfxl0dOxplSTdxmwUTVjx0aNv+C0k1KUjLoFAADImY/9UMcdCx/PuwRoliVjhuddAl3cLQsezbsEAACAiia4hX9bvHZ5/GPpi3mXAc0ya/SAvEugi5u1emE8s3xm3mUAAABULMEt/Nvdi56KtaZJoJOYOqxn3iVQAe5e9GTeJQAAAFQswS382x0Ln8i7BGi2fw31JQPt7+43nsq7BAAAgIoluIWIWLhmWfxz6Ut5lwHN0717PDpgRd5VUAGmV8+Nl1bOzrsMAACAiiS4hX8fDlwTtXmXAc1SO3p0rOxeyrsMKoTpEgAAAPIhuIWI+OviZ/MuAZpt2diReZdABTFdAgAAQD4Et1S81bVrTZNApzJ31MC8S6CCTFn5WrxavSDvMgAAACqO4JaK98iyqbGqdnXeZUCzTRveK+8SqDB/WfR03iUAAABUHMEtFe9+0yTQyTw1zHzMdKy/L3k+7xIAAAAqjuCWinf/YoEEnUhVVTw8aFXeVVBhHls6Lapr1+RdBgAAQEUR3FLR0ryNr1TPy7sMaLbS8BGxqEdN3mVQYVaV1sSjy6blXQYAAEBFEdxS0f5mmgQ6mZVjR+VdAhXqwSVT8i4BAACgoghuqWh/F0TQycwbMzjvEqhQDy15Me8SAAAAKorglopVW6qNxx36SyfzyojeeZdAhXph5ax4Y+2yvMsAAACoGIJbKtaLK1+PpTUWeaJzeWZY3hVQqUpRin8ufSnvMgAAACqG4JaK9fiyl/MuAVrs4UHVeZdABXtk6dS8SwAAAKgYglsqluCWTmfwkJjVe03eVVDBnlz+St4lAAAAVAzBLRVLcEtnUz1udN4lUOFeXDErVtWuzrsMAACAiiC4pSLNql4Yc9YsyrsMaJEFo4fmXQIVbm3UxrPLX827DAAAgIoguKUiPbZsWt4lQIvNHNEn7xIgnjJdAgAAQIcQ3FKR/rV8et4lQIs9P9yfbPJnnlsAAICOIQWgIj23wqG+dD6PDja3KPkT3AIAAHQMwS0Vp7ZUGy+unJ13GdAy/frFlH7VeVcBMXfN4piz2hzhAAAA7U1wS8WZUT3fquh0OmvGjs27BFjn6eUz8i4BAACgyxPcUnGmrJiVdwnQYovGDsu7BFhn6qo5eZcAAADQ5QluqThTVr6WdwnQYq+N7Jd3CbDOVNPNAAAAtDvBLRVnygrBLZ3PC8N65F0CrDN1leAWAACgvQluqThTVpoqgc7niSHmZaY4pq+aG2tLNXmXAQAA0KUJbqkoi9Yuj3lrluRdBrRMr17xRP9VeVcB66wp1cSMVfPzLgMAAKBLE9xSUV5ZNS/vEqDFasaMiRp/rSkY0yUAAAC0L1EAFWVmtRFidD5Lxo7IuwRYjwXKAAAA2pfgloryavWCvEuAFnt9ZP+8S4D1TFs1J+8SAAAAujTBLRVlpuCWTmjqiJ55lwDrmbV6Yd4lAAAAdGmCWyqKEbd0Rv8aUpN3CbCe16vfyLsEAACALk1wS0V51Ry3dDbduscjA1fmXQWsZ8HaZbG6dm3eZQAAAHRZglsqxoqa6pi/dmneZUCL1I4eFSu7l/IuA9ZTilLMXm3ULQAAQHsR3FIxXjMfI53Q8rEj8y4BmvS64BYAAKDdCG6pGHNXL867BGixOaMH5V0CNGmW4BYAAKDdCG6pGAtNk0An9PLwXnmXAE0y4hYAAKD9CG6pGAvXLMu7BGixp4fW5l0CNElwCwAA0H4Et1SMBUbc0gk9PKg67xKgSYvWLs+7BAAAgC5LcEvFMOKWTmfEiFjYc23eVUCTlqxdkXcJAAAAXZbglophxC2dzYqxo/MuATZocY3gFgAAoL0IbqkYRtzS2cwfPTjvEmCDFhlxCwAA0G4Et1QMI27pbF4Z0TvvEmCDlgpuAQAA2o3gloqxdO3KvEuAFnl2WFXeJcAGrY3aWFazKu8yAAAAuiTBLRWhplQbq0pr8i4DWuSRwQIxim/R2uV5lwAAANAlCW6pCCtqq/MuAVpm0KCY0duXDRTf0hpHMwAAALQHwS0VYUWN4JbOpXrcmLxLgGZZWbs67xIAAAC6JMEtFWFVrZGLdC4LxwzNuwRoljW1NXmXAAAA0CUJbqkI1YJbOpmZI/rmXQI0y9rS2rxLAAAA6JIEt1SEaguT0clMGe7PM53DmpIRtxTf8ccfH0ceeeS684ccckh85StfadZtW3JdAABoSz3a9N6goIy4pbN5dJDXLJ2D4JbO6IYbboiePXvmXQYAAGyQ4JaKUFOqzbsEaLZlvariuf6r8i4DmmV1rakS6HyGDRuWdwkAALBRjsWlQlTlXQA023PWJaMTMeKWTVVbWxvnnXdebLnlltG3b9/Yfffd4/e//332s3vvvTeqqqri7rvvjn322Sf69esXBx54YEyZMqXefZx77rkxatSoGDhwYJx44olx+umnxx577NHs6Q8uu+yy2HbbbaNPnz4xevTo+NCHPrRejV//+tezwHfMmDFx1llntfnzAAAADQluqQjdqgS3dB4PDzbals5jjcXJ2EQptP3Vr34VV1xxRTzzzDNxyimnxCc+8Yn461//uu463/zmN+OCCy6IRx55JHr06BGf/OQn1/3sN7/5TXz3u9+NH/zgB/Hoo4/G5ptvHpdffnmzHz/d55e+9KX4zne+kwXCt99+e7zlLW+pd52rr746+vfvHw899FCcf/752XXvvPPOJu+zuro6lixZUu8EAAAtZaoEKkI3I27pRF7vJQij86jy95VNkALO733ve3HXXXfFAQcckF221VZbxf333x8/+clP4qSTTsouS8HsW9/61uzfaTTt4YcfHqtWrcpGyF588cXxqU99Kk444YTs52eeeWb8+c9/jmXLljWrhhkzZmSh7Hvf+95sxO4WW2wRe+65Z73r7LbbbjF58uTs32lk7iWXXJKNAn7nO9/ZZBh99tlnb8IzAwAARtxSIdJhlgC0vZ5V3fMugU7spZdeihUrVmQB6IABA9ad0gjcqVOn1gtOy8aOHZv9f+7cudn/0yjZ/fbbr979Njy/IemxU1ibAuNjjjkmG8Gbaqqr7uOXayg/fmPOOOOMWLx48brTzJkzm10PAACUGXFLRejuOwqAdtGzm+CW1iuPir311ltj/Pjx9X7Wu3fvdeFtz5491/syNs072xbSKNvHHnssm083jdRNI3bTHLb//Oc/Y8iQIes9frmGDT1+qj2dAABgU0izqAhG3AK0j15VvgOm9Xbaaacs4EzTFWyzzTb1ThMmTGjWfWy//fZZyFpXw/Mbk+bNnTRpUjZ/7ZNPPhnTp0+Pe+65p0X3AQAAbc2nLSqCOW4B2kcPwS2bONr1a1/7WrYgWRrBetBBB2VTCzzwwAMxaNCgbAqDjTn55JPj05/+dOyzzz5x4IEHxvXXX5+Fr2nqg+a45ZZbYtq0admCZEOHDo3bbrstqyUFwgAAkCeftqgIPczBCNAuzHHLpjrnnHNi5MiR2YJeKUBN0xPstdde8Y1vfKNZ0yF8/OMfz26XAuC0YNlRRx0Vxx9/fDz88MPNevz0eDfccEM2PUK6fVp87Nprr42dd965DbYOAABar6pUKpU24fbQKcxYNS+OeOa8vMsA6HKu3O5zsd/AbfMuA9ZbcGzMmDHx61//OopgyZIlMXjw4Pjxh38XfXv0z7scOlJVKXqNLcXq16siSo4Aqyj2fWWy3yuXfd+pnXjNe1p920WLFmVHbaWjxtIRY23NiFsqwoDuffMuAaBLMscteVuxYkVcccUVceihh0b37t2z0bJ33XVX3HnnnXmXBgAAm8SnLSrCgO598i4BoEvq061X3iVQ4dICpGle2u9+97vZVAdpbto//OEP2WJjAADQmQluqQi9uvXI5mFcU6rJuxSALmVwj355l0CF69u3bzbCFgAAuppueRcAHcV0CQBtb3B3wS0AAEB7ENxSMQZ07513CQBdbn7bfv62AgAAtAvBLRXDiFuAtmWaBAAAgPYjuKViWKAMoG0NMk0CAABAuxHcUjFG9BiYdwkAXYoRtwAAAO1HcEvFGNVrcN4lAHQpglsAAID2I7ilYozsKbgFaEtDe/TPuwQAAIAuS3BLxRgluAVoU6N7Dsm7BAAAgC5LcEvFGNlrUN4lAHQpo3sJbgEAANqL4JaKYcQtQNsS3AIAALQfwS0VQ3AL0LbGmCoBAACg3QhuqRi9uvWwkA5AGxrXe2jeJQAAAHRZglsqyma9h+ddAkCXMKLHwOjTrVfeZQAAAHRZglsqysTeo/IuAaBLGO+LMAAAgHYluKWiTOwjuAVoC+N7D8u7BAAAgC5NcEtFmdhnZN4lAHQJW/cZk3cJAAAAXZrgloqyhRG3AG1i676CWwAAgPYkuKWibN57RHSLqrzLAOj0thHcAgAAtCvBLRWld7eeMbbX0LzLAOjU+lT1jPG9zHELAADQngS3VJwtTZcAsEm27Ds6ulVpIQAAANqTT11UnG37jsu7BIBObRsLkwEAALQ7wS0VZ6f+m+VdAkCnZmEyAACA9ie4peLs1G9C3iUAdGoWJgMAAGh/glsqzma9h8fg7v3yLgOg09q53+Z5lwAAANDlCW6pSDv2M10CQGts1mt4DOs5IO8yAAAAujzBLRVp5/6mSwBojd0GbJF3CQAAABVBcEtFMs8tQOvs2l9wCwAA0BEEt1QkI24BWmf3/hPzLgEAAKAiCG6pSGN7DY3RPYfkXQZAp9Knqmds129c3mUAAABUBMEtFWufgVvnXQJAp7JDv82iZ1X3vMsAAACoCIJbKta+A7fJuwSATmV3C5MBAAB0GMEtFUtwC9Ay+w3cNu8SAAAAKobgloq1We/h2Vy3AGxcmiJhb1PMAAAAdBjBLRVtnwFCCIDm2L3/xOjbrVfeZQAAAFQMwS0VzXQJAM1zwKDt8y4BAACgoghuqWiCW4DmedOg7fIuAQAAoKIIbqlo43oPiy37jMq7DIBCG9y9X+zUb7O8ywAAAKgoglsq3lsH75x3CQCFtt/AbaNblZYBAACgI/kURsV76xDBLcCGHGCaBAAAgA4nuKXi7dF/Ygzt0T/vMgAKqVtU+YILAAAgB4JbKl46/PfgwTvlXQZAIe3Wf2KM6Dko7zIAAAAqjuAWsnluBbcAjZk0dLe8SwAAAKhIgluIiAMH7RC9qnrkXQZA4bxjyK55lwAAAFCRBLcQEf269479Bm6TdxkAhbJjv81iXO9heZcBAABQkQS38G/vHLpH3iUAFMqkIaZJAAAAyIvgFv7tHUN3NV0CQB3mtwUAAMiP4Bb+bWD3vvHmQTvkXQZAIWzdZ0xM7DMq7zIAAAAqluAW6njPsL3yLgGgEN43fJ+8SwAAAKhogluo45AhO8fA7n3yLgMgV92jW7xXcAsAAJArwS3U0btbz5g0ZPe8ywDI1YGDto8RPQflXQYAAEBFE9xCA0cYZQZUuPeN2DfvEgAAACqe4BYa2GvAVjG+17C8ywDIxeDu/eKQwbvkXQYAAEDFE9xCA1VVVfHBkQfkXQZALt49bM/o1a1H3mUAAABUPMEtNOL9w/ePnlXd8y4DoMO9b7hpEgAAAIpAcAuNGNZzQLxzqEXKgMqyfd/xsUv/zfMuAwAAAMEtNO2okW/OuwSADvWxUQfnXQIAAAD/JriFJuw5YMvYtu/YvMsA6BBDewyIw4btlXcZAAAA/JvgFjbgqJEH5l0CQIf40Ig3WZQMAACgQAS3sAHvHbZP9O/WO+8yANpVj+hmehgAAICCEdzCBvTr3juOHLFf3mUAtKt3DN0tRvUanHcZAAAA1CG4hY04ZvQh0aOqe95lALSbj496S94lAAAA0IDgFjZibK+h8Z6he+ZdBkC72LnfhNh9wMS8ywAAAKABwS00wwlj3h5VUZV3GQBt7sSxk/IuAQAAgEYIbqEZtu47Jt46eKe8ywBoU9v2HRtvG7xL3mUAAADQCMEtNNOnxrwj7xIA2tSnx7wzqqocTQAAAFBEgltopt0GTIx9BmyddxkAbWLLPqPinUN3y7sMAAAAmiC4hRb4lLkggS7ixDGToluVNgAAAKCofGKDFjhw0Pax14Ct8i4DYJNs3ntEvGfYXnmXAQAAwAYIbqGFvjT+8LxLANgknxozKbobbQsAAFBoPrVBC+05YMs4ePBOeZcB0Cqb9x4Z7x2+d95lAAAAsBGCW2iFL407LKrCSuxA53Py+PdEj6rueZcBAADARghuoRW26zcu3jNsz7zLAGiRXfptHu8aukfeZQAAANAMgltopc+Pe7dRa0Cn8uXNzNENAADQWQhuoZUm9B4RHxixf95lADTLWwbvFPsN3DbvMgAAAGgmwS1sgs+NfXcM7N437zIANqhHdIuvbva+vMsAAACgBQS3sAmG9RyQTZkAUGQfHHlATOwzKu8yAAAAaAHBLWyio0e+ObbtOzbvMgAaNbh7v/jcuEPzLgMAAIAWEtzCJupe1S3OmPCBvMsAaNRXNntvDO0xIO8yAAAAaCHBLbSBvQduHe8eumfeZQDUs0f/ifH+4RZRBAAA6IwEt9BGTt3siOjbrVfeZQCsW5Dsm5t/KKqqqvIuBQAAgFYQ3EIbGd1rSJw09p15lwGQ+eiog2O7fuPyLgMAAIBWEtxCGzpm9CGxnYXKgJyN7jkkPj/u3XmXAQAAwCYQ3EIb6lnVPb4z8aPZIcoAefn6hCOjX/feeZcBAADAJpAuQRvbsd9mcdyYt+VdBlChDhm8c0waulveZQAAALCJBLfQDj479tDYqs/ovMsAKsyQ7v3jzC2OyrsMAAAA2oDgFtpBr2494uyJH4luYTV3oON8c4sPxvCeA/MuAwAAgDYguIV2slv/LeLjo96SdxlAhXj30D3jXUP3yLsMAAAA2ojgFtrRF8e/JzbvPTLvMoAubmTPQfGNzT+QdxkAAAC0oR5teWdAfX269Yrvb/mJOHbKj2NtqSbvcmilmuWrY84Vf4/Ff3kp1r6xIvpuPyrGffWQ6LfzmOznT+7zn43ebsyXDo5Rx+7T6M9m/+TBmHvlP+pd1nuLobH9H45fd37WhX+NN255Jrr17RljvnhQDH3Pjut+tuiuF+KNW5+NLf/zyDbaSjqzyVscFYN79M+7DAAAANqQ4Bba2c79J8SXxh0WF772P3mXQiu9eu6dsWrq/JjwnXdHz5ED4o3bnotpn/9DbP+746LnqAGx4+0n1bv+0r9Pj1fP+XMMfvs2G7zf3lsNj60u++C681U9/u8giCX3TY1FdzwfW17ygaiesSi7v4EHTIweQ/pGzbLqmH3ZA/VuS+V6//D94+DBO+VdBgAAAG3MVAnQAY4dfUgcOGj7vMugFWpXrY3F97wYY790cAzYa7PoPWFIjPnMAdn/F/z+X9l1eo7oX++05K9TY8A+E6L3ZkM2eN8pqK17uxTKlq16eWH032uz6LfTmBj67h2ie//esfq1xdnPXv+vv8XwD+4evcYMauetp+i26D0y/t+E/8i7DAAAANqB4BY6QFVVVZw78WMxvIfV3jubUk1tRE0pqnrVP0ChqnePWP7ErPWu///buw/wKMvs7+NnJpPeC0kIJEDovQiI4oIICiwg7UVxEREQREBFxa6LHSurYMWCritIUVBZUQERLDRBLDRFOgKRkkoLybzXuf3PbAJBCUx4pnw/1zWbzDwzz5yZO4vwy5lzF+4vkNyvtkh8z0Z/ee6j2w/Kui6TZUPP12X7ffPk2J5c97HwOpXk8Pq9cjz3iBxav1eKjx6XkPQ4KVizSw5vzJKk/mxCFejCbMHydOYgiQwKs7oUAAAAAEAFILgFzpHE4Gh5pMY/xCY2q0tBOQRFhkhEk8qS9dpyKfw93wS5Oirh0I+7pXBfwUn3Pzh3nQRFBktshz8fkxDRKFXSH+gsNSb1lip3dZRjv+XIr9fNMPN0lY5FiOtaXzZdM1V2PvCpua/Out01fqFUubuj7J/1g2zo86ZsGvKuGeOAwHNnRm+pE5FmdRkAAAAAgArCjFvgHNJxCdektJe39n5hdSkoB51tu/Ohz2R911dFgmxmc7K4znXl8Pqsk+578MO1EtelvthD//yP15i2Nf53pXYlE+Su7/665Mz/WRJ6/dGtqyMZ9OKyd/JSiWqdYUYsZL2xXOq8O1Byv9wiO8Z9KrX/M8CTLxlerntCS+mT1MbqMgAAAAAAFYiOW+Acu7FKN2kSWc3qMlAOOqu25uQrpNGXo6X+f6+T2v/+hziPF0tIldhS9yv4bqcc3XbQHbyWR1B0mIRWi5ejO7PLPH5k6wE5OG+DpNxwoRSs2imRzauIIz5C4i6tI4c3ZLk7deH/MsNS5N4MNqYDAAAAAH9HcAucY8G2IHkm81qpFMzGUr5GRxUEJ0WZubN5S7dJTPvMUscPfLBWwusnm/m05VV06Jgc25ltNik7kdPplF2PLZC0W9pJUESIOIucJjg2x44X/XGn4j+uw7+F2UPkqcxBEhEUanUpAAAAAIAKRnALWCA5JNaEtxriwvvlLd0qed9slWO7ciRv2TbZPGKWhFWPl4TLG7rvU5R/VLIX/CwJp9iUbPMNs2Tf9DXu6789u0TyV+00s20Lvv9Nto39SMRuNyMYTnRgzk/iiAuXmHY1zfXIpmmSv3KHFPy4W36fulpCMxNMxy78330ZfaVWeKrVZQAAAAAAzgFm3AIWaRpVXe7J6CsPbpthdSn4CxrK7nn+aynMypegmFCJvaS2pI5qKzbH/4L37M82ijhF4rrUK/McR3fmyPHsw+7rhXvzZPu9H0tRzhFxxIdLRNM0qfVmfzP+oKTC/QWS9cYKqfXGle7bdB5upavPk61j5pj768Zl8H8Dk9tLj8RWVpcBAAAAADhHbE79DC4AyzyybZbM3PeN1WUA8GJ/i6kvE2sNFbuND8oAvig3N1diY2NlYr+ZEu44eSQO/JjNKSGVnXJst03EabO6GpxLrH1gYt0DF2vv066b2vWMH5udnS3x8fGSk5MjMTGeH4nJvwABi92Z0VuaR9WwugwAXrwZ2eOZAwltAQAAACDA8K9AwEs2K0sJjrO6FABeJi4oUibVuk6igphhDAAAAACBhuAW8AKJwdEyqdZQibSzUzyAPzj0lzo1B0nV0ESrSwEAAAAAWIDgFvASdSOqyNM1rxUH/7cEICL3ZvSVltG1rC4DAAAAAGAREiLAi1wYU1fuq9bP6jIAWGxY6qXSJ6mN1WUAAAAAACxEcAt4md5J58sNlTtbXQYAi/RLulBGVznzXU0BAAAAAP6B4BbwQiPSOsv/S7rA6jIAnGOXxjWVezL6WF0GAAAAAMALENwCXjzf8pK4xlaXAeAcOT+6toyvMUDsNv7TDAAAAAAguAW8loY3T9QYKBfE1LW6FAAVrGFEujxbc4gE2x1WlwIAAAAA8BIEt4AXC7E75Nmag6UVO8sDfqt6aLI8X2uYRASFWl0KAAAAAMCLENwCXi7MHiITaw6VFlGZVpcCwMPSQxNlcp0RkhAcZXUpAAAAAAAvQ3AL+ADtxHuh1jBpGlnd6lIAeEhGaJK8XmeUpITEWV0KAAAAAMALEdwCvhTe1h5mZmEC8G3VQisR2gIAAAAA/hTBLeBDooPC5eXa10u98CpWlwLgLGbavlZnpCSHxFpdCgAAAADAixHcAj4mxhEhr9YZKc0YmwD4nBphyfJaXUJbAAAAAMBfI7gFfFCMI1xerjNCLoypa3UpAE5TZliK6bStFBxjdSkAAAAAAB9AcAv4qHB7iEysOVQui29mdSkA/kKTyGoype5oSSK0BQAAAACcJoJbwIcF2x3yRI2rpW9SG6tLAXAKf4ttIJPr3CBxjkirSwEAAAAA+BCCW8DH2W12+We1K+TalA5WlwLgBD0TW8uzNQebDnkAAAAAAMrDUa57A/Bat1TtIQmOKPnXrrniFKfV5QAB77rUTnJjlb9bXQYAAAAAwEfRcQv4kUGpHeTpzEESRncfYBm72OSu9N6EtgAAAACAs0JwC/iZTvFNZEqdUexcD1hAf2nyVOYguSr5b1aXAgAAAADwcQS3gB9qEJku79QbI/XCq1hdChAwUoLj5M26o80vTwAAAAAAOFsEt4CfSgmJkyl1R0uH2EZWlwL4vcaRGTK1/hipH1HV6lIAAAAAAH6C4BbwYxFBoTKh5rVybUoHq0sB/Nblia3kjTqjJYnxJAAAAAAAD3J48mQAvI/dZpdbqvaQhpHp8sDW6VJQfNTqkgC/4BC73Jp+uQxIbmd1KQAAAAAAP0THLRAgLotvJlPr3yK1wlKtLgXweYmOaHm5zghCWwAAAABAhSG4BQJI9bBk+U/9MdIjoaXVpQA+q010HZnZYKy0iq5ldSkAAAAAAD/GqAQgwITbQ+SRGv+Q5lE15PEds+WY87jVJQE+IUjsckNaZxma2tGMIAEAAAAAoCIR3AIBqm+lC6RBZLrcvvkt2XF0v9XlAF4tJThOHs+8WlpEZVpdCgAAAAAgQNAyBASw+hFVZUb9sdIn6XyrSwG8VvvYhjKjwW2EtgAAAACAc4rgFghwEUGhMq7alfJczaGS4IiyuhzAa4TZguWO9F4ysdZQiXNEWl0OAAAAACDAENwCMC6OayjvNbhDOsQ1sroUwHLNImvIjAZjZUByO6tLAQAAAAAEKIJbAG4JwVHybM0h8mC1/hJpD7W6HMCSLtvbql4uU+qOkmphlawuBwAAAAAQwNicDMBJeiW1llbRteTBbdNled4vVpcDnBNNI6vLQ9X7S/WwZKtLAQAAAACA4BZA2aqEJsjkOjfI3P3fytM7P5SDx/OtLgmoEKE2h4xM6yLXpFwsdhsfRAEAAAAAeAeCWwB/qntiS/lbbH351865Mmf/CnGK0+qSAI9pG1NP7krvLRmMRQAAAAAAeBmCWwB/KdYRKQ9Uv1IuT2wlD2+fKZuP7LW6JOCspIXEy9iqPaVjfBOrSwEAAAAAoEwEtwBOW4voTJlR/zaZsneRvL57gRxxFlpdElAuwbYgGZTSQa6r3EnC7SFWlwMAAAAAwCkR3AIol2C7Q4ZXvlQuT2wpE3d9LB8fWM34BPiEC2Lqyt3pfaQaYxEAAAAAAD6A4BbAGUkNiZfHagyQAcnt5JmdH8iq/M1WlwSUqUZYstxUpZtcEtfY6lIAAAAAADhtBLcAzkrDyHR5o+5oWXjwB3l211zZfnSf1SUBRqXgGBmZ1kV6JraWIJvd6nIAAAAAACgXglsAHqGbPLWLaygzsr6WV3Z/JjlFh6wuCQEqOihMBqdcIgNS2kkYc2wBAAAAAD6K4BaARzd+0rCsZ1Jr+c/exfKfrMWSV3TE6rIQIEJsDrmyUlsZVrmTxDoirS4HAAAAAICzQnALwOOigsJkRFpn+UdyO/n33i9kWtaXkl9MgIuKEWYLlt5J58u1qR3M7GUAAAAAAPwBwS2AChPjCJfRVbrKNSkXy9SsJfJO1hLJLTpsdVnwE1H2MLki+UK5Orm9JAZHW10OAAAAAAAeRXAL4JwEuNqBOzClvUz//WuZmvWl/F6Ya3VZ8FHxjkjTzd2/0kXmZwsAAAAAAH9EcAvgnIkMCpMhqR1lYMrF8tmBNWYG7rpDO60uCz4iLSTeBLZ9k9pIRFCo1eUAAAAAAFChCG4BWLKJWbfE88xldf5ms5HZF9lrpUiKrS4NXqhNdB3pn3yRtI9tIHab3epyAAAAAAA4JwhuAViqRVSmuew6esCMUPhg/3LJK2Ijs0AXaQ+VHomtpH9yW6kRlmJ1OQAAAAAAnHMEtwC8QpXQBLk9vafcWKWrLDj4g8zZv0K+zftVnOK0ujScQ5lhKXJlpbbSI7GlGa0BAAAAAECgIrgF4FXC7CHSPbGluew8ul/m7FshH+1fKXsKs60uDRW42ViX+Oamw7ZhZLrV5QAAAAAA4BUIbgF4raqhiTK6SlcZmdZZluZuNF24S7LXyRFnodWl4SyF2BzSLraB6axtG1vfzD0GAAAAAAD/Q3ALwOvphlQa7unlUNFRWZKzzoxT+DJ3vRwpPmZ1eThNdrFJ86ga0jWhhXSOby4xjnCrSwIAAAAAwGsR3ALwKRFBodIlobm5HC4+Jl/lrJfPDn4vX+asM9fhXRy2IGkVXUs6xTWRDnGNJDE42uqSAAAAAADwCQS3AHxWuD1ELo1vai7aeft1zgb5OneDfJO7UXYfO2h1eQErJihcLoqtL+1jG0rb2HoSHURnLQAAAAAA5UVwC8BvNjXrGN/EXNTmw3vlm/8LcVfl/cpc3Aqk82mbRFaT1tG1pXVMbfO9dtoCAAAAAIAzR3ALwC9lhqeYy9Up7eVocaGszt9sNjj7Ln+LrD+0UwqdRVaX6NOzautFVPkjqI2uLS2iM033MwAAAAAA8ByCWwB+L9QeLBfE1DUXpUHu2kM75Pv8rX9cCrbKgeP5VpfptWKDIqRhZLo0isyQRhEZZoOxGEeE1WUBAAAAAODXCG4BBGSQ2yIq01xcth/5Xb4v2CY/H/5Nfjm8WzYd3i2/F+ZKII6cqBee9kdIG5khDSPSJSOsktVlAQAAAAAQcAhuAUDEhJMnBpQHj+ebENdcDu2WTUd2y86j++Xg8QLxdXFBkVIjPFkyw1KkRliKZIYlm6+VQ+LFZrNZXR4AAAAAAAGP4BYATiHeEeWe41rSoaKjsuvYftl19MAfF9f3xw7IvsJcyT5+SJzitKxum9gkwRElqSFxkqKX4Nj/+z7efM0ITZLE4GjL6gMAAAAAAH+N4BYAyikiKFRqh6eZS1mKnMWmW/dAYb7pzs0pKpCc44fMpaD4iBwrPi5H9eIs/ON7Z6GZu6vfH3cWi8NmlyD3JUgc8r/rwTaHRAeFSbQjXKKDwiUmKMJ9Xb+PCQqXSsExEmznj3cAAAAAAHwZ/7IHAA/TgDUpOMZcAAAAAAAAzoT9jB4FAAAAAAAAAKgwBLcAAAAAAAAA4GUIbgEAAAAAAADAyxDcAgAAAAAAAICXIbgFAAAAAAAAAC9DcAsAAAAAAAAAXobgFgAAAAAAAAC8DMEtAAAAAAAAAHgZglsAAAAAAAAA8DIEtwB82gMPPCDNmjXzmvMAAAAAAAB4AsEt4IOuvfZa6dWr10m3f/HFF2Kz2SQ7O9vjz6nBpp77zy6+QmudM2dOqdvGjh0rCxcutKwmAAAAAACAkghuAZwWDTZ3797tvlStWlUeeuihUreVdOzYMfElUVFRkpiYaHUZAAAAAAAABsEt4Mf2798vV111lVSpUkUiIiKkcePGMm3atFL3mTVrlrk9PDzcBJedOnWSgoKCMoPN1NRU9yUoKEiio6Pd1/v37y+jR4+WMWPGSFJSknTu3Nk8bsKECeb8kZGRkp6eLiNHjpT8/Hz3ed98802Ji4uTTz/9VOrXr2+ep0uXLqWCYO0kbt26tTmH3rdt27aybdu2Ml/zypUr5dJLLzU1xMbGSvv27WX16tXu49WrVzdfe/fubTpvXddPHJVQXFxsgmkNqENDQ82xTz75xH1869at5vHvv/++dOjQwby/TZs2laVLl57RWgEAAAAAAJREcAv4sSNHjsh5550n//3vf+Wnn36S4cOHy8CBA2XFihXmuIajGuwOGTJE1q9fbwLSPn36iNPpPKPne+uttyQkJES+/vprefnll81tdrtdJk6cKGvXrjXHP//8c7njjjtKPe7QoUPy9NNPy9tvvy1LliyR7du3mw5fdfz4cTMWQgPYH374wQSj+jpONZohLy9PBg0aJF999ZUsW7ZMateuLX//+9/N7a5gV02ZMsW8ftf1Ez333HPyzDPPmLr0eTWIvvzyy+WXX34pdb97773X1LpmzRqpU6eOeT+1ZgAAAAAAgLPhOKtHA7DM3LlzTXdqSUVFRaWua6etKwBVN954o+lsnTFjhulg1eBSQ0YNa6tVq2buo92xZ0pD0ieffLLUbdqB66LdrY888oiMGDFCXnzxRffthYWFJuitWbOmua6du9rtqnJzcyUnJ0e6d+/uPq6duadyySWXlLo+efJk06W7ePFic45KlSqZ2/U27RQ+FQ1s77zzTtNJrJ544glZtGiRPPvss/LCCy+476fvb7du3cz3Dz74oDRs2FA2bdok9erVO633DAAAAAAAoCx03AI+Sj+er12eJS+vvfbaSUHuww8/bMLYhIQEE/RqcKsdrUo/2t+xY0dzvF+/fvLqq6/KwYMHz7gm7e490YIFC8xzaIisoxW041dHOGiXrYuOGXCFsqpy5cqSlZVlvte6dTM27Xjt0aOH6YQ9cZ5uSXv37pVhw4aZEFlHJcTExJjRDK7XfDo0LP7tt9/MSIaS9Lp2JpfUpEmTUnUrV+0AAAAAAABniuAW8FE677VWrVqlLhqOlvTUU0+ZoFM7R7VbVMNdDUBdG4fpnNr58+fLvHnzpEGDBjJp0iSpW7eubNmy5YxrKknnwGqXq4ab7733nqxatcrdrVpy87Lg4OBSj9MxCCXHNehYAx2RcOGFF8r06dPNSAIdg1AWHZOgr1Nf9zfffGO+19m9FbVZWsnaXeMbdD4uAAAAAADA2SC4BfyYzprt2bOnXH311aa7NjMzU37++edS99GwUTtJ9WP+3333nZlRO3v2bI88vwa1GmLqrNg2bdqYwFU7Wc9E8+bN5e677zZhbKNGjWTq1KmnfM033XSTmWurYwt0Y7F9+/adFLaeOFaiJO3STUtLM+c68dwacAMAAAAAAFQ0ZtwCfkzHBcyaNcuEnfHx8TJhwgQzSsAVPi5fvlwWLlwol112mSQnJ5vrv//++5/OkC0P7QLW+bXayatjDkpuWna6tPtX59TqxmAapm7cuNFsEHbNNdec8jXrJmctW7Y0Iw9uv/12CQ8PL3UfnbWrr1sDaw129b05kT5u3LhxZoRDs2bNTNevdu++88475XwXAAAAAAAAyo+OW8CP3XfffdKiRQszHuHiiy82m3H16tWrVGfpkiVLTHeqdsPq/bU7tmvXrh55fu3y1bBYN/bSLlkNPcePH1+uc+j82w0bNkjfvn1NjcOHD5dRo0bJ9ddfX+b9X3/9dTOnV1+3ztPV7lsNpUvS16gjItLT000nb1n0cbfeeqvcdtttZgbwJ598Ih9++KEJhgEAAAAAACqazVlykCQAAAAAj9JPgOiGmRP7zZRwR+l58PBzNqeEVHbKsd02Eecfs/ARIFj7wMS6By7W3qddN/XMm9eys7PNp3hzcnJMc5yn0XELAAAAAAAAAF6GGbcAAADAOTBwcieJi4uzugycQ7pJa1ZWlhnbZLfTMxNIWPvAxLoHLtYeFYWfJgAAAAAAAADwMgS3AAAAAAAAAOBlCG4BAAAAAAAAwMsQ3AIAAAAAAACAlyG4BQAAAAAAAAAvQ3ALAAAAAAAAAF6G4BYAAAAAAAAAvAzBLQAAAAAAAAB4GYJbAAAAAAAAAPAyBLcAAAAAAAAA4GUIbgEAAAAAAADAyxDcAgAAAAAAAICXIbgFAAAAAAAAAC9DcAsAAAAAAAAAXobgFgAAAAAAAAC8DMEtAAAAAAAAAHgZglsAAAAAAAAA8DIEtwAAAAAAAADgZRxWFwAAAAD4M6fTab7m5uaK3U7fRCApLi6WvLw8CQsLY+0DDGsfmFj3wMXaB67c3NxSf9/zNIJbAAAAoALt37/ffK1WrZrVpQAAAKCC/r4XGxvr8fMS3AIAAAAVKCEhwXzdvn17hfyFHt7dhZOeni47duyQmJgYq8vBOcTaBybWPXCx9oErJydHMjIy3H/f8zSCWwAAAKACuT4yqaEt/5gLTLrurH1gYu0DE+seuFj7wGWvoBEZDN4AAAAAAAAAAC9DcAsAAAAAAAAAXobgFgAAAKhAoaGhMm7cOPMVgYW1D1ysfWBi3QMXax+4Qit47W1Op9NZIWcGAAAAAAAAAJwROm4BAAAAAAAAwMsQ3AIAAAAAAACAlyG4BQAAAAAAAAAvQ3ALAAAAAAAAAF6G4BYAAACoQC+88IJUr15dwsLC5Pzzz5cVK1ZYXRI8aPz48dKqVSuJjo6W5ORk6dWrl2zcuLHUfY4cOSKjRo2SxMREiYqKkr59+8revXstqxkV4/HHHxebzSZjxoxx38ba+69du3bJ1VdfbdY2PDxcGjduLN9++637uO4D/89//lMqV65sjnfq1El++eUXS2vG2SkqKpL7779fatSoYda0Zs2a8vDDD5u1dmHd/cOSJUukR48ekpaWZv5cnzNnTqnjp7POBw4ckAEDBkhMTIzExcXJ0KFDJT8/v9y1ENwCAAAAFWT69Oly6623yrhx42T16tXStGlT6dy5s2RlZVldGjxk8eLFJphbtmyZzJ8/XwoLC+Wyyy6TgoIC931uueUW+eijj2TmzJnm/r/99pv06dPH0rrhWStXrpRXXnlFmjRpUup21t4/HTx4UNq2bSvBwcEyb948WbdunTzzzDMSHx/vvs+TTz4pEydOlJdfflmWL18ukZGR5s9/DfPhm5544gl56aWX5Pnnn5f169eb67rOkyZNct+HdfcPBQUF5u9s+sv3spzOOmtou3btWvN3g7lz55owePjw4eUvxgkAAACgQrRu3do5atQo9/WioiJnWlqac/z48ZbWhYqTlZWlrVfOxYsXm+vZ2dnO4OBg58yZM933Wb9+vbnP0qVLLawUnpKXl+esXbu2c/78+c727ds7b775ZnM7a++/7rzzTudFF110yuPFxcXO1NRU51NPPeW+TX8eQkNDndOmTTtHVcLTunXr5hwyZEip2/r06eMcMGCA+Z51908i4pw9e7b7+ums87p168zjVq5c6b7PvHnznDabzblr165yPT8dtwAAAEAFOHbsmKxatcp8fM7Fbreb60uXLrW0NlScnJwc8zUhIcF81Z8B7cIt+XNQr149ycjI4OfAT2jHdbdu3UqtsWLt/deHH34oLVu2lH79+pkRKc2bN5dXX33VfXzLli2yZ8+eUmsfGxtrxuWw9r7rwgsvlIULF8rPP/9srn///ffy1VdfSdeuXc111j0wbDmNddavOh5B/5xw0fvr3wO1Q7c8HB6sHQAAAMD/2bdvn5mHl5KSUup2vb5hwwbL6kLFKS4uNvNN9SPUjRo1MrfpP+5CQkLMP+BO/DnQY/Bt7777rhmDoqMSTsTa+6/Nmzebj8zrKJx77rnHrP9NN91k1nvQoEHu9S3rz3/W3nfdddddkpuba34BExQUZP4b/+ijj5qPxCvWPTDsOY111q/6S52SHA6H+aVueX8WCG4BAAAAwEOdlz/99JPpwIL/27Fjh9x8881mfqFuPojA+iWNdtI99thj5rp23Or/93XepQa38E8zZsyQd955R6ZOnSoNGzaUNWvWmF/W6QZWrDsqCqMSAAAAgAqQlJRkOnJO3EFer6emplpWFyrG6NGjzeYjixYtkqpVq7pv17XWsRnZ2dml7s/Pge/TUQi60WCLFi1MJ5VedAMy3bBGv9fuK9beP+lO8g0aNCh1W/369WX79u3me9f68ue/f7n99ttN123//v2lcePGMnDgQLMB4fjx481x1j0wpJ7GOuvXEzeiPX78uBw4cKDcPwsEtwAAAEAF0I/MnnfeeWYeXskuLb1+wQUXWFobPEf3LdHQdvbs2fL5559LjRo1Sh3XnwHdeb7kz8HGjRtNwMPPgW/r2LGj/Pjjj6brznXRLkz92LTre9beP+k4FF3LknTuabVq1cz3+ueAhjMl114/Yq+zLVl733Xo0CEzo7Qk/QWt/rddse6BocZprLN+1V/a6S/4XPTvCPqzorNwy4NRCQAAAEAF0fmH+vFJDXBat24tzz77rBQUFMjgwYOtLg0eHI+gH5v94IMPJDo62j27TjcqCQ8PN1+HDh1qfhZ0tl1MTIzceOON5h91bdq0sbp8nAVdb9csY5fIyEhJTEx0387a+yftstSNqnRUwhVXXCErVqyQyZMnm4uy2WzmI/SPPPKI1K5d2wQ9999/v/lIfa9evawuH2eoR48eZqatbjCooxK+++47mTBhggwZMsQcZ939R35+vmzatKnUhmT6Czn9s1zX/6/WWTvwu3TpIsOGDTMjVHSjSv0lr3Zr6/3KxQkAAACgwkyaNMmZkZHhDAkJcbZu3dq5bNkyq0uCB+k/qcq6TJkyxX2fw4cPO0eOHOmMj493RkREOHv37u3cvXu3pXWjYrRv39558803u6+z9v7ro48+cjZq1MgZGhrqrFevnnPy5MmljhcXFzvvv/9+Z0pKirlPx44dnRs3brSsXpy93Nxc8/9v/W96WFiYMzMz03nvvfc6jx496r4P6+4fFi1aVOZ/2wcNGnTa67x//37nVVdd5YyKinLGxMQ4Bw8e7MzLyyt3LTb9H08n0wAAAAAAAACAM8eMWwAAAAAAAADwMgS3AAAAAAAAAOBlCG4BAAAAAAAAwMsQ3AIAAAAAAACAlyG4BQAAAAAAAAAvQ3ALAAAAAAAAAF6G4BYAAAAAAAAAvAzBLQAAAAAAAAB4GYJbAAAAAAAC0NatW8Vms8maNWskEG3cuFFSU1MlLy/vjM+xbt06qVq1qhQUFHi0NgBQBLcAAAAAAFSQPXv2yI033iiZmZkSGhoq6enp0qNHD1m4cKH4oosvvljGjBkj/uDuu+82axMdHe0Ostu1ayeRkZHmq14vqXv37vLee++Vuq1BgwbSpk0bmTBhwjmtHUBgILgFAAAAAKACaPB33nnnyeeffy5PPfWU/Pjjj/LJJ59Ihw4dZNSoUVaXF9C2b98uc+fOlWuvvdZ922233SZVqlQxHciVK1eWsWPHuo9Nnz5d7Ha79O3b96RzDR48WF566SU5fvz4OasfQGAguAUAAAAAoAKMHDnSjCJYsWKFCfzq1KkjDRs2lFtvvVWWLVtWKkTs2bOnREVFSUxMjFxxxRWyd+9e9/EHHnhAmjVrJm+88YZkZGSY++m5i4qK5MknnzQf909OTpZHH3201PPrc2ug2LVrVwkPDzddv7NmzfrTmn/66Sdzf32OlJQUGThwoOzbt88c05Bz8eLF8txzz5lz68XVlfpnj3N16t50001yxx13SEJCgqlZX1dJ2dnZct1110mlSpXM+3DJJZfI999/7z6u32vorR2yelxD8W+//dYc27Ztm+lkjo+PNx2z+j5//PHHp3ydM2bMkKZNm5qg1mX9+vUyaNAgqV27tnmtet1V13333ScvvPBCmee69NJL5cCBA+a9AQBPIrgFAAAAAMDDNMjT7lrtrNUg8URxcXHma3FxsQltXcHf/PnzZfPmzXLllVeWuv+vv/4q8+bNM+ecNm2avP7669KtWzfZuXOnedwTTzxhwsXly5eXetz9999vQmMNPQcMGCD9+/d3B5In0oBSw9LmzZubQFSfSwNkDZKVBrYXXHCBDBs2THbv3m0uOvrhrx7n8tZbb5n3QmvUwPmhhx4yr9elX79+kpWVZV7nqlWrpEWLFtKxY0fz3iitX+fJrly50hy/6667JDg42BzT9/no0aOyZMkS09ms74eGyKfy5ZdfSsuWLUvdpkHuggULzJp89tln0qRJE3P77bffbs6vr7UsISEhJljXcwKAJzk8ejYAAAAAACCbNm0Sp9Mp9erV+9P76axbDRq3bNniDgb//e9/m45RDShbtWplbtMwUTtutdtU56pq56lurqVdpfoR/rp165qwctGiRXL++eeXCkO1i1U9/PDDJiidNGmSvPjiiyfV8vzzz5vw9bHHHnPfps+pdf3888+mY1hDyoiICNMxW57HKQ1Cx40bZ77XrlZ9nL5+7Vj96quvTGeyBrc6C1g9/fTTMmfOHNMlPHz4cNOZrCGq6z3Vc7joMQ2oGzdubK5rd/Gf0Q7dE4Nbfb7rr79eqlevbmp95ZVXTBCsoxP0vdUgWoPpyy67TCZOnGjeC5e0tDRzTgDwJIJbAAAAAAA8TEPb06Hdrxpwluzm1GBWO3L1mCu41TDRtYmW0nEEQUFBJrQteZsGnyVph+yJ1zWILIt25WrwW1anqnb8ugLYM32cq4PVRefIuurVc+Tn50tiYmKp+xw+fNicQ+mICQ2h3377benUqZMJpWvWrGmO6RiGG264wXTK6jENcU98vhPPGxYWVuo2HZugc29dtIO3c+fOplP4kUceMe+/huVdunQxoa5ubOaioygOHTp0yucDgDPBqAQAAAAAADxMu0F1BuyGDRs8cj7XSAAXPXdZt2ln7pnS4FTnxGqwW/Lyyy+/SLt27c76cX9Wr55Dg9wTz6FBqXbZKp2Ju3btWjMiQjd804B79uzZ5pgGujpiQmfragezdtNqZ/GpJCUlycGDB//0/dAOYu2u1Vm6X3zxhQmD9TX06dPHXC9JxznobF4A8CSCWwAAAAAAPEw34NJuTd3QqqCg4KTjOhdW1a9fX3bs2GEuLuvWrTPHNZg8WyU3QXNd1+csi86U1WBUu3tr1apV6uKa06vjAXRTtPI+7q/oOfbs2SMOh+Okc2jI6qLdu7fccovprNUAdcqUKe5j2rU8YsQIef/99+W2226TV1999ZTPp6Md9H0+Fe12njp1qhkvofQ1FxYWmu/164nvgW7OpucEAE8iuAUAAAAAoAJoaKsBX+vWreW9994zHagaCOp8VNcIA/1Yv85l1Y23Vq9ebea8XnPNNdK+ffuTZrCeiZkzZ5p5szprVufL6vlHjx5d5n11Ay7tHL3qqqvMfF0dUfDpp5/K4MGD3UGlhrO6udjWrVtl3759pmP2dB73V/R90PekV69eJpTV83/zzTdy7733mrmyOtpA69ZOV50l+/XXX5vncoXQY8aMMc+ps4L1fdTRDacKqJWG6kuXLi2zPh1zoTN1//Wvf7mD57Zt25ogWNdPZxDrdRetddeuXeY1AIAnEdwCAAAAAFABdIMsDRF1IzHtAG3UqJHZiEs35HrppZfc4wI++OADiY+PN2MFNPzTx02fPt0jNTz44IPy7rvvmnmvGjhOmzbtlJ28usGWBqIaZuqIAA2UNRDVebuuWbpjx441s3X1HDoaQDcFO53H/RV9H3SjNX0PNPDVztr+/fubkNY1z3f//v0m1NZjulFY165dzetT+twaIGtYqzNo9T5lbcDmoo/V7t4FCxacdGzy5MnmObt37+6+Tcc0HDlyxGz8pl3A+lwu+p7q665WrdppvVYAOF025+lOTAcAAAAAAD5Dw1CdAatdrCi7I/rDDz80nbpn6tixY2aesY5VKNmFCwCe4PDIWQAAAAAAAHzI9ddfb2YJ5+XlSXR09BmdQzuO77nnHkJbABWCjlsAAAAAAPwQHbcA4NvouAUAAAAAwA/RpwUAvo3NyQAAAAAAAADAyxDcAgAAAAAAAICXIbgFAAAAAAAAAC9DcAsAAAAAAAAAXobgFgAAAAAAAAC8DMEtAAAAAAAAAHgZglsAAAAAAAAA8DIEtwAAAAAAAAAg3uX/A9OE9qCp84o2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QC visualizations saved to: Connor/data/outputs/lexicon_qc_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Sentiment distribution\n",
    "if 'sentiment' in df_enriched.columns:\n",
    "    sentiment_counts = df_enriched['sentiment'].value_counts()\n",
    "    axes[0, 0].bar(sentiment_counts.index, sentiment_counts.values, color=['#e74c3c', '#95a5a6', '#2ecc71'])\n",
    "    axes[0, 0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Score distribution histogram\n",
    "if 'score' in df_enriched.columns:\n",
    "    scores = pd.to_numeric(df_enriched['score'], errors='coerce').dropna()\n",
    "    axes[0, 1].hist(scores, bins=30, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].set_title('Sentiment Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Score')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].axvline(0, color='red', linestyle='--', linewidth=2, label='Neutral')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Translation coverage\n",
    "if 'english' in df_enriched.columns:\n",
    "    has_english = df_enriched['english'].notna() & (df_enriched['english'].str.strip() != \"\")\n",
    "    coverage_data = {'Has Translation': has_english.sum(), 'Missing': (~has_english).sum()}\n",
    "    axes[1, 0].pie(coverage_data.values(), labels=coverage_data.keys(), autopct='%1.1f%%',\n",
    "                   colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "    axes[1, 0].set_title('English Translation Coverage', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Data completeness\n",
    "completeness = {}\n",
    "for col in ['french', 'english', 'sentiment', 'score']:\n",
    "    if col in df_enriched.columns:\n",
    "        complete = df_enriched[col].notna().sum()\n",
    "        completeness[col] = (complete / len(df_enriched)) * 100\n",
    "\n",
    "if completeness:\n",
    "    axes[1, 1].barh(list(completeness.keys()), list(completeness.values()), color='#9b59b6')\n",
    "    axes[1, 1].set_title('Data Completeness by Column', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Completeness (%)')\n",
    "    axes[1, 1].set_xlim(0, 100)\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Connor/data/outputs/lexicon_qc_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ QC visualizations saved to: Connor/data/outputs/lexicon_qc_analysis.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
